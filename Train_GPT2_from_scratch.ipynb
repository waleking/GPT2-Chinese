{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所需环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers版本号为2.1.1，pytorch为1.2.0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0415 15:24:11.910086 140436257081152 file_utils.py:39] PyTorch version 1.2.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在requirments.txt中，列出了所需要的其他的包，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==2.1.1\n",
      "torch\n",
      "numpy\n",
      "tqdm\n",
      "sklearn\n",
      "keras\n",
      "tb-nightly\n",
      "future\n",
      "thulac\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录结构以及重要文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前文件夹中包含的文件如下。其中**doupo**文件夹下包含“斗破苍穹”的示例任务，用以简单说明如何构建字典、tokenization、以及在一个小文件上从头训练一个指定层数的GPT模型；**pretrain**文件夹下包含预训练任务，主要用以说明如何在260万篇新闻文档上预训练一个12层的GPT2；**dataaugmentation**文件夹下包含数据增强任务，主要用以说明如何利用预训练的GPT2模型在较小的文档集上进行微调，并用于数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "├── cache\n",
      "├── config\n",
      "├── dataaugmentation\n",
      "├── eval.py\n",
      "├── generate.py\n",
      "├── generate_texts.py\n",
      "├── LICENSE\n",
      "├── pretrain\n",
      "├── README.md\n",
      "├── requirements.txt\n",
      "├── sample\n",
      "├── tasks\n",
      "├── tokenizations\n",
      "├── Train_GPT2_from_scratch.ipynb\n",
      "├── train.json\n",
      "├── train_on_small_file.py\n",
      "└── train_single.py\n",
      "\n",
      "7 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tree -L 1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要重点加以说明的是，上述列表中的tokenizations文件夹，train_single.py, 和train_on_small_file.py。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizations`提供了各种tokenization的方法，具体到本代码库中，使用的是该文件夹中的`tokenization_chars.py`文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization的第一阶段是将纯文本文件依照字典文件vocab.txt进行划分为token，第二阶段是将token转化为字典文件中token对应的数字。举例来说，\"[CLS]《斗破苍穹》天蚕土豆[SEP][CLS] ...\"在第一阶段被分割为列表：['[CLS]', '《', '斗', '破', '苍', '穹', '》', '天', '蚕', '土', '豆', '[SEP]', '[CLS]', ...]，其中[CLS], [SEP]在vocab.txt对应一个单独的占位符；第二阶段在查找vocab.txt之后，将字符串列表转化为整型数列表：101 517 3159 4788 5721 4957 518 1921 6014 1759 6486 102 101。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下是tokenization第一阶段的代码，输入为text字符串，输出为字符串列表result，对于形如[.\\*]的特殊字符使用栈来处理：如果该特殊字符出现在vocab.txt中，则被提取为一个单独的字符串存入列表中，如161行所示；反之若没有出现在vocab.txt中，则逐字符加入到列表中，如163-165行所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140     def _tokenize(self, text):\n",
      "141         #pdb.set_trace()\n",
      "142         stack = []\n",
      "143 \n",
      "144         result = []\n",
      "145         i = 0\n",
      "146         len_txt = len(text)\n",
      "147         while(i<len_txt):\n",
      "148             char = text[i]\n",
      "149             if(len(stack)==0):\n",
      "150                 if(char != '['):\n",
      "151                     result.append(char)\n",
      "152                 else:\n",
      "153                     stack.append('[')\n",
      "154             else:\n",
      "155                 if(char == ']'):\n",
      "156                     stack.append(char) # don't forget to append it firstly\n",
      "157                     # process the content in the stack\n",
      "158                     seq = \"\".join(stack)\n",
      "159                     if(seq in self.vocab):\n",
      "160                         # the [.*] stuff appeared in the vocabulary, e.g. [UNK], [CLS], [SEP], ...\n",
      "161                         result.append(seq)\n",
      "162                     else:\n",
      "163                         # other [.*] stuff which are not valid element in the vocabulary\n",
      "164                         for e in stack:\n",
      "165                             result.append(e)\n",
      "166                     # clean the stack after processing \n",
      "167                     stack = []    \n",
      "168                 else:\n",
      "169                     stack.append(char)\n",
      "170             i += 1\n",
      "171 \n",
      "172         if(len(stack)>0): \n",
      "173             for e in stack:\n",
      "174                 result.append(e)\n",
      "175         # pdb.set_trace()\n",
      "176         return result\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=140 and $.<=176)' tokenizations/tokenization_chars.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization的第二部分如下所示，对第一阶段获得的result列表中的token字符串进行查表操作，如果没有出现在vocab.txt中，那么就以[UNK]对应的id来代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178     def _convert_token_to_id(self, token):\n",
      "179         \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
      "180         #if(token not in self.vocab):\n",
      "181         #    print(token)\n",
      "182         return self.vocab.get(token, self.vocab.get(self.unk_token))\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=178 and $.<=182)' tokenizations/tokenization_chars.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，本代码库中的tokenization方法主要为中文设计，如果训练数据中混杂有英文字符，那么就将英文单词按照char level进行划分，例如\"word\"会被\"w\",\"o\",\"r\",\"d\"四个字符来代替。不同于BPE的方法，我们认为这样处理是最节省计算资源的，比起BPE能够更从容应对几十GB的中文训练语料。从具体的实际效果来看，在斗破苍穹的语料上进行tokenization，普通方法需要耗时77.9秒，使用了我们的方法之后，耗时12.1秒，用时为原来的15%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两种训练方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_single.py`：当输入的训练数据为一篇完整的长文档（例如小说）或是一个单一的大型文档，此时使用`train_single.py`。有两方面的原因。（1）train_single.py在训练过程中不对training samples进行random shuffle，保持training samples之间的顺序，因此完整的长文档使用train_single.py。（2）对顺序无关的超多的样本，例如包含260万篇新闻的大型训练集，在训练过程中进行random shuffle的代价是巨大的，包括shuffle的代价，重新划分training sample的代价和tokenization的代价，并且在大型训练集上我们进行预训练的轮数有限，1到2轮就能获得一个较好的预训练模型。综合考虑shuffle的代价和收益，因此也将大型训练集中的多个样本拼接为一个单一的大型文档，使用train_single.py。我们在《从头训练一个GPT2模型》一节对此进行更详细的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_on_small_file.py`：当输入的训练数据为多个样本，并且这多个样本的总体积较小时，将这些样本汇总到一个文件，每个样本占据一行，然后使用`train_on_small_file.py`。有如下两方面原因：（1）train_on_small_file.py针对每个样本建立一个单独的training sample送入Transformer Encoder中，如果一个样本的长度不足Transformer Encoder的长度（在本代码库中用n_ctx表示），那么不足部分以\\[PAD\\]来补足；超出n_ctx的部分直接截断丢弃不用。这一点和train_single.py不同，train_single.py中多个较短的样本可能会占据一个Transformer Encoder的输入长度（n_ctx）。（2）train_on_small_file.py会在不同的训练轮次中，对training samples进行random shuffle，以提高训练质量。由于train_on_small_file.py的训练数据体积较小，样本数也较小，并且样本之间相互独立。所以，如果是唐诗、宋词、现代诗、意图增强等文本数据，由于其规模较小，完全可以使用`train_on_small_file.py`来完成训练或者微调，这样得到的模型质量要高于`train_single.py`训练得到的模型。我们在《微调GPT2模型进行数据增强》一节对此进行更详细的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从头训练一个GPT2模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们使用一个规模较小的文件用以验证从头训练一个GPT2模型的可行性。该文件为斗破苍穹小说文本文件，规模为16MB，16万行。如下是该文件的一些基本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 weijing weijing 16M Apr  5 10:54 rawdata/train_raw.txt\n",
      "162111 rawdata/train_raw.txt\n",
      "《斗破苍穹》天蚕土豆\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.shuyaya.com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "在线阅读：http://www.shuyaya.com/read/18/\n",
      "--------------------------------------------------\n",
      "\n",
      "第一章 陨落的天才\n",
      "\n",
      "    “斗之力，三段！”\n",
      "\n",
      "    望着测验魔石碑上面闪亮得甚至有些刺眼的五个大字，少年面无表情，唇角有着一抹自嘲，紧握的手掌，因为大力，而导致略微尖锐的指甲深深的刺进了掌心之中，带来一阵阵钻心的疼痛…\n",
      "\n",
      "    “萧炎，斗之力，三段！级别：低级！”测验魔石碑之旁，一位中年男子，看了一眼碑上所显示出来的信息，语气漠然的将之公布了出来…\n",
      "\n",
      "    中年男子话刚刚脱口，便是不出意外的在人头汹涌的广场上带起了一阵嘲讽的骚动。\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; ls -laht rawdata/train_raw.txt; wc -l rawdata/train_raw.txt; head -n 15 rawdata/train_raw.txt; echo \"...\"; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有rawdata/train_raw.txt文件，那么运行`!cd tasks/doupo; bash train.sh`，该脚本帮助建立对应文件夹并下载train_raw.txt。如下是train.sh的部分信息。首先创建raw_data文件夹，divide文件夹，tokenized文件夹，model文件夹和config文件夹。其中，raw_data文件夹用于存储原始的训练数据和进行基本预处理之后的训练数据；divide文件夹用于存储分割后的大文件（在train_single.py中有体现）；tokenized文件夹用于存储token转为数字id之后的文件；model文件夹用于储存训练好的模型；config文件夹用于储存模型的基本配置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tasks/doupo/train.sh`的第26行用于从公网下载原始训练数据并存储到rawdata文件夹下的train_raw.txt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 job_dir=\"tasks/doupo\"\n",
      "2 \n",
      "3 cd ../..\n",
      "4 \n",
      "5 if [ ! -e $job_dir/rawdata ]; then\n",
      "6     mkdir $job_dir/rawdata\n",
      "7 fi\n",
      "8 \n",
      "9 if [ ! -e $job_dir/divide ]; then\n",
      "10     mkdir $job_dir/divide\n",
      "11 fi\n",
      "12 \n",
      "13 if [ ! -e $job_dir/tokenized ]; then\n",
      "14     mkdir $job_dir/tokenized\n",
      "15 fi\n",
      "16 \n",
      "17 if [ ! -e $job_dir/model ]; then\n",
      "18     mkdir $job_dir/model\n",
      "19 fi\n",
      "20 \n",
      "21 if [ ! -e $job_dir/config ]; then\n",
      "22     mkdir $job_dir/config\n",
      "23 fi\n",
      "24 \n",
      "25 if [ ! -e $job_dir/rawdata/train.txt ]; then\n",
      "26     wget -c -O $job_dir/rawdata/train_raw.txt https://github.com/GaoPeng97/transformer-xl-chinese/blob/master/data/doupo/train.txt?raw=true\n",
      "27     # remove empty lines\n",
      "28     python $job_dir/format_raw_txt.py $job_dir/rawdata/train_raw.txt $job_dir/rawdata/train.txt \n",
      "29 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=1 and $.<=29)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tasks/doupo/train.sh`的第28行调用`format_raw_txt.py`，用于移除原始文件中的空行和去除一行中左侧多余的空格(lstrip)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t'''\n",
      "     2\tThis script is used to remove empty lines in the input file \n",
      "     3\t'''\n",
      "     4\timport sys\n",
      "     5\t\n",
      "     6\tdef format(in_filename, out_filename):\n",
      "     7\t    with open(out_filename, \"w\") as fOut:\n",
      "     8\t        with open(in_filename, \"r\") as fIn:\n",
      "     9\t            for line in fIn:\n",
      "    10\t                if(line!=\"\"):\n",
      "    11\t                    fOut.write(line.lstrip())\n",
      "    12\t\n",
      "    13\t\n",
      "    14\tdef test():\n",
      "    15\t    in_filename=\"./rawdata/train_raw.txt\"\n",
      "    16\t    out_filename=\"./rawdata/train.txt\"\n",
      "    17\t    format(in_filename, out_filename)\n",
      "    18\t\n",
      "    19\t\n",
      "    20\tif __name__==\"__main__\":\n",
      "    21\t    if(len(sys.argv)<3):\n",
      "    22\t        print(\"usage: python format_raw_txt.py in_filename out_filename\")\n",
      "    23\t    else:\n",
      "    24\t        in_filename = sys.argv[1]\n",
      "    25\t        out_filename = sys.argv[2]\n",
      "    26\t        format(in_filename, out_filename)\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; cat -n format_raw_txt.py; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前多个模型都直接使用了BERT chinese字表，该字表拥有字符21128个。但是该字表有如下的问题：不包含大写字母A到Z，不包含制表符，空格等字符。我们可以将这些字符追加到BERT chinese字表之后，如`train.sh`代码中的第31行到44行所示，将新的字符表保存在doupo/config/vocab.txt文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 vocab_size=21128\n",
      "32 declare -a additional_chars=(\"“\" \"”\" \"…\" \"’\" \"‘\" \"—\" \" \" \"\\t\" \"\\`\")\n",
      "33 new_vocab_size=$(($vocab_size+${#additional_chars[@]}+26))\n",
      "34 echo 'setting config/vocab.txt and config/model_config.json'\n",
      "35 if [ ! -e $job_dir/config/vocab.txt ]; then\n",
      "36     wget -c -O $job_dir/config/vocab.txt https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\n",
      "37     # append new characters to the vocabulary\n",
      "38     for letter in \"${additional_chars[@]}\"; do\n",
      "39         echo -e \"$letter\" >> $job_dir/config/vocab.txt\n",
      "40     done\n",
      "41     for letter in {A..Z} ; do\n",
      "42         echo $letter >> $job_dir/config/vocab.txt\n",
      "43     done\n",
      "44 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=31 and $.<=44)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以进一步的设置模型参数，并将模型参数保存于config/model_config.json。如果不存在doupo/config/model_config.json，那么doupo/train.sh的第49行拷贝config/model_config.json模板到指定位置，并进行编辑。如下的代码第52，54，56，57行用于编辑模型参数，例如将层数修改为10层，将n_ctx和n_positions从1024修改为512。因为n_ctx是Transformer Encoder能够容纳的最大序列长度，因此减半之后可以大幅度节省训练时占用的显存，同时也考虑到512是一个合理的较长的长度，能够较大限度的捕获文本上远距离的依赖关系。此外我们将stride设置为256，也就是在一个长序列上窗口大小为512，每次移动窗口的幅度为256。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 stride=256\n",
      "47 n_layers=10\n",
      "48 n_ctx=512\n",
      "49 if [ ! -e $job_dir/config/model_config.json ]; then\n",
      "50     cp config/model_config.json $job_dir/config/model_config.json\n",
      "51     # change vocabulary size\n",
      "52     perl -pi -e 's/'$vocab_size'/'$new_vocab_size'/g' $job_dir/config/model_config.json\n",
      "53     # change the number of layers from 12 to $n_layers\n",
      "54     perl -pi -e 's/\"n_layer\": 12/\"n_layer\": '$n_layers'/g' $job_dir/config/model_config.json\n",
      "55     # change the model input length from 1024 to $n_ctx\n",
      "56     perl -pi -e 's/\"n_ctx\": 1024/\"n_ctx\": '$n_ctx'/g' $job_dir/config/model_config.json\n",
      "57     perl -pi -e 's/\"n_positions\": 1024/\"n_positions\": '$n_ctx'/g' $job_dir/config/model_config.json\n",
      "58 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=46 and $.<=58)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在`config/model_config.json`中查看即将要训练的模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t{\n",
      "     2\t  \"initializer_range\": 0.02,\n",
      "     3\t  \"layer_norm_epsilon\": 1e-05,\n",
      "     4\t  \"n_ctx\": 512,\n",
      "     5\t  \"n_embd\": 768,\n",
      "     6\t  \"n_head\": 12,\n",
      "     7\t  \"n_layer\": 10,\n",
      "     8\t  \"n_positions\": 512,\n",
      "     9\t  \"vocab_size\": 21163\n",
      "    10\t}"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; cat -n config/model_config.json; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要的数据预处理发生于`train_single.py`的`build_files`文件中，如下所示。`build_files`包含有多个参数，其中有必要说明的是`num_pieces`和`full_tokenizer`。`full_tokenizer`调用的是《Tokenization》一小节中提到的两阶段方法，效率较高。`num_pieces`将一个大文件分割为若干个小文件，以减小训练时IO一个大文件带来的内存压力。\n",
    "如下，第26行到第35行用以分割大文件到divide文件夹。分割之后，每一行补充[CLS]字符，回车符'\\\\n'转为[SEP]字符，然后调用`full_tokenizer`将token转为id。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例来说，\"《斗破苍穹》天蚕土豆\"这一行转为\"[CLS]《斗破苍穹》天蚕土豆[SEP]\"并转为对应的数字id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 def build_files(raw_data_path, divide_path, tokenized_data_path, full_tokenizer, num_pieces):\n",
      "19     if not os.path.exists(tokenized_data_path):\n",
      "20         os.mkdir(tokenized_data_path)\n",
      "21     if not os.path.exists(divide_path):\n",
      "22         os.mkdir(divide_path)\n",
      "23     print(\"now time: \", datetime.now())\n",
      "24     print(\"begin to divide raw text ...\")\n",
      "25 \n",
      "26     writers = [open(divide_path + 'divide_piece_{}.txt'.format(i), 'w') for i in range(0,num_pieces)]\n",
      "27 \n",
      "28     with open(raw_data_path, 'r', encoding='utf8') as f:\n",
      "29         line_num = 0\n",
      "30         for line in f:\n",
      "31             writers[line_num % num_pieces].write(\"%s\" % line)\n",
      "32             line_num += 1\n",
      "33     \n",
      "34     for i in range(0, num_pieces):\n",
      "35         writers[i].close()\n",
      "36     \n",
      "37     print('now time: ', datetime.now())\n",
      "38     print(\"begin making tokenization ...\")\n",
      "39     files = [filename for filename in os.listdir(divide_path) if f!='.gitignore']\n",
      "40     for i, filename in enumerate(files):\n",
      "41         if(os.path.isdir(filename)):\n",
      "42             continue\n",
      "43 \n",
      "44         with open(divide_path+filename, 'r', encoding='utf8') as reader:\n",
      "45             print(\"reading file {}, now time is {}\".format(filename, datetime.now()))\n",
      "46             lines = []\n",
      "47             for line in reader:\n",
      "48                 line = line.replace('\\n', '[SEP]')\n",
      "49                 line = '[CLS]' + line\n",
      "50                 lines.append(line)\n",
      "51             \n",
      "52             single_file = ''.join(lines)\n",
      "53             single_ids = full_tokenizer.convert_tokens_to_ids(full_tokenizer._tokenize(single_file))\n",
      "54             # pdb.set_trace()\n",
      "55             with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'w') as f:\n",
      "56                 for id in single_ids[:-1]:\n",
      "57                     f.write(str(id) + ' ')\n",
      "58                 f.write(str(single_ids[-1]))\n",
      "59                 f.write('\\n')\n",
      "60         print('now time: {}, tokenized tokenized_train_{}.txt'.format(datetime.now(), i))\n",
      "61 \n",
      "62     print('finish')\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=18 and $.<=62)' train_single.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 确定训练过程中的其他参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在确定模型参数之后，再定义训练过程中的其他参数，如下为`tasks/doupo/train.sh`中该部分代码，这些参数包括：迭代的轮数`epochs`，训练时的`batch_size`，每隔多少轮输出一次NLL的`log_step`，以及使用哪些显卡的`device`。考虑到具体的硬件资源，此处使用两块显卡——0号和1号，以及设置batch size为32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 raw_data_path=$job_dir/rawdata/train.txt\n",
      "61 tokenizer_path=$job_dir/config/vocab.txt\n",
      "62 tokenized_data_path=$job_dir/tokenized/\n",
      "63 divide_path=$job_dir/divide/\n",
      "64 model_config=$job_dir/config/model_config.json\n",
      "65 epochs=30\n",
      "66 batch_size=32\n",
      "67 log_step=100\n",
      "68 output_dir=$job_dir/model/\n",
      "69 num_pieces=1\n",
      "70 \n",
      "71 if [ ! -e $job_dir/tokenized/tokenized_train_0.txt ]; then\n",
      "72     # tokenization then run the training\n",
      "73     python train_single.py \\\n",
      "74         --raw_data_path $raw_data_path \\\n",
      "75         --tokenizer_path $tokenizer_path \\\n",
      "76         --tokenized_data_path $tokenized_data_path \\\n",
      "77         --divide_path $divide_path \\\n",
      "78         --model_config $model_config \\\n",
      "79         --epochs $epochs \\\n",
      "80         --batch_size $batch_size \\\n",
      "81         --stride $stride \\\n",
      "82         --log_step $log_step \\\n",
      "83         --output_dir $output_dir \\\n",
      "84         --num_pieces $num_pieces \\\n",
      "85         --raw \\\n",
      "86         --ignore_intermediate_epoch_model\n",
      "87 else\n",
      "88     # run the training on the tokenized files\n",
      "89     python train_single.py \\\n",
      "90         --raw_data_path $raw_data_path \\\n",
      "91         --tokenizer_path $tokenizer_path \\\n",
      "92         --tokenized_data_path $tokenized_data_path \\\n",
      "93         --divide_path $divide_path \\\n",
      "94         --model_config $model_config \\\n",
      "95         --epochs $epochs \\\n",
      "96         --batch_size $batch_size \\\n",
      "97         --stride $stride \\\n",
      "98         --log_step $log_step \\\n",
      "99         --output_dir $output_dir \\\n",
      "100         --num_pieces $num_pieces \\\n",
      "101         --device 0,1 \\\n",
      "102         --ignore_intermediate_epoch_model\n",
      "103 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=60 and $.<=103)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定完毕模型参数和训练过程中的其他参数之后，可以进入到tasks/doupo文件夹，然后运行train.sh。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting config/vocab.txt and config/model_config.json\n",
      "I0416 01:14:42.333565 140639139563328 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=32, device='0,1', divide_path='tasks/doupo/divide/', epochs=30, fp16=False, fp16_opt_level='O1', gradient_accumulation=1, ignore_intermediate_epoch_model=True, log_step=100, lr=0.00015, max_grad_norm=1.0, model_config='tasks/doupo/config/model_config.json', num_pieces=1, output_dir='tasks/doupo/model/', pretrained_model='', raw=False, raw_data_path='tasks/doupo/rawdata/train.txt', segment=False, stride=256, tokenized_data_path='tasks/doupo/tokenized/', tokenizer_path='tasks/doupo/config/vocab.txt', warmup_steps=2000)\n",
      "config:\n",
      "{\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 512,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21163\n",
      "}\n",
      "\n",
      "using device: cuda\n",
      "calculating total steps\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "total steps = 19625\n",
      "Let's use 2 GPUs!\n",
      "/home/weijing/.conda/envs/weijing-torch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "starting training\n",
      "epoch 1\n",
      "time: 2020-04-16 01:14:48.601299\n",
      "/home/weijing/.conda/envs/weijing-torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "now time: 1:17. Step 100 of piece 0 of epoch 1, loss 9.202316961288453\n",
      "now time: 1:19. Step 200 of piece 0 of epoch 1, loss 7.707304573059082\n",
      "now time: 1:21. Step 300 of piece 0 of epoch 1, loss 6.231863975524902\n",
      "now time: 1:24. Step 400 of piece 0 of epoch 1, loss 5.415362277030945\n",
      "now time: 1:26. Step 500 of piece 0 of epoch 1, loss 4.971680083274841\n",
      "now time: 1:28. Step 600 of piece 0 of epoch 1, loss 4.667091364860535\n",
      "epoch 2\n",
      "time: 2020-04-16 01:30:03.152946\n",
      "now time: 1:32. Step 100 of piece 0 of epoch 2, loss 6.789287486076355\n",
      "now time: 1:34. Step 200 of piece 0 of epoch 2, loss 4.226721558570862\n",
      "now time: 1:37. Step 300 of piece 0 of epoch 2, loss 4.118758888244629\n",
      "now time: 1:39. Step 400 of piece 0 of epoch 2, loss 4.030298998355866\n",
      "now time: 1:41. Step 500 of piece 0 of epoch 2, loss 3.9456340169906614\n",
      "now time: 1:44. Step 600 of piece 0 of epoch 2, loss 3.8575130224227907\n",
      "epoch 3\n",
      "time: 2020-04-16 01:45:20.002546\n",
      "now time: 1:47. Step 100 of piece 0 of epoch 3, loss 5.745515451431275\n",
      "now time: 1:50. Step 200 of piece 0 of epoch 3, loss 3.6111994218826293\n",
      "now time: 1:52. Step 300 of piece 0 of epoch 3, loss 3.532945084571838\n",
      "now time: 1:54. Step 400 of piece 0 of epoch 3, loss 3.4371526789665223\n",
      "now time: 1:57. Step 500 of piece 0 of epoch 3, loss 3.349661066532135\n",
      "now time: 1:59. Step 600 of piece 0 of epoch 3, loss 3.2717025208473207\n",
      "epoch 4\n",
      "time: 2020-04-16 02:00:35.686268\n",
      "now time: 2:2. Step 100 of piece 0 of epoch 4, loss 4.848652067184449\n",
      "now time: 2:5. Step 200 of piece 0 of epoch 4, loss 3.0546720480918883\n",
      "now time: 2:7. Step 300 of piece 0 of epoch 4, loss 2.9983620500564574\n",
      "now time: 2:9. Step 400 of piece 0 of epoch 4, loss 2.9415189242362976\n",
      "now time: 2:12. Step 500 of piece 0 of epoch 4, loss 2.8896154952049256\n",
      "now time: 2:14. Step 600 of piece 0 of epoch 4, loss 2.853637628555298\n",
      "epoch 5\n",
      "time: 2020-04-16 02:15:52.982696\n",
      "now time: 2:18. Step 100 of piece 0 of epoch 5, loss 4.237938556671143\n",
      "now time: 2:20. Step 200 of piece 0 of epoch 5, loss 2.69880491733551\n",
      "now time: 2:22. Step 300 of piece 0 of epoch 5, loss 2.6723411226272584\n",
      "now time: 2:25. Step 400 of piece 0 of epoch 5, loss 2.6393399143218996\n",
      "now time: 2:27. Step 500 of piece 0 of epoch 5, loss 2.61063467502594\n",
      "now time: 2:29. Step 600 of piece 0 of epoch 5, loss 2.590463788509369\n",
      "epoch 6\n",
      "time: 2020-04-16 02:31:08.000836\n",
      "now time: 2:33. Step 100 of piece 0 of epoch 6, loss 3.8711335635185242\n",
      "now time: 2:35. Step 200 of piece 0 of epoch 6, loss 2.4737066078186034\n",
      "now time: 2:38. Step 300 of piece 0 of epoch 6, loss 2.4557159829139708\n",
      "now time: 2:40. Step 400 of piece 0 of epoch 6, loss 2.4382388758659364\n",
      "now time: 2:42. Step 500 of piece 0 of epoch 6, loss 2.421723482608795\n",
      "now time: 2:45. Step 600 of piece 0 of epoch 6, loss 2.3982063722610474\n",
      "epoch 7\n",
      "time: 2020-04-16 02:46:22.395491\n",
      "now time: 2:48. Step 100 of piece 0 of epoch 7, loss 3.5847596406936644\n",
      "now time: 2:51. Step 200 of piece 0 of epoch 7, loss 2.2856746673583985\n",
      "now time: 2:53. Step 300 of piece 0 of epoch 7, loss 2.2785504913330077\n",
      "now time: 2:55. Step 400 of piece 0 of epoch 7, loss 2.2767979526519775\n",
      "now time: 2:58. Step 500 of piece 0 of epoch 7, loss 2.2672856426239014\n",
      "now time: 3:0. Step 600 of piece 0 of epoch 7, loss 2.2545839881896974\n",
      "epoch 8\n",
      "time: 2020-04-16 03:01:38.067122\n",
      "now time: 3:3. Step 100 of piece 0 of epoch 8, loss 3.352515172958374\n",
      "now time: 3:6. Step 200 of piece 0 of epoch 8, loss 2.1405993843078615\n",
      "now time: 3:8. Step 300 of piece 0 of epoch 8, loss 2.1453472924232484\n",
      "now time: 3:10. Step 400 of piece 0 of epoch 8, loss 2.1394610095024107\n",
      "now time: 3:13. Step 500 of piece 0 of epoch 8, loss 2.124991798400879\n",
      "now time: 3:15. Step 600 of piece 0 of epoch 8, loss 2.1123932445049287\n",
      "epoch 9\n",
      "time: 2020-04-16 03:16:51.998176\n",
      "now time: 3:19. Step 100 of piece 0 of epoch 9, loss 3.136663844585419\n",
      "now time: 3:21. Step 200 of piece 0 of epoch 9, loss 2.004424078464508\n",
      "now time: 3:23. Step 300 of piece 0 of epoch 9, loss 2.0003432142734527\n",
      "now time: 3:26. Step 400 of piece 0 of epoch 9, loss 2.0081939208507538\n",
      "now time: 3:28. Step 500 of piece 0 of epoch 9, loss 1.9936890637874602\n",
      "now time: 3:30. Step 600 of piece 0 of epoch 9, loss 1.9920781135559082\n",
      "epoch 10\n",
      "time: 2020-04-16 03:32:05.659645\n",
      "now time: 3:34. Step 100 of piece 0 of epoch 10, loss 2.9409005546569826\n",
      "now time: 3:36. Step 200 of piece 0 of epoch 10, loss 1.873287514448166\n",
      "now time: 3:39. Step 300 of piece 0 of epoch 10, loss 1.8740946745872498\n",
      "now time: 3:41. Step 400 of piece 0 of epoch 10, loss 1.8776152431964874\n",
      "now time: 3:43. Step 500 of piece 0 of epoch 10, loss 1.8794782698154449\n",
      "now time: 3:46. Step 600 of piece 0 of epoch 10, loss 1.8748361718654634\n",
      "epoch 11\n",
      "time: 2020-04-16 03:47:19.714032\n",
      "now time: 3:49. Step 100 of piece 0 of epoch 11, loss 2.7505292212963104\n",
      "now time: 3:51. Step 200 of piece 0 of epoch 11, loss 1.7488766956329345\n",
      "now time: 3:54. Step 300 of piece 0 of epoch 11, loss 1.7564728021621705\n",
      "now time: 3:56. Step 400 of piece 0 of epoch 11, loss 1.7629074132442475\n",
      "now time: 3:58. Step 500 of piece 0 of epoch 11, loss 1.7594583714008332\n",
      "now time: 4:1. Step 600 of piece 0 of epoch 11, loss 1.7469587874412538\n",
      "epoch 12\n",
      "time: 2020-04-16 04:02:33.384908\n",
      "now time: 4:4. Step 100 of piece 0 of epoch 12, loss 2.5669286179542543\n",
      "now time: 4:7. Step 200 of piece 0 of epoch 12, loss 1.6365967988967896\n",
      "now time: 4:9. Step 300 of piece 0 of epoch 12, loss 1.6398653757572175\n",
      "now time: 4:11. Step 400 of piece 0 of epoch 12, loss 1.6442046630382539\n",
      "now time: 4:14. Step 500 of piece 0 of epoch 12, loss 1.642245318889618\n",
      "now time: 4:16. Step 600 of piece 0 of epoch 12, loss 1.6417443752288818\n",
      "epoch 13\n",
      "time: 2020-04-16 04:17:44.574765\n",
      "now time: 4:20. Step 100 of piece 0 of epoch 13, loss 2.4030781209468843\n",
      "now time: 4:22. Step 200 of piece 0 of epoch 13, loss 1.5256192553043366\n",
      "now time: 4:24. Step 300 of piece 0 of epoch 13, loss 1.5365435802936553\n",
      "now time: 4:27. Step 400 of piece 0 of epoch 13, loss 1.5389688789844513\n",
      "now time: 4:29. Step 500 of piece 0 of epoch 13, loss 1.5388435673713685\n",
      "now time: 4:31. Step 600 of piece 0 of epoch 13, loss 1.537399308681488\n",
      "epoch 14\n",
      "time: 2020-04-16 04:32:55.731805\n",
      "now time: 4:35. Step 100 of piece 0 of epoch 14, loss 2.245744732618332\n",
      "now time: 4:37. Step 200 of piece 0 of epoch 14, loss 1.4283682191371918\n",
      "now time: 4:39. Step 300 of piece 0 of epoch 14, loss 1.439707955121994\n",
      "now time: 4:42. Step 400 of piece 0 of epoch 14, loss 1.4385538804531097\n",
      "now time: 4:44. Step 500 of piece 0 of epoch 14, loss 1.4388358771800995\n",
      "now time: 4:46. Step 600 of piece 0 of epoch 14, loss 1.4463050270080566\n",
      "epoch 15\n",
      "time: 2020-04-16 04:48:07.590364\n",
      "now time: 4:50. Step 100 of piece 0 of epoch 15, loss 2.1014110934734345\n",
      "now time: 4:52. Step 200 of piece 0 of epoch 15, loss 1.3430095541477203\n",
      "now time: 4:55. Step 300 of piece 0 of epoch 15, loss 1.3440466809272766\n",
      "now time: 4:57. Step 400 of piece 0 of epoch 15, loss 1.3469557309150695\n",
      "now time: 4:59. Step 500 of piece 0 of epoch 15, loss 1.3533795082569122\n",
      "now time: 5:2. Step 600 of piece 0 of epoch 15, loss 1.350544846057892\n",
      "epoch 16\n",
      "time: 2020-04-16 05:03:21.359050\n",
      "now time: 5:5. Step 100 of piece 0 of epoch 16, loss 1.9660143125057221\n",
      "now time: 5:8. Step 200 of piece 0 of epoch 16, loss 1.2541267347335816\n",
      "now time: 5:10. Step 300 of piece 0 of epoch 16, loss 1.259226269721985\n",
      "now time: 5:12. Step 400 of piece 0 of epoch 16, loss 1.2658753824234008\n",
      "now time: 5:14. Step 500 of piece 0 of epoch 16, loss 1.2690154457092284\n",
      "now time: 5:17. Step 600 of piece 0 of epoch 16, loss 1.2719350898265838\n",
      "epoch 17\n",
      "time: 2020-04-16 05:18:33.537657\n",
      "now time: 5:20. Step 100 of piece 0 of epoch 17, loss 1.8483317708969116\n",
      "now time: 5:23. Step 200 of piece 0 of epoch 17, loss 1.1701875877380372\n",
      "now time: 5:25. Step 300 of piece 0 of epoch 17, loss 1.1864917719364165\n",
      "now time: 5:27. Step 400 of piece 0 of epoch 17, loss 1.1902322900295257\n",
      "now time: 5:30. Step 500 of piece 0 of epoch 17, loss 1.191501111984253\n",
      "now time: 5:32. Step 600 of piece 0 of epoch 17, loss 1.1947383069992066\n",
      "epoch 18\n",
      "time: 2020-04-16 05:33:44.124617\n",
      "now time: 5:36. Step 100 of piece 0 of epoch 18, loss 1.7342723166942597\n",
      "now time: 5:38. Step 200 of piece 0 of epoch 18, loss 1.1028614771366119\n",
      "now time: 5:40. Step 300 of piece 0 of epoch 18, loss 1.109117056131363\n",
      "now time: 5:43. Step 400 of piece 0 of epoch 18, loss 1.118437031507492\n",
      "now time: 5:45. Step 500 of piece 0 of epoch 18, loss 1.1209260213375092\n",
      "now time: 5:47. Step 600 of piece 0 of epoch 18, loss 1.124591029882431\n",
      "epoch 19\n",
      "time: 2020-04-16 05:48:56.031032\n",
      "now time: 5:51. Step 100 of piece 0 of epoch 19, loss 1.6284554076194764\n",
      "now time: 5:53. Step 200 of piece 0 of epoch 19, loss 1.0364246493577958\n",
      "now time: 5:55. Step 300 of piece 0 of epoch 19, loss 1.0447640657424926\n",
      "now time: 5:58. Step 400 of piece 0 of epoch 19, loss 1.0529798078536987\n",
      "now time: 6:0. Step 500 of piece 0 of epoch 19, loss 1.0552983176708222\n",
      "now time: 6:2. Step 600 of piece 0 of epoch 19, loss 1.0555978977680207\n",
      "epoch 20\n",
      "time: 2020-04-16 06:04:08.537717\n",
      "now time: 6:6. Step 100 of piece 0 of epoch 20, loss 1.5358845353126527\n",
      "now time: 6:8. Step 200 of piece 0 of epoch 20, loss 0.9747134763002395\n",
      "now time: 6:11. Step 300 of piece 0 of epoch 20, loss 0.9841806548833847\n",
      "now time: 6:13. Step 400 of piece 0 of epoch 20, loss 0.9879507029056549\n",
      "now time: 6:15. Step 500 of piece 0 of epoch 20, loss 0.9952266401052475\n",
      "now time: 6:18. Step 600 of piece 0 of epoch 20, loss 0.9997190886735916\n",
      "epoch 21\n",
      "time: 2020-04-16 06:19:19.802103\n",
      "now time: 6:21. Step 100 of piece 0 of epoch 21, loss 1.4491075098514556\n",
      "now time: 6:23. Step 200 of piece 0 of epoch 21, loss 0.9256779366731643\n",
      "now time: 6:26. Step 300 of piece 0 of epoch 21, loss 0.9296008312702179\n",
      "now time: 6:28. Step 400 of piece 0 of epoch 21, loss 0.9329792124032974\n",
      "now time: 6:30. Step 500 of piece 0 of epoch 21, loss 0.9387668401002884\n",
      "now time: 6:33. Step 600 of piece 0 of epoch 21, loss 0.9413994669914245\n",
      "epoch 22\n",
      "time: 2020-04-16 06:34:32.006479\n",
      "now time: 6:36. Step 100 of piece 0 of epoch 22, loss 1.369708793759346\n",
      "now time: 6:39. Step 200 of piece 0 of epoch 22, loss 0.8698766320943833\n",
      "now time: 6:41. Step 300 of piece 0 of epoch 22, loss 0.879535500407219\n",
      "now time: 6:43. Step 400 of piece 0 of epoch 22, loss 0.882276793718338\n",
      "now time: 6:46. Step 500 of piece 0 of epoch 22, loss 0.8877784383296966\n",
      "now time: 6:48. Step 600 of piece 0 of epoch 22, loss 0.8892823547124863\n",
      "epoch 23\n",
      "time: 2020-04-16 06:49:43.700830\n",
      "now time: 6:52. Step 100 of piece 0 of epoch 23, loss 1.3021733957529067\n",
      "now time: 6:54. Step 200 of piece 0 of epoch 23, loss 0.8236924302577973\n",
      "now time: 6:56. Step 300 of piece 0 of epoch 23, loss 0.835443131327629\n",
      "now time: 6:59. Step 400 of piece 0 of epoch 23, loss 0.8385501962900161\n",
      "now time: 7:1. Step 500 of piece 0 of epoch 23, loss 0.8440354722738266\n",
      "now time: 7:3. Step 600 of piece 0 of epoch 23, loss 0.8432110524177552\n",
      "epoch 24\n",
      "time: 2020-04-16 07:04:55.318724\n",
      "now time: 7:7. Step 100 of piece 0 of epoch 24, loss 1.23594005048275\n",
      "now time: 7:9. Step 200 of piece 0 of epoch 24, loss 0.784839220046997\n",
      "now time: 7:11. Step 300 of piece 0 of epoch 24, loss 0.795030711889267\n",
      "now time: 7:14. Step 400 of piece 0 of epoch 24, loss 0.7949240291118622\n",
      "now time: 7:16. Step 500 of piece 0 of epoch 24, loss 0.7992287290096283\n",
      "now time: 7:18. Step 600 of piece 0 of epoch 24, loss 0.8026183515787124\n",
      "epoch 25\n",
      "time: 2020-04-16 07:20:06.546957\n",
      "now time: 7:22. Step 100 of piece 0 of epoch 25, loss 1.1748134887218475\n",
      "now time: 7:24. Step 200 of piece 0 of epoch 25, loss 0.7487787473201751\n",
      "now time: 7:27. Step 300 of piece 0 of epoch 25, loss 0.7541225963830948\n",
      "now time: 7:29. Step 400 of piece 0 of epoch 25, loss 0.7591692370176315\n",
      "now time: 7:31. Step 500 of piece 0 of epoch 25, loss 0.7653062713146209\n",
      "now time: 7:34. Step 600 of piece 0 of epoch 25, loss 0.7620276564359665\n",
      "epoch 26\n",
      "time: 2020-04-16 07:35:15.116928\n",
      "now time: 7:37. Step 100 of piece 0 of epoch 26, loss 1.1271157205104827\n",
      "now time: 7:39. Step 200 of piece 0 of epoch 26, loss 0.7196405810117722\n",
      "now time: 7:42. Step 300 of piece 0 of epoch 26, loss 0.7242998188734054\n",
      "now time: 7:44. Step 400 of piece 0 of epoch 26, loss 0.725955114364624\n",
      "now time: 7:46. Step 500 of piece 0 of epoch 26, loss 0.725811847448349\n",
      "now time: 7:49. Step 600 of piece 0 of epoch 26, loss 0.7297996699810028\n",
      "epoch 27\n",
      "time: 2020-04-16 07:50:25.795469\n",
      "now time: 7:52. Step 100 of piece 0 of epoch 27, loss 1.07976269364357\n",
      "now time: 7:55. Step 200 of piece 0 of epoch 27, loss 0.6927031743526458\n",
      "now time: 7:57. Step 300 of piece 0 of epoch 27, loss 0.6950196087360382\n",
      "now time: 7:59. Step 400 of piece 0 of epoch 27, loss 0.6976888173818588\n",
      "now time: 8:2. Step 500 of piece 0 of epoch 27, loss 0.6997041636705399\n",
      "now time: 8:4. Step 600 of piece 0 of epoch 27, loss 0.6983793312311173\n",
      "epoch 28\n",
      "time: 2020-04-16 08:05:36.768842\n",
      "now time: 8:7. Step 100 of piece 0 of epoch 28, loss 1.041203372478485\n",
      "now time: 8:10. Step 200 of piece 0 of epoch 28, loss 0.6684932935237885\n",
      "now time: 8:12. Step 300 of piece 0 of epoch 28, loss 0.6704179030656815\n",
      "now time: 8:14. Step 400 of piece 0 of epoch 28, loss 0.6714022094011307\n",
      "now time: 8:17. Step 500 of piece 0 of epoch 28, loss 0.6705535060167312\n",
      "now time: 8:19. Step 600 of piece 0 of epoch 28, loss 0.6737305068969727\n",
      "epoch 29\n",
      "time: 2020-04-16 08:20:47.409262\n",
      "now time: 8:23. Step 100 of piece 0 of epoch 29, loss 1.0074528831243514\n",
      "now time: 8:25. Step 200 of piece 0 of epoch 29, loss 0.648998013138771\n",
      "now time: 8:27. Step 300 of piece 0 of epoch 29, loss 0.6515571510791779\n",
      "now time: 8:30. Step 400 of piece 0 of epoch 29, loss 0.6512559533119202\n",
      "now time: 8:32. Step 500 of piece 0 of epoch 29, loss 0.652387506365776\n",
      "now time: 8:34. Step 600 of piece 0 of epoch 29, loss 0.6522213840484619\n",
      "epoch 30\n",
      "time: 2020-04-16 08:35:58.999761\n",
      "now time: 8:38. Step 100 of piece 0 of epoch 30, loss 0.986364706158638\n",
      "now time: 8:40. Step 200 of piece 0 of epoch 30, loss 0.6341836673021316\n",
      "now time: 8:42. Step 300 of piece 0 of epoch 30, loss 0.6344272303581238\n",
      "now time: 8:45. Step 400 of piece 0 of epoch 30, loss 0.635601818561554\n",
      "now time: 8:47. Step 500 of piece 0 of epoch 30, loss 0.6358217197656632\n",
      "now time: 8:49. Step 600 of piece 0 of epoch 30, loss 0.6335765486955642\n",
      "training finished\n",
      "I0416 08:51:11.429557 140639139563328 configuration_utils.py:71] Configuration saved in tasks/doupo/model/final_model/config.json\n",
      "I0416 08:51:12.115916 140639139563328 modeling_utils.py:205] Model weights saved in tasks/doupo/model/final_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; bash train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型保存于`tasks/doupo/model/final_model`中，该文件夹下有两个文件，`config.json`，`pytorch_model.bin`。在`config.json`中包含了模型的基本参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 344M\n",
      "-rw-rw-r--  1 weijing weijing 344M Apr 16 08:51 pytorch_model.bin\n",
      "-rw-rw-r--  1 weijing weijing  596 Apr 16 08:51 config.json\n",
      "drwxrwxr-x  2 weijing weijing 4.0K Mar 31 06:32 .\n",
      "drwxrwxr-x 33 weijing weijing 4.0K Mar 31 06:32 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -laht tasks/doupo/model/final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch_model.bin`可以被直接加载，并能够输出模型结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0416 10:41:52.995795 140436257081152 configuration_utils.py:148] loading configuration file tasks/doupo/model/final_model/config.json\n",
      "I0416 10:41:52.998218 140436257081152 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 512,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21163\n",
      "}\n",
      "\n",
      "I0416 10:41:52.999756 140436257081152 modeling_utils.py:334] loading weights file tasks/doupo/model/final_model/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(21163, 768)\n",
      "    (wpe): Embedding(512, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=21163, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\"tasks/doupo/model/final_model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若要查看模型中的参数个数，可以使用如下命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the model is 87526656\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"The number of parameters in the model is %s\" % pytorch_total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在训练好的模型上做Inference，生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate.py`脚本使用top-k sampling生成文本。`tasks/doupo/generate.sh`定义了top-k sampling的参数，如下所示，topk=50，nsamples=10，temperature=0.8，length=18，prefix=[SEP][CLS]萧炎。topk越小，多样性越低。temperature $T$也定义了多样性：取到词表中第i个词的概率由logits向量$z_{1:V}$和温度T决定, $q_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$。具体的，T越大，向量$z_{1:V}$中各分量的绝对值就越趋向于0，造成多样性提高而采样质量下降；T越小，则各分量就越远离0，多样性下降而质量提升。一般的，top-k sampling中使用的T为0.8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tjob_dir=\"tasks/doupo\"\n",
      "     2\t\n",
      "     3\tcd ../..\n",
      "     4\t\n",
      "     5\tif [ ! -e $job_dir/outputs ]; then\n",
      "     6\t    mkdir $job_dir/outputs\n",
      "     7\tfi\n",
      "     8\t\n",
      "     9\tpython generate.py \\\n",
      "    10\t    --device 0 \\\n",
      "    11\t    --model_path $job_dir/model/final_model/ \\\n",
      "    12\t    --model_config $job_dir/model/final_model/model_config.json \\\n",
      "    13\t    --tokenizer_path $job_dir/config/vocab.txt \\\n",
      "    14\t    --temperature 0.8 \\\n",
      "    15\t    --prefix [SEP][CLS]萧炎 \\\n",
      "    16\t    --length 100 \\\n",
      "    17\t    --topk 50 \\\n",
      "    18\t    --nsamples 10 \\\n",
      "    19\t    --save_samples \\\n",
      "    20\t    --save_samples_path $job_dir/outputs/\n"
     ]
    }
   ],
   "source": [
    "!cat -n tasks/doupo/generate.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0416 09:33:09.533224 139695085782848 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=1, device='0', fast_pattern=False, length=100, model_config='tasks/doupo/model/final_model/model_config.json', model_path='tasks/doupo/model/final_model/', no_wordpiece=False, nsamples=10, prefix='[SEP][CLS]萧炎', repetition_penalty=1.0, save_samples=True, save_samples_path='tasks/doupo/outputs/', segment=False, temperature=0.8, tokenizer_path='tasks/doupo/config/vocab.txt', topk=50, topp=0)\n",
      "I0416 09:33:09.818720 139695085782848 configuration_utils.py:148] loading configuration file tasks/doupo/model/final_model/config.json\n",
      "I0416 09:33:09.819159 139695085782848 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 512,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21163\n",
      "}\n",
      "\n",
      "I0416 09:33:09.819901 139695085782848 modeling_utils.py:334] loading weights file tasks/doupo/model/final_model/pytorch_model.bin\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 142.36it/s]\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎目光平淡的望着那些闪掠而来的黑影，从纳戒中取出一块玉牌，玉牌之上，隐隐间渗透着一丝诡异紫色火焰。[SEP][CLS]那些细小的火焰缓缓飘荡在伤疤之上，整个房间之中，都是泛着一点点的光泽。[SEP][CLS]萧炎目光眨也不眨的盯着玉牌\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 144.02it/s]\n",
      "======================================== SAMPLE 2 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎微微点头，他并没有与魂殿有太大的恩怨，既然如此，也只能将这两次的麻烦解决。[SEP][CLS]“既然如此，那便现身，若是再留意的话，或许便是可以试试能否将你二人请到与菩提化体涎交予我。”[SEP][CLS]见到萧炎并未在意，丹塔老祖\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 146.70it/s]\n",
      "======================================== SAMPLE 3 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎手掌刚刚握下“天妖凰族的王八蛋”[SEP][CLS]心头松了一口气，眼中也是掠过一抹凝重，果然是名不虚传冉雨的斗技，而且还是真正的王族血脉，这等存在，方才是真正的血脉，在萧炎手中，也是真正的能够竞得上天妖凰族。[SEP][CLS]“\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 146.62it/s]\n",
      "======================================== SAMPLE 4 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎面色凝重，这种时候，他也是首次选择了手脚，未曾料到，这一次，是他的竞争对手，以前的速度虽略微有些狼狈，但也是完全不是他所希望的，当下只能一咬牙，速度急忙加快。[SEP][CLS]在萧炎加入掉曹颖与丹轩之前，曹颖，宋清\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 146.68it/s]\n",
      "======================================== SAMPLE 5 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎的身形，在无数道震惊目光注视中，直接是诡异消失[SEP][CLS]原本高达几十米的北方天际之上，一闪间，便是消失在了天际之边。[SEP][CLS]“嘭！”[SEP][CLS]翎泉脚掌刚刚落地，一道低沉闷响突然自城外传来，旋即一道身影从城市之外暴掠而\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 146.10it/s]\n",
      "======================================== SAMPLE 6 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎的目光，忽然转向一旁的石台，那里，一位身穿白衣的青年，正脸带微笑的缓缓走来，在他的面前，摆放着一个小袋，笑眯眯的望着他。[SEP][CLS]“这位老先生，如果您所说的是他的体内火毒，这些小女孩的体内正统，而且还全部都\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 139.50it/s]\n",
      "======================================== SAMPLE 7 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎笑了笑，也不在这个话题上继续纠缠，拱手道：“多谢姚坊主帮忙，今天的事，交给我便好。”[SEP][CLS]“姚坊主，我来此处，是你自己介绍的地方，多加我与大哥让得你再次共享这换丹药的资格。”见到萧炎摇头，那姚坊主却是摆\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 134.48it/s]\n",
      "======================================== SAMPLE 8 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎手掌轻轻碰撞，这股巨大的暗劲风涟漪，犹如涟漪一般，从那乱石堆中扩散而出，一时间，这片混乱战场，便是陷入了混乱的战场中。[SEP][CLS]广场边缘处，薰儿纤手一开飘落额前的青丝，淡淡的声音，在全场回荡着：“没想到这妮\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 134.20it/s]\n",
      "======================================== SAMPLE 9 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎的目光，并未因为那黑甲身影的半点而有所停留，他的目光，死死的锁定着那不断蠕动的身体，但其他的那种熟悉，却是令得他的灵魂都是有种被绞碎的感觉，他明白，这是灵魂的主人！[SEP][CLS]“是灵魂力量体？”[SEP][CLS]萧炎的灵魂\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 133.47it/s]\n",
      "======================================== SAMPLE 10 ========================================\n",
      "\n",
      "[SEP][CLS]萧炎笑了笑，也不说话，直接将话题转移到了最为激烈的大厅，笑吟吟的宣布着打滚的地方，今天的事，照旧围确是萧家的后辈，日后他们再次为萧家做了嫁衣，照料她回复了家族在加玛帝国的地位，日后若是还有机会，也尽数去抓\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo;bash generate.sh;cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述文本生成结果保存于`tasks/doupo/generate.sh`中定义的`$save_samples_path`文件夹下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调GPT2模型进行数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节描述如何通过加载预训练模型，然后在较小的训练集上微调之后获得一个新的语言模型，并用于数据增强，为较少的训练样本提供模型生成的“数据增强”样本，其用途包括但不限定于为多轮对话中的意图识别训练样本进行数据增强，仿写具有某种语言风格的短小的文本。我们以对[海子的诗歌](https://github.com/sheepzh/poetry/tree/master/data/origin/%E6%B5%B7%E5%AD%90_haizi)进行数据增强为例进行说明这部分的使用方法：加载[散文预训练模型](https://www.dropbox.com/s/yqxuu6fszqto4od/pytorch_model.bin?dl=0)，然后使用海子的诗歌进行微调，最后使用海子的语言风格进行仿写。意图识别训练样本的数据增强也可以同理进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的文件如下所示，`download.py`用于下载训练语料，`format_raw_txt.py`用于一定的数据处理，将处于不同文件中的语料合并到一个文件中，`finetune.sh`用于进行微调，作为对比`train_from_scratch.sh`用于在小样本训练集上从头训练一个语言模型。`generate_from_finetuned.sh`用于数据增强，`generate_from_scratch.sh`使用从头训练的语言模型进行文本生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtasks/dataaugmentation\u001b[00m\n",
      "├── download.py\n",
      "├── finetune.sh\n",
      "├── format_raw_txt.py\n",
      "├── generate_from_finetuned.sh\n",
      "├── generate_from_scratch.sh\n",
      "├── \u001b[01;34mrawdata\u001b[00m\n",
      "│   └── poem_list.txt\n",
      "└── train_from_scratch.sh\n",
      "\n",
      "1 directory, 7 files\n"
     ]
    }
   ],
   "source": [
    "!tree tasks/dataaugmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`download.py`读取`rawdata/poem_list.txt`，并下载海子的诗歌到`rawdata`文件夹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'wget*': No such file or directory\n",
      "\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log’.\n",
      "\n",
      "Redirecting output to ‘wget-log.1’.\n",
      "\n",
      "Redirecting output to ‘wget-log.2’.\n",
      "\n",
      "Redirecting output to ‘wget-log.3’.\n",
      "\n",
      "Redirecting output to ‘wget-log.4’.\n",
      "\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.5’.\n",
      "\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.6’.\n",
      "\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.8’.\n",
      "\n",
      "Redirecting output to ‘wget-log.9’.\n",
      "\n",
      "Redirecting output to ‘wget-log.10’.\n",
      "\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.12’.\n",
      "\n",
      "Redirecting output to ‘wget-log.13’.\n",
      "\n",
      "Redirecting output to ‘wget-log.14’.\n",
      "\n",
      "Redirecting output to ‘wget-log.15’.\n",
      "\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.16’.\n",
      "\n",
      "Redirecting output to ‘wget-log.17’.\n",
      "\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.18’.\n",
      "\n",
      "Redirecting output to ‘wget-log.19’.\n",
      "\n",
      "Redirecting output to ‘wget-log.20’.\n",
      "\n",
      "Redirecting output to ‘wget-log.7’.\n",
      "\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.11’.\n",
      "\n",
      "SIGHUP received.\n",
      "Redirecting output to ‘wget-log.21’.\n",
      "\n",
      "Redirecting output to ‘wget-log.22’.\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; python download.py rawdata; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ls -laht tasks/dataaugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'wget-log*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; rm wget-log*; cd ../..; #去除多余的wget-log*日志"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载之后，共有143首诗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 weijing weijing  400 Apr 16 14:24 rawdata/莲界慈航.pt\n",
      "-rw-rw-r-- 1 weijing weijing  829 Apr 16 14:24 rawdata/盲目——给维特根施坦.pt\n",
      "-rw-rw-r-- 1 weijing weijing  824 Apr 16 14:24 rawdata/黑风.pt\n",
      "-rw-rw-r-- 1 weijing weijing  364 Apr 16 14:24 rawdata/自杀者之歌.pt\n",
      "-rw-rw-r-- 1 weijing weijing  441 Apr 16 14:24 rawdata/敦煌.pt\n",
      "-rw-rw-r-- 1 weijing weijing  429 Apr 16 14:24 rawdata/黄金草原.pt\n",
      "-rw-rw-r-- 1 weijing weijing  517 Apr 16 14:24 rawdata/雨鞋.pt\n",
      "-rw-rw-r-- 1 weijing weijing  337 Apr 16 14:24 rawdata/黎明：一首小诗.pt\n",
      "-rw-rw-r-- 1 weijing weijing  261 Apr 16 14:24 rawdata/魂曲.pt\n",
      "-rw-rw-r-- 1 weijing weijing  773 Apr 16 14:24 rawdata/马、火、灰——鼎.pt\n",
      "ls: write error: Broken pipe\n",
      "...\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; ls -laht rawdata/*.pt | head -n 10; echo \"...\"; ls -laht rawdata/*.pt | wc -l; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始文件中，诗歌自然分行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:莲界慈航\n",
      "date:198505\n",
      "\n",
      "七叶树下\n",
      "九根香\n",
      "照见菩萨的\n",
      "第一次失恋\n",
      "\n",
      "你盘坐莲花\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; head -n 10 rawdata/莲界慈航.pt; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好的训练，我们使用`format_raw_txt.py`进行处理，让一首诗独占一行。我们使用[MASK]来代替回车符，使用[unused1]来代替空格符\" \"，其中[unused1]出现在bert chinese的字表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t'''\n",
      "     2\tturn the following file into a line in train.txt\n",
      "     3\ttitle:月\n",
      "     4\tdate:\n",
      "     5\t\n",
      "     6\t炊烟上下\n",
      "     7\t月亮是掘井的白猿\n",
      "     8\t月亮是惨笑的河流上的白猿\n",
      "     9\t\n",
      "    10\t多少回天上的伤口淌血\n",
      "    11\t白猿流过钟楼\n",
      "    12\t流过南方老人的头顶\n",
      "    13\t\n",
      "    14\t掘井的白猿\n",
      "    15\t村庄喂养的白猿\n",
      "    16\t月亮是惨笑的白猿\n",
      "    17\t月亮自己心碎\n",
      "    18\t月亮早已心碎\n",
      "    19\t\n",
      "    20\t==>\n",
      "    21\t月[MASK][MASK]炊烟上下[MASK]月亮是掘井的白猿[MASK]月亮是惨笑的河流上的白猿[MASK][MASK]多少回天上的伤口淌血...\n",
      "    22\tRule 1: replace \\n with [MASK]\n",
      "    23\tRule 2: replace the whitespace \" \" with [unused1] \n",
      "    24\t'''\n",
      "    25\t\n",
      "    26\timport os\n",
      "    27\timport sys\n",
      "    28\timport pdb\n",
      "    29\t\n",
      "    30\tdef format(foldername):\n",
      "    31\t    files = [f for f in os.listdir(foldername) if \".pt\" in f]\n",
      "    32\t    # pdb.set_trace()\n",
      "    33\t    with open(foldername+\"/\"+\"train.txt\", \"w\") as fWriter:\n",
      "    34\t        for filename in files:\n",
      "    35\t            result = \"\"\n",
      "    36\t            with open(foldername+\"/\"+filename, \"r\") as fIn:\n",
      "    37\t                for line in fIn:\n",
      "    38\t                    if(\"title:\" in line):\n",
      "    39\t                        result += line.strip(\"title:\").replace(\"\\n\", \"[MASK]\")\n",
      "    40\t                    elif(\"date:\" in line):\n",
      "    41\t                        result += \"[MASK]\"\n",
      "    42\t                    else:\n",
      "    43\t                        result += line.replace(\"\\n\", \"[MASK]\").replace(\" \", \"[unused1]\")\n",
      "    44\t            fWriter.write(\"%s\\n\" % result)\n",
      "    45\t\n",
      "    46\tdef test():\n",
      "    47\t    format(\"rawdata\")\n",
      "    48\t\n",
      "    49\t\n",
      "    50\tif __name__==\"__main__\":\n",
      "    51\t    if(len(sys.argv)!=2):\n",
      "    52\t        print(\"usage: python format_raw_txt.py target_folder\")\n",
      "    53\t        sys.exit(0)\n",
      "    54\t    else:\n",
      "    55\t        target_folder = sys.argv[1]\n",
      "    56\t        format(target_folder)\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; cat -n format_raw_txt.py; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了能在`train.txt`的不同的行（即不同的文本）之间进行区分，我们使用[CLS]来表示一行的开头，[SEP]表示一行的结束，如果不足n_ctx，那么就用[PAD]来补齐。例如，line1字符数超过n_ctx-2，line2和line3则未超过，那么：`line1\\nline2\\nline3` => '[CLS]line1(trimmed)[SEP][CLS]line2[SEP][PAD]...[PAD][CLS]line3[SEP][PAD]...[PAD]'。这一处理过程体现于`train_on_small_file.py`的第102行到116行之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102     print('start preparing data')\n",
      "103     contents = []\n",
      "104     for line in lines:\n",
      "105         line = line.strip()\n",
      "106         if(len(line)>(n_ctx-2)):\n",
      "107             line = line[0:(n_ctx-2)] # trim out very long sequences\n",
      "108         contents.append(full_tokenizer.convert_tokens_to_ids(full_tokenizer._tokenize(line)))\n",
      "109     tokens = []\n",
      "110     for content in contents:\n",
      "111         token = []\n",
      "112         token.append(full_tokenizer.convert_tokens_to_ids('[CLS]'))\n",
      "113         token.extend(content)\n",
      "114         token.append(full_tokenizer.convert_tokens_to_ids('[SEP]'))\n",
      "115         token.extend(full_tokenizer.convert_tokens_to_ids(['[PAD]'])*(n_ctx-len(token)) )\n",
      "116         tokens.append(token)\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=102 and $.<=116)' train_on_small_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，由于一个样本占据一个n_ctx长度的输入，因此样本和样本之间是完全独立的，random shuffle之后的训练质量也较好。如下代码所示，每一个`batch`取了`batch_size`个样本进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118     print('starting training')\n",
      "119     running_loss = 0\n",
      "120     for epoch in range(epochs):\n",
      "121         print('epoch {}'.format(epoch + 1))\n",
      "122         now = datetime.now()\n",
      "123         print('time: {}'.format(now))\n",
      "124         samples = tokens\n",
      "125         random.shuffle(samples)\n",
      "126         for step in range(len(samples) // batch_size): # drop last\n",
      "127 \n",
      "128             #  prepare data\n",
      "129             batch = samples[step * batch_size: (step + 1) * batch_size]\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=118 and $.<=129)' train_on_small_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`finetune.sh`脚本中，创建pretrained_model文件夹，并将[散文预训练模型](https://drive.google.com/drive/folders/1rJC4niJKMVwixUQkuL9k5teLRnEYTmUf)保存于该文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 if [ ! -e $job_dir/pretrained_model ]; then\n",
      "14     mkdir $job_dir/pretrained_model\n",
      "15     wget -c -O $job_dir/pretrained_model/pytorch_model.bin https://www.dropbox.com/s/yqxuu6fszqto4od/pytorch_model.bin?dl=0\n",
      "16     wget -c -O $job_dir/pretrained_model/config.json https://www.dropbox.com/s/7z599ixdzyghkth/config.json?dl=0 \n",
      "17     wget -c -O $job_dir/pretrained_model/vocab.txt https://www.dropbox.com/s/9obd0qadtst347l/vocab.txt?dl=0 \n",
      "18 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; perl -ne 'print \"$. $_\" if ($.>=13 and $.<=18)' finetune.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于需要数据增强的文本规模较小，因此不需要分割文件和提前保存好tokenization的结果。我们可以直接确定好训练参数，然后开始进行finetuning。如下所示，epochs=30，使用双卡训练，使用`--pretrained_model`来指定需要加载的预训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 \n",
      "28 pretrained_model=$job_dir/pretrained_model\n",
      "29 raw_data_path=$job_dir/rawdata/train.txt\n",
      "30 tokenizer_path=$job_dir/pretrained_model/vocab.txt\n",
      "31 model_config=$job_dir/pretrained_model/config.json\n",
      "32 epochs=200\n",
      "33 batch_size=8\n",
      "34 log_step=10\n",
      "35 output_dir=$job_dir/model_finetuned/\n",
      "36 num_pieces=1\n",
      "37 \n",
      "38 python train_on_small_file.py \\\n",
      "39     --pretrained_model $pretrained_model \\\n",
      "40     --raw_data_path $raw_data_path \\\n",
      "41     --tokenizer_path $tokenizer_path \\\n",
      "42     --model_config $model_config \\\n",
      "43     --epochs $epochs \\\n",
      "44     --batch_size $batch_size \\\n",
      "45     --log_step $log_step \\\n",
      "46     --output_dir $output_dir \\\n",
      "47     --num_pieces $num_pieces \\\n",
      "48     --device 0,1"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; perl -ne 'print \"$. $_\" if ($.>=27 and $.<=48)' finetune.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weijing/git/GPT2-Chinese/tasks/dataaugmentation\n",
      "I0418 16:27:42.270872 140709942798144 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=8, device='0,1', epochs=300, fp16=False, fp16_opt_level='O1', gradient_accumulation=1, ignore_intermediate_epoch_model=True, log_step=10, lr=0.00015, max_grad_norm=1.0, model_config='tasks/dataaugmentation/pretrained_model/config.json', num_pieces=1, output_dir='tasks/dataaugmentation/model_finetuned/', pretrained_model='tasks/dataaugmentation/pretrained_model', raw_data_path='tasks/dataaugmentation/rawdata/train.txt', segment=False, stride=768, tokenizer_path='tasks/dataaugmentation/pretrained_model/vocab.txt', warmup_steps=2000)\n",
      "config:\n",
      "{\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "using device: cuda\n",
      "I0418 16:27:42.583670 140709942798144 configuration_utils.py:148] loading configuration file tasks/dataaugmentation/pretrained_model/config.json\n",
      "I0418 16:27:42.584254 140709942798144 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0418 16:27:42.585492 140709942798144 modeling_utils.py:334] loading weights file tasks/dataaugmentation/pretrained_model/pytorch_model.bin\n",
      "calculating total steps\n",
      "total steps = 17\n",
      "Let's use 2 GPUs!\n",
      "/home/weijing/.conda/envs/weijing-torch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "start preparing data\n",
      "starting training\n",
      "epoch 1\n",
      "time: 2020-04-18 16:27:47.784330\n",
      "/home/weijing/.conda/envs/weijing-torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "now time: 16:27. Step 10 of piece 0 of epoch 1, loss 14.73789939880371\n",
      "epoch 2\n",
      "time: 2020-04-18 16:28:04.890754\n",
      "now time: 16:28. Step 10 of piece 0 of epoch 2, loss 21.72820911407471\n",
      "epoch 3\n",
      "time: 2020-04-18 16:28:19.480155\n",
      "now time: 16:28. Step 10 of piece 0 of epoch 3, loss 15.100232028961182\n",
      "epoch 4\n",
      "time: 2020-04-18 16:28:34.098097\n",
      "now time: 16:28. Step 10 of piece 0 of epoch 4, loss 7.7675494194030765\n",
      "epoch 5\n",
      "time: 2020-04-18 16:28:48.703349\n",
      "now time: 16:28. Step 10 of piece 0 of epoch 5, loss 3.0752493500709535\n",
      "epoch 6\n",
      "time: 2020-04-18 16:29:03.328550\n",
      "now time: 16:29. Step 10 of piece 0 of epoch 6, loss 1.884317046403885\n",
      "epoch 7\n",
      "time: 2020-04-18 16:29:17.989001\n",
      "now time: 16:29. Step 10 of piece 0 of epoch 7, loss 1.5620037257671355\n",
      "epoch 8\n",
      "time: 2020-04-18 16:29:32.737187\n",
      "now time: 16:29. Step 10 of piece 0 of epoch 8, loss 1.3823623955249786\n",
      "epoch 9\n",
      "time: 2020-04-18 16:29:47.504764\n",
      "now time: 16:29. Step 10 of piece 0 of epoch 9, loss 1.2680854678153992\n",
      "epoch 10\n",
      "time: 2020-04-18 16:30:02.408630\n",
      "now time: 16:30. Step 10 of piece 0 of epoch 10, loss 1.3216347992420197\n",
      "epoch 11\n",
      "time: 2020-04-18 16:30:17.305389\n",
      "now time: 16:30. Step 10 of piece 0 of epoch 11, loss 1.2088375329971313\n",
      "epoch 12\n",
      "time: 2020-04-18 16:30:32.283418\n",
      "now time: 16:30. Step 10 of piece 0 of epoch 12, loss 1.2334250509738922\n",
      "epoch 13\n",
      "time: 2020-04-18 16:30:47.114862\n",
      "now time: 16:30. Step 10 of piece 0 of epoch 13, loss 1.1426235526800155\n",
      "epoch 14\n",
      "time: 2020-04-18 16:31:02.042026\n",
      "now time: 16:31. Step 10 of piece 0 of epoch 14, loss 1.1442518383264542\n",
      "epoch 15\n",
      "time: 2020-04-18 16:31:16.973847\n",
      "now time: 16:31. Step 10 of piece 0 of epoch 15, loss 1.0738641023635864\n",
      "epoch 16\n",
      "time: 2020-04-18 16:31:31.935037\n",
      "now time: 16:31. Step 10 of piece 0 of epoch 16, loss 1.0986874610185624\n",
      "epoch 17\n",
      "time: 2020-04-18 16:31:47.003899\n",
      "now time: 16:31. Step 10 of piece 0 of epoch 17, loss 1.00964335501194\n",
      "epoch 18\n",
      "time: 2020-04-18 16:32:02.032895\n",
      "now time: 16:32. Step 10 of piece 0 of epoch 18, loss 1.0236358553171159\n",
      "epoch 19\n",
      "time: 2020-04-18 16:32:17.069895\n",
      "now time: 16:32. Step 10 of piece 0 of epoch 19, loss 1.0070772767066956\n",
      "epoch 20\n",
      "time: 2020-04-18 16:32:31.994835\n",
      "now time: 16:32. Step 10 of piece 0 of epoch 20, loss 0.9196786969900131\n",
      "epoch 21\n",
      "time: 2020-04-18 16:32:47.079824\n",
      "now time: 16:32. Step 10 of piece 0 of epoch 21, loss 0.7956881165504456\n",
      "epoch 22\n",
      "time: 2020-04-18 16:33:02.010384\n",
      "now time: 16:33. Step 10 of piece 0 of epoch 22, loss 0.8584056228399277\n",
      "epoch 23\n",
      "time: 2020-04-18 16:33:16.985143\n",
      "now time: 16:33. Step 10 of piece 0 of epoch 23, loss 0.7418496042490006\n",
      "epoch 24\n",
      "time: 2020-04-18 16:33:32.098486\n",
      "now time: 16:33. Step 10 of piece 0 of epoch 24, loss 0.6719987720251084\n",
      "epoch 25\n",
      "time: 2020-04-18 16:33:47.082758\n",
      "now time: 16:33. Step 10 of piece 0 of epoch 25, loss 0.7078431397676468\n",
      "epoch 26\n",
      "time: 2020-04-18 16:34:02.190927\n",
      "now time: 16:34. Step 10 of piece 0 of epoch 26, loss 0.5863134697079658\n",
      "epoch 27\n",
      "time: 2020-04-18 16:34:17.121234\n",
      "now time: 16:34. Step 10 of piece 0 of epoch 27, loss 0.5270837783813477\n",
      "epoch 28\n",
      "time: 2020-04-18 16:34:32.099299\n",
      "now time: 16:34. Step 10 of piece 0 of epoch 28, loss 0.47135054469108584\n",
      "epoch 29\n",
      "time: 2020-04-18 16:34:47.050826\n",
      "now time: 16:34. Step 10 of piece 0 of epoch 29, loss 0.4376502513885498\n",
      "epoch 30\n",
      "time: 2020-04-18 16:35:01.964332\n",
      "now time: 16:35. Step 10 of piece 0 of epoch 30, loss 0.36917820274829866\n",
      "epoch 31\n",
      "time: 2020-04-18 16:35:16.961451\n",
      "now time: 16:35. Step 10 of piece 0 of epoch 31, loss 0.31771988719701766\n",
      "epoch 32\n",
      "time: 2020-04-18 16:35:31.876851\n",
      "now time: 16:35. Step 10 of piece 0 of epoch 32, loss 0.2960539810359478\n",
      "epoch 33\n",
      "time: 2020-04-18 16:35:47.004226\n",
      "now time: 16:35. Step 10 of piece 0 of epoch 33, loss 0.23030379936099052\n",
      "epoch 34\n",
      "time: 2020-04-18 16:36:01.996081\n",
      "now time: 16:36. Step 10 of piece 0 of epoch 34, loss 0.21836489737033843\n",
      "epoch 35\n",
      "time: 2020-04-18 16:36:17.078901\n",
      "now time: 16:36. Step 10 of piece 0 of epoch 35, loss 0.17260930240154265\n",
      "epoch 36\n",
      "time: 2020-04-18 16:36:32.111849\n",
      "now time: 16:36. Step 10 of piece 0 of epoch 36, loss 0.15431733913719653\n",
      "epoch 37\n",
      "time: 2020-04-18 16:36:47.117363\n",
      "now time: 16:36. Step 10 of piece 0 of epoch 37, loss 0.12944445572793484\n",
      "epoch 38\n",
      "time: 2020-04-18 16:37:02.200995\n",
      "now time: 16:37. Step 10 of piece 0 of epoch 38, loss 0.13093428909778596\n",
      "epoch 39\n",
      "time: 2020-04-18 16:37:17.186417\n",
      "now time: 16:37. Step 10 of piece 0 of epoch 39, loss 0.09703588709235192\n",
      "epoch 40\n",
      "time: 2020-04-18 16:37:32.220431\n",
      "now time: 16:37. Step 10 of piece 0 of epoch 40, loss 0.08638252578675747\n",
      "epoch 41\n",
      "time: 2020-04-18 16:37:47.203634\n",
      "now time: 16:37. Step 10 of piece 0 of epoch 41, loss 0.07639692723751068\n",
      "epoch 42\n",
      "time: 2020-04-18 16:38:02.245470\n",
      "now time: 16:38. Step 10 of piece 0 of epoch 42, loss 0.06496242936700583\n",
      "epoch 43\n",
      "time: 2020-04-18 16:38:17.240255\n",
      "now time: 16:38. Step 10 of piece 0 of epoch 43, loss 0.06512548327445984\n",
      "epoch 44\n",
      "time: 2020-04-18 16:38:32.215825\n",
      "now time: 16:38. Step 10 of piece 0 of epoch 44, loss 0.05547212176024914\n",
      "epoch 45\n",
      "time: 2020-04-18 16:38:47.277893\n",
      "now time: 16:38. Step 10 of piece 0 of epoch 45, loss 0.05469777956604958\n",
      "epoch 46\n",
      "time: 2020-04-18 16:39:02.449004\n",
      "now time: 16:39. Step 10 of piece 0 of epoch 46, loss 0.047487042285501956\n",
      "epoch 47\n",
      "time: 2020-04-18 16:39:17.451169\n",
      "now time: 16:39. Step 10 of piece 0 of epoch 47, loss 0.04465657901018858\n",
      "epoch 48\n",
      "time: 2020-04-18 16:39:32.578540\n",
      "now time: 16:39. Step 10 of piece 0 of epoch 48, loss 0.040395396295934916\n",
      "epoch 49\n",
      "time: 2020-04-18 16:39:47.557250\n",
      "now time: 16:39. Step 10 of piece 0 of epoch 49, loss 0.039883851259946826\n",
      "epoch 50\n",
      "time: 2020-04-18 16:40:02.638781\n",
      "now time: 16:40. Step 10 of piece 0 of epoch 50, loss 0.035636765975505115\n",
      "epoch 51\n",
      "time: 2020-04-18 16:40:17.714733\n",
      "now time: 16:40. Step 10 of piece 0 of epoch 51, loss 0.030668544489890336\n",
      "epoch 52\n",
      "time: 2020-04-18 16:40:32.825782\n",
      "now time: 16:40. Step 10 of piece 0 of epoch 52, loss 0.03261901708319783\n",
      "epoch 53\n",
      "time: 2020-04-18 16:40:47.859317\n",
      "now time: 16:40. Step 10 of piece 0 of epoch 53, loss 0.031193183083087207\n",
      "epoch 54\n",
      "time: 2020-04-18 16:41:02.873184\n",
      "now time: 16:41. Step 10 of piece 0 of epoch 54, loss 0.028607837110757827\n",
      "epoch 55\n",
      "time: 2020-04-18 16:41:17.917218\n",
      "now time: 16:41. Step 10 of piece 0 of epoch 55, loss 0.029043549299240114\n",
      "epoch 56\n",
      "time: 2020-04-18 16:41:32.962522\n",
      "now time: 16:41. Step 10 of piece 0 of epoch 56, loss 0.028297946508973836\n",
      "epoch 57\n",
      "time: 2020-04-18 16:41:48.121646\n",
      "now time: 16:41. Step 10 of piece 0 of epoch 57, loss 0.026911271177232264\n",
      "epoch 58\n",
      "time: 2020-04-18 16:42:03.276214\n",
      "now time: 16:42. Step 10 of piece 0 of epoch 58, loss 0.026496085245162247\n",
      "epoch 59\n",
      "time: 2020-04-18 16:42:18.261369\n",
      "now time: 16:42. Step 10 of piece 0 of epoch 59, loss 0.02608219552785158\n",
      "epoch 60\n",
      "time: 2020-04-18 16:42:33.415517\n",
      "now time: 16:42. Step 10 of piece 0 of epoch 60, loss 0.02296778652817011\n",
      "epoch 61\n",
      "time: 2020-04-18 16:42:48.426893\n",
      "now time: 16:42. Step 10 of piece 0 of epoch 61, loss 0.02278460282832384\n",
      "epoch 62\n",
      "time: 2020-04-18 16:43:03.471137\n",
      "now time: 16:43. Step 10 of piece 0 of epoch 62, loss 0.02255934737622738\n",
      "epoch 63\n",
      "time: 2020-04-18 16:43:18.554655\n",
      "now time: 16:43. Step 10 of piece 0 of epoch 63, loss 0.02153145857155323\n",
      "epoch 64\n",
      "time: 2020-04-18 16:43:33.731101\n",
      "now time: 16:43. Step 10 of piece 0 of epoch 64, loss 0.02233936293050647\n",
      "epoch 65\n",
      "time: 2020-04-18 16:43:48.887091\n",
      "now time: 16:43. Step 10 of piece 0 of epoch 65, loss 0.019872965477406978\n",
      "epoch 66\n",
      "time: 2020-04-18 16:44:03.867381\n",
      "now time: 16:44. Step 10 of piece 0 of epoch 66, loss 0.021421484369784593\n",
      "epoch 67\n",
      "time: 2020-04-18 16:44:18.970265\n",
      "now time: 16:44. Step 10 of piece 0 of epoch 67, loss 0.020335312373936175\n",
      "epoch 68\n",
      "time: 2020-04-18 16:44:34.021348\n",
      "now time: 16:44. Step 10 of piece 0 of epoch 68, loss 0.018390879221260547\n",
      "epoch 69\n",
      "time: 2020-04-18 16:44:49.124416\n",
      "now time: 16:44. Step 10 of piece 0 of epoch 69, loss 0.019520888198167087\n",
      "epoch 70\n",
      "time: 2020-04-18 16:45:04.282617\n",
      "now time: 16:45. Step 10 of piece 0 of epoch 70, loss 0.018671777658164503\n",
      "epoch 71\n",
      "time: 2020-04-18 16:45:19.469291\n",
      "now time: 16:45. Step 10 of piece 0 of epoch 71, loss 0.018518638471141456\n",
      "epoch 72\n",
      "time: 2020-04-18 16:45:34.478209\n",
      "now time: 16:45. Step 10 of piece 0 of epoch 72, loss 0.017665452510118484\n",
      "epoch 73\n",
      "time: 2020-04-18 16:45:49.513942\n",
      "now time: 16:45. Step 10 of piece 0 of epoch 73, loss 0.01784371193498373\n",
      "epoch 74\n",
      "time: 2020-04-18 16:46:04.683530\n",
      "now time: 16:46. Step 10 of piece 0 of epoch 74, loss 0.01909736106172204\n",
      "epoch 75\n",
      "time: 2020-04-18 16:46:19.775841\n",
      "now time: 16:46. Step 10 of piece 0 of epoch 75, loss 0.016863411106169224\n",
      "epoch 76\n",
      "time: 2020-04-18 16:46:34.932449\n",
      "now time: 16:46. Step 10 of piece 0 of epoch 76, loss 0.017565199173986912\n",
      "epoch 77\n",
      "time: 2020-04-18 16:46:50.012009\n",
      "now time: 16:46. Step 10 of piece 0 of epoch 77, loss 0.01647572163492441\n",
      "epoch 78\n",
      "time: 2020-04-18 16:47:05.207976\n",
      "now time: 16:47. Step 10 of piece 0 of epoch 78, loss 0.016319660749286414\n",
      "epoch 79\n",
      "time: 2020-04-18 16:47:20.307303\n",
      "now time: 16:47. Step 10 of piece 0 of epoch 79, loss 0.01690553119406104\n",
      "epoch 80\n",
      "time: 2020-04-18 16:47:35.437156\n",
      "now time: 16:47. Step 10 of piece 0 of epoch 80, loss 0.017141045164316894\n",
      "epoch 81\n",
      "time: 2020-04-18 16:47:50.494915\n",
      "now time: 16:47. Step 10 of piece 0 of epoch 81, loss 0.019426034297794104\n",
      "epoch 82\n",
      "time: 2020-04-18 16:48:05.510518\n",
      "now time: 16:48. Step 10 of piece 0 of epoch 82, loss 0.020762093504890798\n",
      "epoch 83\n",
      "time: 2020-04-18 16:48:20.586454\n",
      "now time: 16:48. Step 10 of piece 0 of epoch 83, loss 0.022158677410334348\n",
      "epoch 84\n",
      "time: 2020-04-18 16:48:35.578101\n",
      "now time: 16:48. Step 10 of piece 0 of epoch 84, loss 0.01963968398049474\n",
      "epoch 85\n",
      "time: 2020-04-18 16:48:50.658321\n",
      "now time: 16:48. Step 10 of piece 0 of epoch 85, loss 0.020133274607360363\n",
      "epoch 86\n",
      "time: 2020-04-18 16:49:05.735071\n",
      "now time: 16:49. Step 10 of piece 0 of epoch 86, loss 0.01818443639203906\n",
      "epoch 87\n",
      "time: 2020-04-18 16:49:20.775309\n",
      "now time: 16:49. Step 10 of piece 0 of epoch 87, loss 0.016016343235969545\n",
      "epoch 88\n",
      "time: 2020-04-18 16:49:35.847660\n",
      "now time: 16:49. Step 10 of piece 0 of epoch 88, loss 0.016001883102580904\n",
      "epoch 89\n",
      "time: 2020-04-18 16:49:50.929716\n",
      "now time: 16:49. Step 10 of piece 0 of epoch 89, loss 0.016164490673691034\n",
      "epoch 90\n",
      "time: 2020-04-18 16:50:06.025429\n",
      "now time: 16:50. Step 10 of piece 0 of epoch 90, loss 0.015418692678213119\n",
      "epoch 91\n",
      "time: 2020-04-18 16:50:21.194959\n",
      "now time: 16:50. Step 10 of piece 0 of epoch 91, loss 0.01432459931820631\n",
      "epoch 92\n",
      "time: 2020-04-18 16:50:36.345195\n",
      "now time: 16:50. Step 10 of piece 0 of epoch 92, loss 0.015407247841358185\n",
      "epoch 93\n",
      "time: 2020-04-18 16:50:51.420195\n",
      "now time: 16:51. Step 10 of piece 0 of epoch 93, loss 0.014391154097393156\n",
      "epoch 94\n",
      "time: 2020-04-18 16:51:06.611417\n",
      "now time: 16:51. Step 10 of piece 0 of epoch 94, loss 0.01508838557638228\n",
      "epoch 95\n",
      "time: 2020-04-18 16:51:21.649783\n",
      "now time: 16:51. Step 10 of piece 0 of epoch 95, loss 0.014976264396682381\n",
      "epoch 96\n",
      "time: 2020-04-18 16:51:36.656544\n",
      "now time: 16:51. Step 10 of piece 0 of epoch 96, loss 0.014096463797613978\n",
      "epoch 97\n",
      "time: 2020-04-18 16:51:51.733895\n",
      "now time: 16:52. Step 10 of piece 0 of epoch 97, loss 0.01382772675715387\n",
      "epoch 98\n",
      "time: 2020-04-18 16:52:06.834318\n",
      "now time: 16:52. Step 10 of piece 0 of epoch 98, loss 0.01390645643696189\n",
      "epoch 99\n",
      "time: 2020-04-18 16:52:22.006473\n",
      "now time: 16:52. Step 10 of piece 0 of epoch 99, loss 0.013935786299407482\n",
      "epoch 100\n",
      "time: 2020-04-18 16:52:37.017954\n",
      "now time: 16:52. Step 10 of piece 0 of epoch 100, loss 0.013671448174864053\n",
      "epoch 101\n",
      "time: 2020-04-18 16:52:52.108455\n",
      "now time: 16:53. Step 10 of piece 0 of epoch 101, loss 0.014145402424037456\n",
      "epoch 102\n",
      "time: 2020-04-18 16:53:07.217173\n",
      "now time: 16:53. Step 10 of piece 0 of epoch 102, loss 0.013643282325938344\n",
      "epoch 103\n",
      "time: 2020-04-18 16:53:22.381054\n",
      "now time: 16:53. Step 10 of piece 0 of epoch 103, loss 0.013375579798594116\n",
      "epoch 104\n",
      "time: 2020-04-18 16:53:37.449518\n",
      "now time: 16:53. Step 10 of piece 0 of epoch 104, loss 0.0131764005869627\n",
      "epoch 105\n",
      "time: 2020-04-18 16:53:52.571499\n",
      "now time: 16:54. Step 10 of piece 0 of epoch 105, loss 0.015102721052244306\n",
      "epoch 106\n",
      "time: 2020-04-18 16:54:07.651671\n",
      "now time: 16:54. Step 10 of piece 0 of epoch 106, loss 0.01391812302172184\n",
      "epoch 107\n",
      "time: 2020-04-18 16:54:22.665187\n",
      "now time: 16:54. Step 10 of piece 0 of epoch 107, loss 0.0146352703217417\n",
      "epoch 108\n",
      "time: 2020-04-18 16:54:37.759643\n",
      "now time: 16:54. Step 10 of piece 0 of epoch 108, loss 0.015141064953058957\n",
      "epoch 109\n",
      "time: 2020-04-18 16:54:52.923550\n",
      "now time: 16:55. Step 10 of piece 0 of epoch 109, loss 0.014405102422460913\n",
      "epoch 110\n",
      "time: 2020-04-18 16:55:08.035936\n",
      "now time: 16:55. Step 10 of piece 0 of epoch 110, loss 0.013900439580902457\n",
      "epoch 111\n",
      "time: 2020-04-18 16:55:23.114574\n",
      "now time: 16:55. Step 10 of piece 0 of epoch 111, loss 0.012935704831033946\n",
      "epoch 112\n",
      "time: 2020-04-18 16:55:38.162847\n",
      "now time: 16:55. Step 10 of piece 0 of epoch 112, loss 0.013357947487384081\n",
      "epoch 113\n",
      "time: 2020-04-18 16:55:53.247604\n",
      "now time: 16:56. Step 10 of piece 0 of epoch 113, loss 0.013589018490165472\n",
      "epoch 114\n",
      "time: 2020-04-18 16:56:08.320289\n",
      "now time: 16:56. Step 10 of piece 0 of epoch 114, loss 0.013608850492164492\n",
      "epoch 115\n",
      "time: 2020-04-18 16:56:23.420863\n",
      "now time: 16:56. Step 10 of piece 0 of epoch 115, loss 0.014626320265233517\n",
      "epoch 116\n",
      "time: 2020-04-18 16:56:38.556338\n",
      "now time: 16:56. Step 10 of piece 0 of epoch 116, loss 0.015091490093618631\n",
      "epoch 117\n",
      "time: 2020-04-18 16:56:53.658120\n",
      "now time: 16:57. Step 10 of piece 0 of epoch 117, loss 0.014804546488448977\n",
      "epoch 118\n",
      "time: 2020-04-18 16:57:08.812786\n",
      "now time: 16:57. Step 10 of piece 0 of epoch 118, loss 0.013953110855072736\n",
      "epoch 119\n",
      "time: 2020-04-18 16:57:23.925017\n",
      "now time: 16:57. Step 10 of piece 0 of epoch 119, loss 0.012164235254749656\n",
      "epoch 120\n",
      "time: 2020-04-18 16:57:39.055444\n",
      "now time: 16:57. Step 10 of piece 0 of epoch 120, loss 0.012330045038834215\n",
      "epoch 121\n",
      "time: 2020-04-18 16:57:54.127439\n",
      "now time: 16:58. Step 10 of piece 0 of epoch 121, loss 0.012157690385356546\n",
      "epoch 122\n",
      "time: 2020-04-18 16:58:09.207930\n",
      "now time: 16:58. Step 10 of piece 0 of epoch 122, loss 0.012886423384770751\n",
      "epoch 123\n",
      "time: 2020-04-18 16:58:24.324770\n",
      "now time: 16:58. Step 10 of piece 0 of epoch 123, loss 0.012114845449104905\n",
      "epoch 124\n",
      "time: 2020-04-18 16:58:39.507251\n",
      "now time: 16:58. Step 10 of piece 0 of epoch 124, loss 0.012502336828038096\n",
      "epoch 125\n",
      "time: 2020-04-18 16:58:54.613068\n",
      "now time: 16:59. Step 10 of piece 0 of epoch 125, loss 0.011991187231615186\n",
      "epoch 126\n",
      "time: 2020-04-18 16:59:09.681634\n",
      "now time: 16:59. Step 10 of piece 0 of epoch 126, loss 0.012626549368724227\n",
      "epoch 127\n",
      "time: 2020-04-18 16:59:24.780007\n",
      "now time: 16:59. Step 10 of piece 0 of epoch 127, loss 0.012847388675436378\n",
      "epoch 128\n",
      "time: 2020-04-18 16:59:39.966163\n",
      "now time: 16:59. Step 10 of piece 0 of epoch 128, loss 0.01195912817493081\n",
      "epoch 129\n",
      "time: 2020-04-18 16:59:55.026916\n",
      "now time: 17:0. Step 10 of piece 0 of epoch 129, loss 0.012568460777401924\n",
      "epoch 130\n",
      "time: 2020-04-18 17:00:10.080411\n",
      "now time: 17:0. Step 10 of piece 0 of epoch 130, loss 0.012333839107304812\n",
      "epoch 131\n",
      "time: 2020-04-18 17:00:25.132301\n",
      "now time: 17:0. Step 10 of piece 0 of epoch 131, loss 0.012182066775858402\n",
      "epoch 132\n",
      "time: 2020-04-18 17:00:40.191060\n",
      "now time: 17:0. Step 10 of piece 0 of epoch 132, loss 0.013446337589994074\n",
      "epoch 133\n",
      "time: 2020-04-18 17:00:55.281387\n",
      "now time: 17:1. Step 10 of piece 0 of epoch 133, loss 0.013186403550207615\n",
      "epoch 134\n",
      "time: 2020-04-18 17:01:10.358432\n",
      "now time: 17:1. Step 10 of piece 0 of epoch 134, loss 0.012335525266826153\n",
      "epoch 135\n",
      "time: 2020-04-18 17:01:25.441253\n",
      "now time: 17:1. Step 10 of piece 0 of epoch 135, loss 0.012191974371671677\n",
      "epoch 136\n",
      "time: 2020-04-18 17:01:40.472527\n",
      "now time: 17:1. Step 10 of piece 0 of epoch 136, loss 0.012826099060475826\n",
      "epoch 137\n",
      "time: 2020-04-18 17:01:55.691856\n",
      "now time: 17:2. Step 10 of piece 0 of epoch 137, loss 0.012194927781820297\n",
      "epoch 138\n",
      "time: 2020-04-18 17:02:10.868137\n",
      "now time: 17:2. Step 10 of piece 0 of epoch 138, loss 0.012988979928195477\n",
      "epoch 139\n",
      "time: 2020-04-18 17:02:26.040651\n",
      "now time: 17:2. Step 10 of piece 0 of epoch 139, loss 0.012407416291534901\n",
      "epoch 140\n",
      "time: 2020-04-18 17:02:41.080842\n",
      "now time: 17:2. Step 10 of piece 0 of epoch 140, loss 0.01254913117736578\n",
      "epoch 141\n",
      "time: 2020-04-18 17:02:56.230873\n",
      "now time: 17:3. Step 10 of piece 0 of epoch 141, loss 0.011578092863783241\n",
      "epoch 142\n",
      "time: 2020-04-18 17:03:11.370564\n",
      "now time: 17:3. Step 10 of piece 0 of epoch 142, loss 0.01242411215789616\n",
      "epoch 143\n",
      "time: 2020-04-18 17:03:26.537955\n",
      "now time: 17:3. Step 10 of piece 0 of epoch 143, loss 0.012898606108501553\n",
      "epoch 144\n",
      "time: 2020-04-18 17:03:41.640878\n",
      "now time: 17:3. Step 10 of piece 0 of epoch 144, loss 0.012525058491155506\n",
      "epoch 145\n",
      "time: 2020-04-18 17:03:56.753553\n",
      "now time: 17:4. Step 10 of piece 0 of epoch 145, loss 0.011971736093983054\n",
      "epoch 146\n",
      "time: 2020-04-18 17:04:11.878511\n",
      "now time: 17:4. Step 10 of piece 0 of epoch 146, loss 0.012802399089559912\n",
      "epoch 147\n",
      "time: 2020-04-18 17:04:27.029134\n",
      "now time: 17:4. Step 10 of piece 0 of epoch 147, loss 0.012018832657486201\n",
      "epoch 148\n",
      "time: 2020-04-18 17:04:42.161304\n",
      "now time: 17:4. Step 10 of piece 0 of epoch 148, loss 0.012668001465499401\n",
      "epoch 149\n",
      "time: 2020-04-18 17:04:57.300405\n",
      "now time: 17:5. Step 10 of piece 0 of epoch 149, loss 0.013279399974271655\n",
      "epoch 150\n",
      "time: 2020-04-18 17:05:12.440985\n",
      "now time: 17:5. Step 10 of piece 0 of epoch 150, loss 0.012154674530029297\n",
      "epoch 151\n",
      "time: 2020-04-18 17:05:27.549312\n",
      "now time: 17:5. Step 10 of piece 0 of epoch 151, loss 0.012354744132608175\n",
      "epoch 152\n",
      "time: 2020-04-18 17:05:42.669468\n",
      "now time: 17:5. Step 10 of piece 0 of epoch 152, loss 0.012440371559932828\n",
      "epoch 153\n",
      "time: 2020-04-18 17:05:57.717660\n",
      "now time: 17:6. Step 10 of piece 0 of epoch 153, loss 0.012578821880742908\n",
      "epoch 154\n",
      "time: 2020-04-18 17:06:12.928393\n",
      "now time: 17:6. Step 10 of piece 0 of epoch 154, loss 0.012704484444111586\n",
      "epoch 155\n",
      "time: 2020-04-18 17:06:27.988335\n",
      "now time: 17:6. Step 10 of piece 0 of epoch 155, loss 0.012349525094032287\n",
      "epoch 156\n",
      "time: 2020-04-18 17:06:43.114690\n",
      "now time: 17:6. Step 10 of piece 0 of epoch 156, loss 0.012160651199519635\n",
      "epoch 157\n",
      "time: 2020-04-18 17:06:58.238052\n",
      "now time: 17:7. Step 10 of piece 0 of epoch 157, loss 0.01273829941637814\n",
      "epoch 158\n",
      "time: 2020-04-18 17:07:13.315540\n",
      "now time: 17:7. Step 10 of piece 0 of epoch 158, loss 0.012539606494829058\n",
      "epoch 159\n",
      "time: 2020-04-18 17:07:28.483440\n",
      "now time: 17:7. Step 10 of piece 0 of epoch 159, loss 0.01252607866190374\n",
      "epoch 160\n",
      "time: 2020-04-18 17:07:43.567724\n",
      "now time: 17:7. Step 10 of piece 0 of epoch 160, loss 0.01274620252661407\n",
      "epoch 161\n",
      "time: 2020-04-18 17:07:58.658355\n",
      "now time: 17:8. Step 10 of piece 0 of epoch 161, loss 0.012442442448809743\n",
      "epoch 162\n",
      "time: 2020-04-18 17:08:13.822815\n",
      "now time: 17:8. Step 10 of piece 0 of epoch 162, loss 0.012141453800722957\n",
      "epoch 163\n",
      "time: 2020-04-18 17:08:28.932768\n",
      "now time: 17:8. Step 10 of piece 0 of epoch 163, loss 0.012716972921043634\n",
      "epoch 164\n",
      "time: 2020-04-18 17:08:43.947310\n",
      "now time: 17:8. Step 10 of piece 0 of epoch 164, loss 0.012681759521365165\n",
      "epoch 165\n",
      "time: 2020-04-18 17:08:59.060596\n",
      "now time: 17:9. Step 10 of piece 0 of epoch 165, loss 0.013580080680549144\n",
      "epoch 166\n",
      "time: 2020-04-18 17:09:14.118497\n",
      "now time: 17:9. Step 10 of piece 0 of epoch 166, loss 0.012625597324222327\n",
      "epoch 167\n",
      "time: 2020-04-18 17:09:29.180808\n",
      "now time: 17:9. Step 10 of piece 0 of epoch 167, loss 0.012840186478570104\n",
      "epoch 168\n",
      "time: 2020-04-18 17:09:44.391876\n",
      "now time: 17:9. Step 10 of piece 0 of epoch 168, loss 0.012245152005925774\n",
      "epoch 169\n",
      "time: 2020-04-18 17:09:59.416263\n",
      "now time: 17:10. Step 10 of piece 0 of epoch 169, loss 0.012570387311279773\n",
      "epoch 170\n",
      "time: 2020-04-18 17:10:14.464549\n",
      "now time: 17:10. Step 10 of piece 0 of epoch 170, loss 0.012411962682381272\n",
      "epoch 171\n",
      "time: 2020-04-18 17:10:29.541674\n",
      "now time: 17:10. Step 10 of piece 0 of epoch 171, loss 0.012535727955400944\n",
      "epoch 172\n",
      "time: 2020-04-18 17:10:44.629933\n",
      "now time: 17:10. Step 10 of piece 0 of epoch 172, loss 0.012404328351840377\n",
      "epoch 173\n",
      "time: 2020-04-18 17:10:59.697892\n",
      "now time: 17:11. Step 10 of piece 0 of epoch 173, loss 0.012364353192970157\n",
      "epoch 174\n",
      "time: 2020-04-18 17:11:14.771799\n",
      "now time: 17:11. Step 10 of piece 0 of epoch 174, loss 0.012264137575402855\n",
      "epoch 175\n",
      "time: 2020-04-18 17:11:29.956715\n",
      "now time: 17:11. Step 10 of piece 0 of epoch 175, loss 0.012701325630769134\n",
      "epoch 176\n",
      "time: 2020-04-18 17:11:45.026783\n",
      "now time: 17:11. Step 10 of piece 0 of epoch 176, loss 0.012212220672518015\n",
      "epoch 177\n",
      "time: 2020-04-18 17:12:00.064791\n",
      "now time: 17:12. Step 10 of piece 0 of epoch 177, loss 0.012273967592045664\n",
      "epoch 178\n",
      "time: 2020-04-18 17:12:15.137468\n",
      "now time: 17:12. Step 10 of piece 0 of epoch 178, loss 0.01229897364974022\n",
      "epoch 179\n",
      "time: 2020-04-18 17:12:30.194030\n",
      "now time: 17:12. Step 10 of piece 0 of epoch 179, loss 0.012153956666588783\n",
      "epoch 180\n",
      "time: 2020-04-18 17:12:45.225833\n",
      "now time: 17:12. Step 10 of piece 0 of epoch 180, loss 0.012808914156630635\n",
      "epoch 181\n",
      "time: 2020-04-18 17:13:00.331995\n",
      "now time: 17:13. Step 10 of piece 0 of epoch 181, loss 0.012554448703303933\n",
      "epoch 182\n",
      "time: 2020-04-18 17:13:15.426255\n",
      "now time: 17:13. Step 10 of piece 0 of epoch 182, loss 0.012229518825188279\n",
      "epoch 183\n",
      "time: 2020-04-18 17:13:30.543972\n",
      "now time: 17:13. Step 10 of piece 0 of epoch 183, loss 0.012636690912768245\n",
      "epoch 184\n",
      "time: 2020-04-18 17:13:45.561461\n",
      "now time: 17:13. Step 10 of piece 0 of epoch 184, loss 0.01255841702222824\n",
      "epoch 185\n",
      "time: 2020-04-18 17:14:00.695855\n",
      "now time: 17:14. Step 10 of piece 0 of epoch 185, loss 0.012458449928089976\n",
      "epoch 186\n",
      "time: 2020-04-18 17:14:15.703316\n",
      "now time: 17:14. Step 10 of piece 0 of epoch 186, loss 0.01278245383873582\n",
      "epoch 187\n",
      "time: 2020-04-18 17:14:30.724265\n",
      "now time: 17:14. Step 10 of piece 0 of epoch 187, loss 0.01270564366132021\n",
      "epoch 188\n",
      "time: 2020-04-18 17:14:45.844911\n",
      "now time: 17:14. Step 10 of piece 0 of epoch 188, loss 0.01260196827352047\n",
      "epoch 189\n",
      "time: 2020-04-18 17:15:01.016471\n",
      "now time: 17:15. Step 10 of piece 0 of epoch 189, loss 0.012835858715698123\n",
      "epoch 190\n",
      "time: 2020-04-18 17:15:16.134803\n",
      "now time: 17:15. Step 10 of piece 0 of epoch 190, loss 0.012642697477713228\n",
      "epoch 191\n",
      "time: 2020-04-18 17:15:31.251566\n",
      "now time: 17:15. Step 10 of piece 0 of epoch 191, loss 0.01257738503627479\n",
      "epoch 192\n",
      "time: 2020-04-18 17:15:46.350072\n",
      "now time: 17:15. Step 10 of piece 0 of epoch 192, loss 0.012530676927417517\n",
      "epoch 193\n",
      "time: 2020-04-18 17:16:01.357428\n",
      "now time: 17:16. Step 10 of piece 0 of epoch 193, loss 0.012725000968202949\n",
      "epoch 194\n",
      "time: 2020-04-18 17:16:16.548837\n",
      "now time: 17:16. Step 10 of piece 0 of epoch 194, loss 0.012397608952596784\n",
      "epoch 195\n",
      "time: 2020-04-18 17:16:31.589351\n",
      "now time: 17:16. Step 10 of piece 0 of epoch 195, loss 0.012286874046549202\n",
      "epoch 196\n",
      "time: 2020-04-18 17:16:46.675233\n",
      "now time: 17:16. Step 10 of piece 0 of epoch 196, loss 0.012334374617785216\n",
      "epoch 197\n",
      "time: 2020-04-18 17:17:01.716054\n",
      "now time: 17:17. Step 10 of piece 0 of epoch 197, loss 0.012269948888570071\n",
      "epoch 198\n",
      "time: 2020-04-18 17:17:16.750180\n",
      "now time: 17:17. Step 10 of piece 0 of epoch 198, loss 0.012217641388997436\n",
      "epoch 199\n",
      "time: 2020-04-18 17:17:31.745705\n",
      "now time: 17:17. Step 10 of piece 0 of epoch 199, loss 0.012864843849092722\n",
      "epoch 200\n",
      "time: 2020-04-18 17:17:46.896837\n",
      "now time: 17:17. Step 10 of piece 0 of epoch 200, loss 0.012178466562181712\n",
      "epoch 201\n",
      "time: 2020-04-18 17:18:01.984124\n",
      "now time: 17:18. Step 10 of piece 0 of epoch 201, loss 0.01280486946925521\n",
      "epoch 202\n",
      "time: 2020-04-18 17:18:17.016788\n",
      "now time: 17:18. Step 10 of piece 0 of epoch 202, loss 0.012325547961518168\n",
      "epoch 203\n",
      "time: 2020-04-18 17:18:31.959655\n",
      "now time: 17:18. Step 10 of piece 0 of epoch 203, loss 0.01249260939657688\n",
      "epoch 204\n",
      "time: 2020-04-18 17:18:47.095660\n",
      "now time: 17:18. Step 10 of piece 0 of epoch 204, loss 0.01248097000643611\n",
      "epoch 205\n",
      "time: 2020-04-18 17:19:02.122049\n",
      "now time: 17:19. Step 10 of piece 0 of epoch 205, loss 0.012782143615186214\n",
      "epoch 206\n",
      "time: 2020-04-18 17:19:17.375438\n",
      "now time: 17:19. Step 10 of piece 0 of epoch 206, loss 0.012643399415537715\n",
      "epoch 207\n",
      "time: 2020-04-18 17:19:32.631235\n",
      "now time: 17:19. Step 10 of piece 0 of epoch 207, loss 0.012759946985170245\n",
      "epoch 208\n",
      "time: 2020-04-18 17:19:47.738977\n",
      "now time: 17:19. Step 10 of piece 0 of epoch 208, loss 0.012651434214785696\n",
      "epoch 209\n",
      "time: 2020-04-18 17:20:02.812737\n",
      "now time: 17:20. Step 10 of piece 0 of epoch 209, loss 0.012974049476906658\n",
      "epoch 210\n",
      "time: 2020-04-18 17:20:17.851841\n",
      "now time: 17:20. Step 10 of piece 0 of epoch 210, loss 0.012155980244278908\n",
      "epoch 211\n",
      "time: 2020-04-18 17:20:32.978119\n",
      "now time: 17:20. Step 10 of piece 0 of epoch 211, loss 0.011976102041080594\n",
      "epoch 212\n",
      "time: 2020-04-18 17:20:47.956893\n",
      "now time: 17:20. Step 10 of piece 0 of epoch 212, loss 0.012649450730532408\n",
      "epoch 213\n",
      "time: 2020-04-18 17:21:02.964508\n",
      "now time: 17:21. Step 10 of piece 0 of epoch 213, loss 0.012195915449410677\n",
      "epoch 214\n",
      "time: 2020-04-18 17:21:18.060742\n",
      "now time: 17:21. Step 10 of piece 0 of epoch 214, loss 0.01214947197586298\n",
      "epoch 215\n",
      "time: 2020-04-18 17:21:33.157465\n",
      "now time: 17:21. Step 10 of piece 0 of epoch 215, loss 0.012749665835872292\n",
      "epoch 216\n",
      "time: 2020-04-18 17:21:48.232790\n",
      "now time: 17:21. Step 10 of piece 0 of epoch 216, loss 0.011543324822559953\n",
      "epoch 217\n",
      "time: 2020-04-18 17:22:03.268124\n",
      "now time: 17:22. Step 10 of piece 0 of epoch 217, loss 0.01250932584516704\n",
      "epoch 218\n",
      "time: 2020-04-18 17:22:18.385863\n",
      "now time: 17:22. Step 10 of piece 0 of epoch 218, loss 0.01238190121948719\n",
      "epoch 219\n",
      "time: 2020-04-18 17:22:33.673486\n",
      "now time: 17:22. Step 10 of piece 0 of epoch 219, loss 0.012981256889179348\n",
      "epoch 220\n",
      "time: 2020-04-18 17:22:48.798012\n",
      "now time: 17:22. Step 10 of piece 0 of epoch 220, loss 0.012203059019520878\n",
      "epoch 221\n",
      "time: 2020-04-18 17:23:03.916307\n",
      "now time: 17:23. Step 10 of piece 0 of epoch 221, loss 0.011864092666655779\n",
      "epoch 222\n",
      "time: 2020-04-18 17:23:18.935856\n",
      "now time: 17:23. Step 10 of piece 0 of epoch 222, loss 0.012918770918622613\n",
      "epoch 223\n",
      "time: 2020-04-18 17:23:33.942020\n",
      "now time: 17:23. Step 10 of piece 0 of epoch 223, loss 0.011968862544745208\n",
      "epoch 224\n",
      "time: 2020-04-18 17:23:49.021044\n",
      "now time: 17:23. Step 10 of piece 0 of epoch 224, loss 0.012724763108417391\n",
      "epoch 225\n",
      "time: 2020-04-18 17:24:04.116822\n",
      "now time: 17:24. Step 10 of piece 0 of epoch 225, loss 0.012566016800701619\n",
      "epoch 226\n",
      "time: 2020-04-18 17:24:19.094660\n",
      "now time: 17:24. Step 10 of piece 0 of epoch 226, loss 0.012381104007363319\n",
      "epoch 227\n",
      "time: 2020-04-18 17:24:34.237032\n",
      "now time: 17:24. Step 10 of piece 0 of epoch 227, loss 0.01316230082884431\n",
      "epoch 228\n",
      "time: 2020-04-18 17:24:49.271952\n",
      "now time: 17:24. Step 10 of piece 0 of epoch 228, loss 0.012374892318621277\n",
      "epoch 229\n",
      "time: 2020-04-18 17:25:04.338434\n",
      "now time: 17:25. Step 10 of piece 0 of epoch 229, loss 0.012584666581824422\n",
      "epoch 230\n",
      "time: 2020-04-18 17:25:19.372307\n",
      "now time: 17:25. Step 10 of piece 0 of epoch 230, loss 0.012479931581765413\n",
      "epoch 231\n",
      "time: 2020-04-18 17:25:34.414826\n",
      "now time: 17:25. Step 10 of piece 0 of epoch 231, loss 0.012882129149511456\n",
      "epoch 232\n",
      "time: 2020-04-18 17:25:49.540407\n",
      "now time: 17:25. Step 10 of piece 0 of epoch 232, loss 0.012469731085002423\n",
      "epoch 233\n",
      "time: 2020-04-18 17:26:04.545427\n",
      "now time: 17:26. Step 10 of piece 0 of epoch 233, loss 0.012071964889764785\n",
      "epoch 234\n",
      "time: 2020-04-18 17:26:19.772890\n",
      "now time: 17:26. Step 10 of piece 0 of epoch 234, loss 0.012626388110220432\n",
      "epoch 235\n",
      "time: 2020-04-18 17:26:34.920872\n",
      "now time: 17:26. Step 10 of piece 0 of epoch 235, loss 0.012437798734754323\n",
      "epoch 236\n",
      "time: 2020-04-18 17:26:50.133404\n",
      "now time: 17:26. Step 10 of piece 0 of epoch 236, loss 0.012372670695185661\n",
      "epoch 237\n",
      "time: 2020-04-18 17:27:05.343305\n",
      "now time: 17:27. Step 10 of piece 0 of epoch 237, loss 0.011919225007295609\n",
      "epoch 238\n",
      "time: 2020-04-18 17:27:20.442675\n",
      "now time: 17:27. Step 10 of piece 0 of epoch 238, loss 0.012614228017628192\n",
      "epoch 239\n",
      "time: 2020-04-18 17:27:35.479485\n",
      "now time: 17:27. Step 10 of piece 0 of epoch 239, loss 0.012602211069315672\n",
      "epoch 240\n",
      "time: 2020-04-18 17:27:50.550910\n",
      "now time: 17:27. Step 10 of piece 0 of epoch 240, loss 0.011949705285951495\n",
      "epoch 241\n",
      "time: 2020-04-18 17:28:05.612016\n",
      "now time: 17:28. Step 10 of piece 0 of epoch 241, loss 0.012642136309295893\n",
      "epoch 242\n",
      "time: 2020-04-18 17:28:20.663743\n",
      "now time: 17:28. Step 10 of piece 0 of epoch 242, loss 0.012679743161424995\n",
      "epoch 243\n",
      "time: 2020-04-18 17:28:35.723656\n",
      "now time: 17:28. Step 10 of piece 0 of epoch 243, loss 0.012527830945327879\n",
      "epoch 244\n",
      "time: 2020-04-18 17:28:50.884402\n",
      "now time: 17:28. Step 10 of piece 0 of epoch 244, loss 0.012366548087447882\n",
      "epoch 245\n",
      "time: 2020-04-18 17:29:05.889203\n",
      "now time: 17:29. Step 10 of piece 0 of epoch 245, loss 0.01271544829942286\n",
      "epoch 246\n",
      "time: 2020-04-18 17:29:20.936744\n",
      "now time: 17:29. Step 10 of piece 0 of epoch 246, loss 0.012655289331451058\n",
      "epoch 247\n",
      "time: 2020-04-18 17:29:35.988270\n",
      "now time: 17:29. Step 10 of piece 0 of epoch 247, loss 0.012497701868414878\n",
      "epoch 248\n",
      "time: 2020-04-18 17:29:51.064375\n",
      "now time: 17:29. Step 10 of piece 0 of epoch 248, loss 0.012040337175130844\n",
      "epoch 249\n",
      "time: 2020-04-18 17:30:06.160273\n",
      "now time: 17:30. Step 10 of piece 0 of epoch 249, loss 0.01238631596788764\n",
      "epoch 250\n",
      "time: 2020-04-18 17:30:21.254260\n",
      "now time: 17:30. Step 10 of piece 0 of epoch 250, loss 0.01226638276129961\n",
      "epoch 251\n",
      "time: 2020-04-18 17:30:36.336890\n",
      "now time: 17:30. Step 10 of piece 0 of epoch 251, loss 0.012966505996882916\n",
      "epoch 252\n",
      "time: 2020-04-18 17:30:51.370103\n",
      "now time: 17:31. Step 10 of piece 0 of epoch 252, loss 0.013181689288467168\n",
      "epoch 253\n",
      "time: 2020-04-18 17:31:06.536215\n",
      "now time: 17:31. Step 10 of piece 0 of epoch 253, loss 0.012547920970246196\n",
      "epoch 254\n",
      "time: 2020-04-18 17:31:21.536383\n",
      "now time: 17:31. Step 10 of piece 0 of epoch 254, loss 0.01226532687433064\n",
      "epoch 255\n",
      "time: 2020-04-18 17:31:36.569273\n",
      "now time: 17:31. Step 10 of piece 0 of epoch 255, loss 0.012403867114335299\n",
      "epoch 256\n",
      "time: 2020-04-18 17:31:51.639330\n",
      "now time: 17:32. Step 10 of piece 0 of epoch 256, loss 0.012395175313577056\n",
      "epoch 257\n",
      "time: 2020-04-18 17:32:06.685025\n",
      "now time: 17:32. Step 10 of piece 0 of epoch 257, loss 0.01255612620152533\n",
      "epoch 258\n",
      "time: 2020-04-18 17:32:21.740339\n",
      "now time: 17:32. Step 10 of piece 0 of epoch 258, loss 0.012225890951231121\n",
      "epoch 259\n",
      "time: 2020-04-18 17:32:36.962079\n",
      "now time: 17:32. Step 10 of piece 0 of epoch 259, loss 0.012526782462373376\n",
      "epoch 260\n",
      "time: 2020-04-18 17:32:52.048778\n",
      "now time: 17:33. Step 10 of piece 0 of epoch 260, loss 0.0124636126216501\n",
      "epoch 261\n",
      "time: 2020-04-18 17:33:07.202576\n",
      "now time: 17:33. Step 10 of piece 0 of epoch 261, loss 0.012236179085448384\n",
      "epoch 262\n",
      "time: 2020-04-18 17:33:22.230407\n",
      "now time: 17:33. Step 10 of piece 0 of epoch 262, loss 0.012630123272538185\n",
      "epoch 263\n",
      "time: 2020-04-18 17:33:37.329952\n",
      "now time: 17:33. Step 10 of piece 0 of epoch 263, loss 0.011827342258766294\n",
      "epoch 264\n",
      "time: 2020-04-18 17:33:52.440042\n",
      "now time: 17:34. Step 10 of piece 0 of epoch 264, loss 0.012777615571394563\n",
      "epoch 265\n",
      "time: 2020-04-18 17:34:07.502727\n",
      "now time: 17:34. Step 10 of piece 0 of epoch 265, loss 0.013058808306232095\n",
      "epoch 266\n",
      "time: 2020-04-18 17:34:22.573462\n",
      "now time: 17:34. Step 10 of piece 0 of epoch 266, loss 0.011794919241219759\n",
      "epoch 267\n",
      "time: 2020-04-18 17:34:37.697222\n",
      "now time: 17:34. Step 10 of piece 0 of epoch 267, loss 0.012726918747648596\n",
      "epoch 268\n",
      "time: 2020-04-18 17:34:52.888932\n",
      "now time: 17:35. Step 10 of piece 0 of epoch 268, loss 0.012268611043691636\n",
      "epoch 269\n",
      "time: 2020-04-18 17:35:08.018926\n",
      "now time: 17:35. Step 10 of piece 0 of epoch 269, loss 0.012484415154904126\n",
      "epoch 270\n",
      "time: 2020-04-18 17:35:23.130601\n",
      "now time: 17:35. Step 10 of piece 0 of epoch 270, loss 0.012108387425541878\n",
      "epoch 271\n",
      "time: 2020-04-18 17:35:38.241064\n",
      "now time: 17:35. Step 10 of piece 0 of epoch 271, loss 0.012311245175078511\n",
      "epoch 272\n",
      "time: 2020-04-18 17:35:53.310606\n",
      "now time: 17:36. Step 10 of piece 0 of epoch 272, loss 0.012469225935637951\n",
      "epoch 273\n",
      "time: 2020-04-18 17:36:08.412320\n",
      "now time: 17:36. Step 10 of piece 0 of epoch 273, loss 0.012134592374786734\n",
      "epoch 274\n",
      "time: 2020-04-18 17:36:23.553454\n",
      "now time: 17:36. Step 10 of piece 0 of epoch 274, loss 0.012357862340286374\n",
      "epoch 275\n",
      "time: 2020-04-18 17:36:38.609595\n",
      "now time: 17:36. Step 10 of piece 0 of epoch 275, loss 0.012439230969175697\n",
      "epoch 276\n",
      "time: 2020-04-18 17:36:53.687796\n",
      "now time: 17:37. Step 10 of piece 0 of epoch 276, loss 0.012544427812099457\n",
      "epoch 277\n",
      "time: 2020-04-18 17:37:08.715542\n",
      "now time: 17:37. Step 10 of piece 0 of epoch 277, loss 0.012098901765421033\n",
      "epoch 278\n",
      "time: 2020-04-18 17:37:23.893887\n",
      "now time: 17:37. Step 10 of piece 0 of epoch 278, loss 0.012956651533022523\n",
      "epoch 279\n",
      "time: 2020-04-18 17:37:38.888601\n",
      "now time: 17:37. Step 10 of piece 0 of epoch 279, loss 0.012864028569310904\n",
      "epoch 280\n",
      "time: 2020-04-18 17:37:53.942582\n",
      "now time: 17:38. Step 10 of piece 0 of epoch 280, loss 0.012104605045169591\n",
      "epoch 281\n",
      "time: 2020-04-18 17:38:08.997391\n",
      "now time: 17:38. Step 10 of piece 0 of epoch 281, loss 0.012081788945943117\n",
      "epoch 282\n",
      "time: 2020-04-18 17:38:24.051793\n",
      "now time: 17:38. Step 10 of piece 0 of epoch 282, loss 0.012657768744975328\n",
      "epoch 283\n",
      "time: 2020-04-18 17:38:39.009365\n",
      "now time: 17:38. Step 10 of piece 0 of epoch 283, loss 0.012477041780948639\n",
      "epoch 284\n",
      "time: 2020-04-18 17:38:54.043067\n",
      "now time: 17:39. Step 10 of piece 0 of epoch 284, loss 0.012171331839635969\n",
      "epoch 285\n",
      "time: 2020-04-18 17:39:09.214047\n",
      "now time: 17:39. Step 10 of piece 0 of epoch 285, loss 0.012711007380858064\n",
      "epoch 286\n",
      "time: 2020-04-18 17:39:24.284160\n",
      "now time: 17:39. Step 10 of piece 0 of epoch 286, loss 0.012541057309135795\n",
      "epoch 287\n",
      "time: 2020-04-18 17:39:39.435965\n",
      "now time: 17:39. Step 10 of piece 0 of epoch 287, loss 0.012652888894081116\n",
      "epoch 288\n",
      "time: 2020-04-18 17:39:54.646080\n",
      "now time: 17:40. Step 10 of piece 0 of epoch 288, loss 0.012537352042272687\n",
      "epoch 289\n",
      "time: 2020-04-18 17:40:09.736973\n",
      "now time: 17:40. Step 10 of piece 0 of epoch 289, loss 0.012133522657677531\n",
      "epoch 290\n",
      "time: 2020-04-18 17:40:24.824535\n",
      "now time: 17:40. Step 10 of piece 0 of epoch 290, loss 0.012653550738468767\n",
      "epoch 291\n",
      "time: 2020-04-18 17:40:39.911853\n",
      "now time: 17:40. Step 10 of piece 0 of epoch 291, loss 0.011525173345580697\n",
      "epoch 292\n",
      "time: 2020-04-18 17:40:54.932293\n",
      "now time: 17:41. Step 10 of piece 0 of epoch 292, loss 0.012787446286529303\n",
      "epoch 293\n",
      "time: 2020-04-18 17:41:09.996430\n",
      "now time: 17:41. Step 10 of piece 0 of epoch 293, loss 0.012540654093027116\n",
      "epoch 294\n",
      "time: 2020-04-18 17:41:25.063952\n",
      "now time: 17:41. Step 10 of piece 0 of epoch 294, loss 0.013592596771195531\n",
      "epoch 295\n",
      "time: 2020-04-18 17:41:40.201307\n",
      "now time: 17:41. Step 10 of piece 0 of epoch 295, loss 0.012076722737401725\n",
      "epoch 296\n",
      "time: 2020-04-18 17:41:55.296147\n",
      "now time: 17:42. Step 10 of piece 0 of epoch 296, loss 0.01259075440466404\n",
      "epoch 297\n",
      "time: 2020-04-18 17:42:10.459626\n",
      "now time: 17:42. Step 10 of piece 0 of epoch 297, loss 0.012445503287017346\n",
      "epoch 298\n",
      "time: 2020-04-18 17:42:25.446880\n",
      "now time: 17:42. Step 10 of piece 0 of epoch 298, loss 0.0126290547195822\n",
      "epoch 299\n",
      "time: 2020-04-18 17:42:40.555573\n",
      "now time: 17:42. Step 10 of piece 0 of epoch 299, loss 0.0126536272931844\n",
      "epoch 300\n",
      "time: 2020-04-18 17:42:55.705250\n",
      "now time: 17:43. Step 10 of piece 0 of epoch 300, loss 0.01298530139029026\n",
      "training finished\n",
      "I0418 17:43:10.799661 140709942798144 configuration_utils.py:71] Configuration saved in tasks/dataaugmentation/model_finetuned/final_model/config.json\n",
      "I0418 17:43:11.509091 140709942798144 modeling_utils.py:205] Model weights saved in tasks/dataaugmentation/model_finetuned/final_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; pwd; bash finetune.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用微调之后的模型进行数据增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了微调之后的模型，我们使用top-k sampling生成指定开头的文本，进行数据增强。如下，我们以\"[MASK][MASK][MASK]月亮\"作为开头（使用3个[MASK]的原因是在原始的语料中，一首诗的第一行之前有3个回车符），进行数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tprefix=$1 #e.g., [MASK][MASK][MASK]月亮\n",
      "     2\t\n",
      "     3\techo \"prefix=\"$prefix\n",
      "     4\t\n",
      "     5\tjob_dir=\"tasks/dataaugmentation\"\n",
      "     6\t\n",
      "     7\tcd ../..\n",
      "     8\t\n",
      "     9\tif [ ! -e $job_dir/outputs_finetuned ]; then\n",
      "    10\t    mkdir $job_dir/outputs_finetuned\n",
      "    11\tfi\n",
      "    12\ttokenizer_path=$job_dir/pretrained_model/vocab.txt\n",
      "    13\tmodel_path=$job_dir/model_finetuned/final_model/\n",
      "    14\tmodel_config=$model_path/model_config.json\n",
      "    15\tsample_path=$job_dir/outputs_finetuned/\n",
      "    16\t\n",
      "    17\tpython generate.py \\\n",
      "    18\t    --device 0 \\\n",
      "    19\t    --model_path $model_path \\\n",
      "    20\t    --model_config $model_config \\\n",
      "    21\t    --tokenizer_path $tokenizer_path \\\n",
      "    22\t    --temperature 0.8 \\\n",
      "    23\t    --prefix $prefix \\\n",
      "    24\t    --length 50 \\\n",
      "    25\t    --topk 50 \\\n",
      "    26\t    --nsamples 10 \\\n",
      "    27\t    --save_samples \\\n",
      "    28\t    --save_samples_path $sample_path\n"
     ]
    }
   ],
   "source": [
    "!cat -n tasks/dataaugmentation/generate_from_finetuned.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix=[MASK][MASK][MASK]宽阔的大海\n",
      "epoch=119\n",
      "nsamples=50\n",
      "I0418 16:01:46.499238 139921282234176 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=1, device='0', fast_pattern=False, length=50, model_config='tasks/dataaugmentation/model_finetuned/model_epoch119//model_config.json', model_path='tasks/dataaugmentation/model_finetuned/model_epoch119/', no_wordpiece=False, nsamples=50, prefix='[MASK][MASK][MASK]宽阔的大海', repetition_penalty=1.0, save_samples=True, save_samples_path='tasks/dataaugmentation/outputs_finetuned/', segment=False, temperature=0.8, tokenizer_path='tasks/dataaugmentation/pretrained_model/vocab.txt', topk=50, topp=0)\n",
      "I0418 16:01:46.791843 139921282234176 configuration_utils.py:148] loading configuration file tasks/dataaugmentation/model_finetuned/model_epoch119/config.json\n",
      "I0418 16:01:46.792851 139921282234176 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0418 16:01:46.794697 139921282234176 modeling_utils.py:334] loading weights file tasks/dataaugmentation/model_finetuned/model_epoch119/pytorch_model.bin\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 150.83it/s]\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]这就是我张开手指所要叙说的故事[MASK]那洞窟不会在今夜关闭。明天夜晚也不会关闭[MASK]额头披满钟声的[MASK]土地[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 148.61it/s]\n",
      "======================================== SAMPLE 2 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]水面上[MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]石头[MASK]印满你的脚印[MASK][MASK]水面上\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 143.30it/s]\n",
      "======================================== SAMPLE 3 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？你的容颜[MASK][MASK]三丈有三十二条[MASK]成为我的妻子[MASK]我在意到你是一位美丽结实的女子[MASK]我们偶然相\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.22it/s]\n",
      "======================================== SAMPLE 4 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我感到魅惑[MASK]我感到魅惑[MASK]小小的鼓浪[MASK]听见海水的声音[MASK][MASK]我感到魅惑[MASK]小小的碧怀抱[MASK]就想在这条魅惑之\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.35it/s]\n",
      "======================================== SAMPLE 5 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面的小山[MASK][MASK]我们打开手指[MASK]指向大海[MASK]我们指向海来[MASK]一定是王冠[MASK]一定是我们[MASK]，海水没有[MASK]把我们打\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.02it/s]\n",
      "======================================== SAMPLE 6 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]这就是我张开手指所要叙说的故事[MASK]那洞窟不会在今夜关闭。明天夜晚也不会关闭[MASK]额头披满钟声的[MASK]土地[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.92it/s]\n",
      "======================================== SAMPLE 7 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的心上[MASK]有一座村庄[MASK][MASK]我们都在远方[MASK]像村庄[MASK]根连着你[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.10it/s]\n",
      "======================================== SAMPLE 8 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.50it/s]\n",
      "======================================== SAMPLE 9 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK][MASK]我是一只花鹿[MASK]走上前来[MASK]抱住火车[MASK]我就是一只鱼[MASK]我们在火焰中[MASK]踩到厦门[MASK]我是一\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.56it/s]\n",
      "======================================== SAMPLE 10 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我给你旧木屋[MASK]你坐在家乡的木凳上想我[MASK][MASK]我给你倒一碗水[MASK][MASK]我给你一个娇小的拥抱[MASK][MASK]我给你一个娇小\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.72it/s]\n",
      "======================================== SAMPLE 11 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我的梦之蛇[MASK][MASK]为什么[MASK]让我从此不再[MASK]独自一人[MASK]为什么[MASK]让我长眠不醒[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.45it/s]\n",
      "======================================== SAMPLE 12 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]面对棵棵绿树[MASK]坐着[MASK]一动不动[MASK]汽车声音响起在[MASK]脊背上[MASK]我这就想把我这[MASK]盖满落叶的旧外套[MASK]寄给这城\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.67it/s]\n",
      "======================================== SAMPLE 13 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于一个无比幸福的人[MASK][MASK]我愿意[MASK]用胸脯接住无比幸福的你[MASK]我愿意[MASK]用胸脯接住庸俗的你[MASK][MASK]你不用算命\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.60it/s]\n",
      "======================================== SAMPLE 14 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]用我的一生[MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]我愿意[MASK]陪伴我[MASK]我身体健\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.65it/s]\n",
      "======================================== SAMPLE 15 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于普通的职业[MASK]认为，普希金和我的父亲[MASK][MASK]用行动[MASK]用行动[MASK]用行动[MASK]用行动[MASK]歌唱[MASK][MASK]河水[MASK]一直流到\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.59it/s]\n",
      "======================================== SAMPLE 16 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你是我的[MASK]用一生的情诗[MASK][MASK]用一生的情诗[MASK][MASK]为了美丽[MASK]我们共同[MASK]为了美丽[MASK]我们收获了真谛[MASK][MASK]我是上\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.01it/s]\n",
      "======================================== SAMPLE 17 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]愿意像一座宝塔[MASK]在夜里悄悄建成[MASK][MASK]我愿意[MASK]愿意像一座宝塔[MASK]在夜\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.11it/s]\n",
      "======================================== SAMPLE 18 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对了[MASK]我身体里第一个女人[MASK]我愿意[MASK]愿意像一座宝塔[MASK]在夜里悄悄建成[MASK][MASK]我愿意[MASK]用我的身体[MASK]在夜里成\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.22it/s]\n",
      "======================================== SAMPLE 19 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]三角洲和碎花的笑[MASK]一起甩到脑后[MASK]一块大陆在愤怒地骚动[MASK]北方平原上红高粱[MASK]已酿成新生的青春期鲜血[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.46it/s]\n",
      "======================================== SAMPLE 20 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我感到魅惑[MASK][MASK]我感到魅惑[MASK]小鱼儿，金黄的麦子[MASK]看见了吗[MASK]蓝色的海水[MASK][MASK]我感到魅惑[MASK]小鱼儿，龙儿[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.01it/s]\n",
      "======================================== SAMPLE 21 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]愿意像一片叶子[MASK]落入我的身上[MASK][MASK]脊背上有一片霞光[MASK][MASK]我愿意[MASK]愿意像一片叶子[MASK]躺在我的身上\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.21it/s]\n",
      "======================================== SAMPLE 22 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺在这里，什么也不要想起大海[MASK][MASK]我是一只鱼筐[MASK]想着另一只鱼筐[MASK][MASK]我是一只鱼筐[MASK]想着另一只鱼[MASK]想\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.02it/s]\n",
      "======================================== SAMPLE 23 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我感到魅惑[MASK]我想起悲伤的热带[MASK][MASK]我感到魅惑[MASK][MASK]美丽的秋天[MASK]使我旧情难忘[MASK][MASK]我感到魅惑[MASK]蓝色的麦地\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.04it/s]\n",
      "======================================== SAMPLE 24 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对我的窗户里埋着一只为你祝福的杯子[MASK][MASK]我的杯子上[MASK]渭水的井沿城[MASK]象一只为你祝福的杯子[MASK][MASK]美丽的你\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.76it/s]\n",
      "======================================== SAMPLE 25 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的一生[MASK]也是这块大海[MASK][MASK]目击众神死亡的草原上野花一片[MASK]远在远方的风比远方更远[MASK]我的琴声呜咽，魂\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.91it/s]\n",
      "======================================== SAMPLE 26 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面山[MASK][MASK]我[MASK]径直走入[MASK]你在一个小渔船[MASK]我就想把你搂在怀中[MASK]我的指引[MASK]不能说[MASK]著就是你的情义[MASK]由\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.93it/s]\n",
      "======================================== SAMPLE 27 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的黑色主人[MASK]使我的黑色主人[MASK]我的黑色主人[MASK][MASK]主人美丽[MASK]我的黑色主人[MASK]我的黑色主人[MASK]我的黑色主人\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.90it/s]\n",
      "======================================== SAMPLE 28 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺在我的头上[MASK]我知道，这是我自己的水[MASK]水滴中露水的妇人[MASK][MASK]我不能没有[MASK]一切源于我的血[MASK]源于我[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.97it/s]\n",
      "======================================== SAMPLE 29 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面山[MASK][MASK]我们指引另一个[MASK]一起去看山[MASK][MASK]我们赢了[MASK]一切都很陌生[MASK]只有一个字[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.91it/s]\n",
      "======================================== SAMPLE 30 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你离开我们[MASK]也就走进了另一个世界[MASK][MASK]我的心上人[MASK]全都是黑色的[MASK]我的心上人[MASK]梦中人[MASK]一切都很美[MASK][MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.32it/s]\n",
      "======================================== SAMPLE 31 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海的女人[MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]那个庸俗的故事[MASK]使我们陌生[MASK]没有留下痕迹[MASK][MASK]海水没\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.34it/s]\n",
      "======================================== SAMPLE 32 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你在渔市上[MASK]寻找下弦月[MASK]我在月光下[MASK]经过小河流[MASK][MASK]你在婚礼上[MASK]使用红筷子[MASK]我在向阳坡[MASK]栽下两行竹\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.93it/s]\n",
      "======================================== SAMPLE 33 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面山[MASK][MASK]我[MASK]径直走入[MASK]你红色的裤带[MASK]我的胸脯[MASK]就象一片又瘦又长的树叶[MASK]落上稻草：唉，这没有泥土\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.07it/s]\n",
      "======================================== SAMPLE 34 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你脱下黑色的袜子[MASK]我就象一个小女人[MASK]我的梦都没有[MASK][MASK]我是在早上[MASK]在传说中[MASK]2.[MASK][MASK]早上[MASK]我端起水\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.18it/s]\n",
      "======================================== SAMPLE 35 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]这就是我张开手指所要叙说的故事[MASK]那洞窟不会在今夜关闭。明天夜晚也不会关闭[MASK]额头披满钟声的[MASK]土地[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.81it/s]\n",
      "======================================== SAMPLE 36 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你脱下黑色的女靴[MASK]在水里[MASK]扑打着肌肤[MASK][MASK]我也是退下白色的女人[MASK][MASK]我更是喜欢黑色的女人[MASK]我更喜欢黑\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.63it/s]\n",
      "======================================== SAMPLE 37 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺在另一个世界上[MASK]我在故乡的海底[MASK][MASK]我的梦中[MASK]美丽的女儿，永远是我痛苦的女儿[MASK][MASK]我是一个粗枝大\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.85it/s]\n",
      "======================================== SAMPLE 38 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海来说[MASK]没有像海水这么认为[MASK]水只是一个简朴的人[MASK]用泥沙这么说[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.85it/s]\n",
      "======================================== SAMPLE 39 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK][MASK]我就是那个情种[MASK]一个开始[MASK]为你创造的天空[MASK]我是一只[MASK]为你一个战争的臣子[MASK][MASK]我就\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.78it/s]\n",
      "======================================== SAMPLE 40 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK][MASK]我是一个粗枝大叶的人[MASK]我是一个无比幸福的人[MASK][MASK]我是一个无比幸福的人[MASK]我全身的黑\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.40it/s]\n",
      "======================================== SAMPLE 41 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面的小山[MASK][MASK]我们全都背叛自己的故乡[MASK]我们会把幸福当成祖传的职业[MASK]放下手中痛苦的诗篇[MASK][MASK]我们会把\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.79it/s]\n",
      "======================================== SAMPLE 42 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于我[MASK]提起小小的故事[MASK]那个人[MASK]在微寒的早晨[MASK]我双手捂住耳朵[MASK]想起隔山隔水的[MASK]那个人[MASK]如今在我身\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.28it/s]\n",
      "======================================== SAMPLE 43 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对了，我的窗户里埋着一只为你祝福的杯子[MASK][MASK]我的窗户里埋着一只为你祝福的杯子[MASK][MASK]那是我最后一次想起\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.88it/s]\n",
      "======================================== SAMPLE 44 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]用胸脯接住海水[MASK]用内外的大海[MASK]让他们插上洁白的帆[MASK]就像他们的身体[MASK]就像他们的头发[MASK][MASK]我用双手分开\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.20it/s]\n",
      "======================================== SAMPLE 45 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你是我的[MASK]用心感受[MASK]我是一个无比幸福的人[MASK]我是一个无比幸福的人[MASK]我全身的黑暗因太阳升起而解除[MASK]我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.17it/s]\n",
      "======================================== SAMPLE 46 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]等你[MASK]我[MASK]赴一个美丽的花期[MASK][MASK]你是我的[MASK]半截的诗[MASK]半截用心爱着[MASK]半截用肉体埋着[MASK]你是我的[MASK]半截的\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.22it/s]\n",
      "======================================== SAMPLE 47 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面小镇[MASK]我是次年进故乡的[MASK]我是在海上渡过的[MASK]我是在水下生锈的旧人[MASK][MASK]我是在一个离开故乡的陌生人\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.74it/s]\n",
      "======================================== SAMPLE 48 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]等你[MASK]我[MASK]渡过了三十丈[MASK]起了大海[MASK][MASK]我在渔市上[MASK]经过小河[MASK][MASK]我的身躯[MASK]像白天下[MASK]了解你的羊[MASK][MASK]我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.90it/s]\n",
      "======================================== SAMPLE 49 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海[MASK][MASK]我们是荒野上第一根被晒坏的石柱[MASK][MASK]我们是荒野上第一根被晒坏的石柱[MASK][MASK]我们的木杆上[MASK]挂\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.28it/s]\n",
      "======================================== SAMPLE 50 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面的小姑娘[MASK]在我们面前[MASK]像湖泊一样[MASK][MASK]我们把我们照亮[MASK][MASK]我们把我们的故乡[MASK]在我们的坟墓上[MASK]放下\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; bash generate_from_finetuned.sh [MASK][MASK][MASK]宽阔的大海 119 50; cd ../..;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0418 16:19:59.791799 140198131083072 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=1, device='0', fast_pattern=False, length=50, model_config='tasks/dataaugmentation/model_finetuned/model_epoch119//model_config.json', model_path='tasks/dataaugmentation/model_finetuned/model_epoch119/', no_wordpiece=False, nsamples=50, prefix='[MASK][MASK][MASK]宽阔的大海', repetition_penalty=1.0, save_samples=True, save_samples_path='tasks/dataaugmentation/outputs_finetuned/', segment=False, temperature=0.8, tokenizer_path='tasks/dataaugmentation/pretrained_model/vocab.txt', topk=50, topp=0)\n",
      "I0418 16:20:00.077131 140198131083072 configuration_utils.py:148] loading configuration file tasks/dataaugmentation/model_finetuned/model_epoch119/config.json\n",
      "I0418 16:20:00.077562 140198131083072 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0418 16:20:00.078293 140198131083072 modeling_utils.py:334] loading weights file tasks/dataaugmentation/model_finetuned/model_epoch119/pytorch_model.bin\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 147.59it/s]\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海的诗人[MASK]以粮食为主题[MASK]以粮食为主题[MASK]以粮食为主题[MASK]以粮食为主题[MASK]以粮食为主题[MASK]以粮食为主\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 144.28it/s]\n",
      "======================================== SAMPLE 2 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？那就[MASK]小雨儿如蜻蜓[MASK]我的心情开始平静而开朗[MASK][MASK]我看见你的笑容[MASK][MASK]我看见你的侧脸[MASK]我看\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 145.95it/s]\n",
      "======================================== SAMPLE 3 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？那就想着，我[MASK]进出出出出出的[MASK][MASK]北方[MASK]是荷尔[MASK]站在圆月罐上[MASK]想着，我[MASK]想着，能有几样\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.33it/s]\n",
      "======================================== SAMPLE 4 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]或许，这是我的[MASK]水[MASK]也或许是我的[MASK]模仿他的影子[MASK]但我更愿意[MASK]这是我的外表[MASK]看到了水滴中的水[MASK][MASK]看\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.73it/s]\n",
      "======================================== SAMPLE 5 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK]我是一个粗暴的人[MASK]我不知道[MASK]这是一个怎样的一个节[MASK]但是我知道只有一个穿过世俗的的\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.03it/s]\n",
      "======================================== SAMPLE 6 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]拉着你的手[MASK]手[MASK]摘下手套[MASK]她们就是两盏小灯[MASK][MASK]我的肩膀[MASK]是两座旧房子[MASK]容纳了那么多[MASK]甚至容纳过夜\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.89it/s]\n",
      "======================================== SAMPLE 7 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK]我是一个粗暴的人[MASK]也是最后一个人[MASK]让我着迷的人[MASK]在船上[MASK]我穿上它吧[MASK]我的洁白的羊\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.95it/s]\n",
      "======================================== SAMPLE 8 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？[MASK]那也是庸俗的[MASK]早上[MASK]吹熄我的窗户[MASK]在有星星和月亮的夜晚[MASK]如婴儿[MASK]吹熄我的窗户[MASK]在有\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.75it/s]\n",
      "======================================== SAMPLE 9 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于我[MASK]提起一个字“世界上最遥远的距离[MASK][MASK]爱情的村庄[MASK]我的窗户里埋葬野兽[MASK]在我出生的一所酒馆[MASK]那\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.73it/s]\n",
      "======================================== SAMPLE 10 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我给你赐给我的爱情[MASK]我给你的手[MASK]手写给你的手[MASK][MASK]是雨滴[MASK]把我的手指献给你[MASK]我的嘴唇[MASK][MASK]给你倒在[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.89it/s]\n",
      "======================================== SAMPLE 11 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我的梦之蛇[MASK][MASK]用一个很长的句子[MASK]写给你[MASK]很远的一次[MASK]我的梦之蛇[MASK][MASK]用一个很远的日子[MASK]为你写着[MASK]塌\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.73it/s]\n",
      "======================================== SAMPLE 12 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.12it/s]\n",
      "======================================== SAMPLE 13 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你象一个小小的水罐[MASK]容纳了那么多[MASK]甚至容纳过夜晚[MASK]你头发散乱[MASK]清澈的小水滴[MASK][MASK]你象一个珍贵的翡翠\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.16it/s]\n",
      "======================================== SAMPLE 14 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我感到魅惑[MASK][MASK]我感到魅惑[MASK]小夜曲[MASK]我感到魅惑[MASK][MASK]我感到魅惑[MASK]小夜曲[MASK]就感到魅惑[MASK]小夜曲[MASK]我感到魅\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.76it/s]\n",
      "======================================== SAMPLE 15 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]两座村庄[MASK]和平与情欲的村庄[MASK]诗的村庄[MASK]村庄母亲昙花一现[MASK]村庄母亲美丽绝伦[MASK][MASK]五月的麦地上[unu\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.97it/s]\n",
      "======================================== SAMPLE 16 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你从远方来到我的[MASK]你象山谷[MASK]里埋葬的我[MASK]挂在寨子里[MASK]你不姓李也不姓王[MASK]你嫁给的男人[MASK]脾气怎么样[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.17it/s]\n",
      "======================================== SAMPLE 17 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你从大海来到落日的那一天起[MASK][MASK]（阳光下[MASK]我们全都背叛自己的故乡[MASK]我们会把幸福当成祖传的职业[MASK]放下\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.17it/s]\n",
      "======================================== SAMPLE 18 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对另一位小姑娘[MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]水面上[MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]海水上流下[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.54it/s]\n",
      "======================================== SAMPLE 19 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]面对棵棵绿树[MASK]坐着[MASK]一动不动[MASK]汽车声音响起在[MASK]脊背上[MASK]我这就想把我这[MASK]盖满落叶的旧外套[MASK]寄给这城\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.80it/s]\n",
      "======================================== SAMPLE 20 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对着你的手[MASK]指引着我[MASK][MASK]我的身子是一只船[MASK][MASK]我的身子是一只船[MASK][MASK]我的身子是一只船[MASK][MASK]容纳了那么多\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.70it/s]\n",
      "======================================== SAMPLE 21 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你象一片又一片的树叶[MASK]落上稻草：[MASK]我的窗户里[MASK]挂满了红薯[MASK][MASK]我感到魅惑[MASK]我就想在这条魅惑之河上渡\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.93it/s]\n",
      "======================================== SAMPLE 22 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于文字的人来说，道道中至简直太多[MASK]你来过[MASK]的确是一个不言而喻[MASK][MASK]你在渔市上[MASK]寻找下弦月[MASK]我在月\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.85it/s]\n",
      "======================================== SAMPLE 23 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]愿意与你[MASK]说话的时刻[MASK]向日葵[MASK]像牛奶[MASK]愿意和你[MASK]说话的时刻[MASK]向日葵[MASK]像牛奶[MASK]在土地上[MASK]栽\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.94it/s]\n",
      "======================================== SAMPLE 24 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对於宽宽的河水[MASK]我们普通的农民[MASK]把我们的柴米[MASK]在一起[MASK][MASK]作为公务的你[MASK]把我们的麦子[MASK]放在锅勺上[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.49it/s]\n",
      "======================================== SAMPLE 25 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗[MASK]为什么[MASK]为什么[MASK]为什么[MASK]喝下自己的酒[MASK]为什么[MASK]喝下自己的梨子[MASK]为什么[MASK]喝下自己的梨子\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 133.26it/s]\n",
      "======================================== SAMPLE 26 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于爱情诗集[MASK]我是一个爱情诗集[MASK]我是在寻找另一个人的地方[MASK]那就是你居住的城市[MASK]我的门环上有一个美\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.35it/s]\n",
      "======================================== SAMPLE 27 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面的小姑娘[MASK]我[MASK]径直走入[MASK]你的身子[MASK]我的心情[MASK]就象一片叶子完整地藏在树上[MASK][MASK]我的身子上[MASK]我有些\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.05it/s]\n",
      "======================================== SAMPLE 28 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.41it/s]\n",
      "======================================== SAMPLE 29 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你象一只美丽的小鱼[MASK][MASK]我躺在你的身子上[MASK]我感到魅惑[MASK]你象在屋顶上画了一只美丽的彩鸽[MASK][MASK]我感到魅惑\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.04it/s]\n",
      "======================================== SAMPLE 30 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]等你[MASK]我[MASK]渡过的中午[MASK][MASK]我在渔市上[MASK]寻找下弦月[MASK][MASK]我在月光下[MASK]经过小河流[MASK][MASK]我在月光下[MASK]经过小河\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.20it/s]\n",
      "======================================== SAMPLE 31 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我的窗户里埋着一只为你祝福的杯子[MASK][MASK]我的屋顶[MASK]有一只为你祝福的手[MASK][MASK]我的屋顶[MASK]有一只为你祝福的杯\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.57it/s]\n",
      "======================================== SAMPLE 32 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.28it/s]\n",
      "======================================== SAMPLE 33 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对着九月的天空[MASK]我有些惶恐[MASK]天空向我滚来[MASK]我是在我自己的远方[MASK]我在故乡的海底[MASK]在我自己的远方[MASK]我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.18it/s]\n",
      "======================================== SAMPLE 34 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺着，我端坐[MASK]我端坐在微温的被窝上[MASK]我知道你是一位美丽结实的女子[MASK]我们都很陌生[MASK]你总是笑笑[MASK]穿\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.06it/s]\n",
      "======================================== SAMPLE 35 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你脱下黑色的双腿[MASK]我知道自己是黑色的王子[MASK][MASK]我也知道自己是黑色的王子[MASK]亲人是黑色的化身[MASK]你将自己\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.46it/s]\n",
      "======================================== SAMPLE 36 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我的梦之蛇[MASK][MASK]你在哪里[MASK]渡口站成[MASK]水面上漂来[MASK]我的梦之蛇[MASK][MASK]你在哪里[MASK]手推着自己的小木门[MASK]往前迈\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.94it/s]\n",
      "======================================== SAMPLE 37 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]两座村庄[MASK]和平与情欲的村庄[MASK]诗的村庄[MASK]村庄母亲昙花一现[MASK]村庄母亲美丽绝伦[MASK][MASK]五月的麦地上[unu\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.52it/s]\n",
      "======================================== SAMPLE 38 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺着，我在这里[MASK][MASK]我在这里[MASK]我是最后一个人的[MASK]我在这里[MASK]到过你上班的那一个[MASK]也是最后一个[MASK][MASK]在\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.10it/s]\n",
      "======================================== SAMPLE 39 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK]我的伤口[MASK]像一片又瘦又长的树叶[MASK][MASK]我是一只树叶[MASK]落上天空的一滴露水[MASK][MASK]我是一只树\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.27it/s]\n",
      "======================================== SAMPLE 40 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]陪伴我自己[MASK]我自己[MASK]我感觉到自己是一只花豹子[MASK]我感到自己是一只\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.37it/s]\n",
      "======================================== SAMPLE 41 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.31it/s]\n",
      "======================================== SAMPLE 42 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面山[MASK][MASK]我[MASK]径直走来[MASK]你在一枝新叶[MASK][MASK]我在一枝青丫下窃窃作弱[MASK]其实我们本不是一枝花[SEP][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.18it/s]\n",
      "======================================== SAMPLE 43 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我的梦之蛇[MASK][MASK]以及你的手[MASK]在这个寒冷的冬天[MASK]如今只剩下我一个[MASK]为你祝福[MASK][MASK]那就是你的梦之蛇[MASK]让我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.04it/s]\n",
      "======================================== SAMPLE 44 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]和你的手[MASK]为了美丽[MASK]我砸了一个坑[MASK][MASK]也是为了下雨[MASK][MASK]清亮的积水上[MASK]高一只[MASK]低一只[MASK]小雨儿如鸟[MASK][MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.14it/s]\n",
      "======================================== SAMPLE 45 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的宽容[MASK]你的细腻[MASK]像小孩子[MASK]在民歌中[MASK]我们共同啜饮[MASK]我们共同啜饮[MASK]我们共同啜饮[MASK]我们共同啜饮[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 138.62it/s]\n",
      "======================================== SAMPLE 46 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用担心[MASK]因为我是你的[MASK][MASK]我是你的[MASK][MASK]我是你的[MASK][MASK]我是你的[MASK][MASK]我是你的[MASK][MASK]我是靠着你[MASK][MASK]我是在\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.05it/s]\n",
      "======================================== SAMPLE 47 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对面的小山[MASK][MASK]我们全都背叛自己的故乡[MASK]我们会把自己的故乡抛在一边[MASK]把自己的泪水一边[MASK]我们会把幸福\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 139.26it/s]\n",
      "======================================== SAMPLE 48 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]这就是我张开手指所要叙说的故事[MASK]那洞窟不会在今夜关闭。明天夜晚也不会关闭[MASK]额头披满钟声的[MASK]土地[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 131.72it/s]\n",
      "======================================== SAMPLE 49 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于一个无比幸福的人[MASK][MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]那个庸俗的故事[MASK]使用货币或麦子[MASK]卖鱼的卖鱼\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 124.97it/s]\n",
      "======================================== SAMPLE 50 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为你祝福[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; bash generate_from_finetuned.sh [MASK][MASK][MASK]月亮升起 80; cd ../..;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 weijing weijing 376M Apr 16 16:35 model_finetuned/model_epoch100/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; ls -laht model_finetuned/model_epoch100/pytorch_model.bin; cd ../..;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0418 16:22:10.876216 140089112344384 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=1, device='0', fast_pattern=False, length=50, model_config='tasks/dataaugmentation/model_finetuned/model_epoch119//model_config.json', model_path='tasks/dataaugmentation/model_finetuned/model_epoch119/', no_wordpiece=False, nsamples=50, prefix='[MASK][MASK][MASK]宽阔的大海', repetition_penalty=1.0, save_samples=True, save_samples_path='tasks/dataaugmentation/outputs_finetuned/', segment=False, temperature=0.8, tokenizer_path='tasks/dataaugmentation/pretrained_model/vocab.txt', topk=50, topp=0)\n",
      "I0418 16:22:11.161861 140089112344384 configuration_utils.py:148] loading configuration file tasks/dataaugmentation/model_finetuned/model_epoch119/config.json\n",
      "I0418 16:22:11.162296 140089112344384 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0418 16:22:11.163034 140089112344384 modeling_utils.py:334] loading weights file tasks/dataaugmentation/model_finetuned/model_epoch119/pytorch_model.bin\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 132.03it/s]\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海的想象[MASK]以前的我们是否独自一人[MASK]为什么有这样的简朴[MASK]为什么有这样的简朴[MASK]我们就义无反顾[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.68it/s]\n",
      "======================================== SAMPLE 2 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺在另一个世界上[MASK]我为你祝福[MASK]我是赠给你的爱情[MASK][MASK]我是赠给你的子弹[MASK][MASK]我是赠给你的子弹[MASK][MASK]我是\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.66it/s]\n",
      "======================================== SAMPLE 3 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？那是一片寂静的天空[MASK][MASK]我躺在你的身子上[MASK]想起了许多[MASK][MASK]有一只小蜜蜂[MASK]吹在你的身子[MASK][MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.11it/s]\n",
      "======================================== SAMPLE 4 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]面对棵棵绿树[MASK]坐着[MASK]一动不动[MASK]汽车声音响起在[MASK]脊背上[MASK]我这就想把我这[MASK]盖满落叶的旧外套[MASK]寄给这城\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.88it/s]\n",
      "======================================== SAMPLE 5 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]如果我是一只船[MASK]我是一只船[MASK]为了一只[MASK]我抱着三只小船[MASK]为了光明[MASK]我是一只船[MASK]为了光明[MASK]我是一只蜂\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.14it/s]\n",
      "======================================== SAMPLE 6 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]等你[MASK]我[MASK]半截拉着大海[MASK][MASK]半截拉着[MASK]你搓手[MASK]在沙滩上画了许多匹子[MASK][MASK]半截拉着裤带的手[MASK][MASK]我的肩膀\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.05it/s]\n",
      "======================================== SAMPLE 7 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我感到魅惑[MASK]我打开窗户[MASK]去寻找下一个人[MASK][MASK]我感到魅惑[MASK][MASK]我感到魅惑[MASK]我就想在这条魅惑之河上渡过我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.03it/s]\n",
      "======================================== SAMPLE 8 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你在渔市上[MASK]寻找下弦月[MASK]我在月光下[MASK]经过小河流[MASK][MASK]你在婚礼上[MASK]使用红筷子[MASK]我在向阳坡[MASK]栽下两行竹\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 133.21it/s]\n",
      "======================================== SAMPLE 9 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你象一只美丽的蝴蝶[MASK]落在我的头发上[MASK][MASK]美丽的荷尔德林[MASK][MASK]我头发在微雨中[MASK]飘动[MASK]我知道你就在我的头\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.00it/s]\n",
      "======================================== SAMPLE 10 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺倒在血泊中[MASK]我在四肢安排下[MASK]我在水中[MASK]经过小女人的手中[MASK]为了美丽[MASK]我掀起了一扇门[MASK][MASK]我在人间\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.76it/s]\n",
      "======================================== SAMPLE 11 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我是一只鱼筐[MASK]想着另一只鱼筐[MASK]想着另一只鱼[MASK][MASK]而无人到处诉说[MASK][MASK]等你[MASK]我是鱼筐中的火苗[MASK]放在溪水\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.97it/s]\n",
      "======================================== SAMPLE 12 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]或我的梦中[MASK]闯进另一个人的内心[MASK]让我在此时此刻[MASK]为了美丽[MASK]我愿意[MASK]我在这里[MASK]生下了一个决定[MASK]也是\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 130.43it/s]\n",
      "======================================== SAMPLE 13 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你脱下黑色的双腿[MASK]我从茫茫的征途中[MASK]来到这座大海[MASK][MASK]我有三次渡过这座大海[MASK][MASK]我有三次渡过这座大海\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.95it/s]\n",
      "======================================== SAMPLE 14 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.03it/s]\n",
      "======================================== SAMPLE 15 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了一份美丽[MASK]我愿意[MASK]为了一个弃我的士兵[MASK]为了一个弃我的士兵[MASK][MASK]伤我[MASK]就是那个溺我的人[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.07it/s]\n",
      "======================================== SAMPLE 16 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海的诗歌中[MASK][MASK]我们是最平凡的一个[MASK]有的平凡得不能再平凡[MASK][MASK]我们为什么还要结束[MASK]我们就象树根\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.07it/s]\n",
      "======================================== SAMPLE 17 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你象一个穿着花布的人[MASK]在木桶里饮水[MASK][MASK]你脱下破旧的袜子[MASK]我象一个粗布上沾满水的人[MASK][MASK]我是一个赤裸\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.54it/s]\n",
      "======================================== SAMPLE 18 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？那个人[MASK]和你的爱人[MASK]我是荒野上第一根被晒坏的石柱[MASK][MASK]我是荒野上第一根被晒坏的石柱[MASK][MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.34it/s]\n",
      "======================================== SAMPLE 19 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]其实我是一个粗枝大叶的人[MASK]我觉得自己很美[MASK]我也喜欢我自己[MASK]我也不算是一个高人[MASK]一只踩入囊中[MASK][MASK]我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.59it/s]\n",
      "======================================== SAMPLE 20 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的黑色主人[MASK]用雪水[MASK]打开木头[MASK]就是打开木头[MASK][MASK]你的黑色主人[MASK]就是把血[MASK]烧伤[MASK]你的黑色主人[MASK][MASK]就\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.58it/s]\n",
      "======================================== SAMPLE 21 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对你的爱人[MASK]我的梦中[MASK]我们共同在海滩上行走[MASK][MASK]我们是带着甜蜜的[MASK]真正的平静[MASK]我们是带着梦想的少女\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.66it/s]\n",
      "======================================== SAMPLE 22 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]只有一只鱼[MASK][MASK]半截用水[MASK]用河水做成的妻子[MASK]因为我是鱼[MASK]半截用水[MASK]粘着你[MASK]我的嘴唇[MASK]填满你的嘴唇[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.62it/s]\n",
      "======================================== SAMPLE 23 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]先生的黑色主人[MASK]拉着大海壶[MASK]拉着大海壶[MASK]拉着爱人的手[MASK]手[MASK]径直走进房屋[MASK][MASK]水桶中央[MASK]先生[MASK]瓷的小\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.46it/s]\n",
      "======================================== SAMPLE 24 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我怀抱妻子[MASK]就象水儿抱鱼[MASK]我一边伸出手去[MASK]试着摸到小雨水，并且嘴唇开花[MASK][MASK]而鱼是哑女人[MASK]睡在河水\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.36it/s]\n",
      "======================================== SAMPLE 25 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？那也足够了[MASK][MASK]我们全都背叛自己的故乡[MASK]我们会把幸福当成祖传的职业[MASK]放下手中痛苦的诗篇\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.68it/s]\n",
      "======================================== SAMPLE 26 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.04it/s]\n",
      "======================================== SAMPLE 27 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]我愿意是一个傻子[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]一生一世[MASK][MASK]付出你\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.59it/s]\n",
      "======================================== SAMPLE 28 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海来说，水只是一种水[MASK][MASK]宽阔的胸脯[MASK]用泥土筑起的房屋[MASK]我的心情[MASK]也就是说，大海来了[MASK]我的心\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.56it/s]\n",
      "======================================== SAMPLE 29 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]等你[MASK]我[MASK]渡过一个无比幸福的早晨[MASK][MASK]我足够与你[MASK]我在这里[MASK]划分着[MASK]我在这里[MASK]我在这里[MASK]我在这里[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.43it/s]\n",
      "======================================== SAMPLE 30 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你躺着，喘息[MASK]我在海水中[MASK]洗洗双手[MASK]我知道，这是一片埋着海的自己[MASK][MASK]我的存在[MASK]我也知道[MASK]海水没有\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.32it/s]\n",
      "======================================== SAMPLE 31 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你看见了吗？那就是你的水[MASK]我的乳房[MASK][MASK]我的乳房[MASK]从一颗榕树下走过[MASK]你的乳房[MASK]我们和植物一样幸福[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.99it/s]\n",
      "======================================== SAMPLE 32 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海来说[MASK]没有任何的不幸[MASK][MASK]我们确实被太阳烤焦，秋天的月亮[MASK]却如同天堂的马匹[MASK]和我们自己[MASK]抱\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.46it/s]\n",
      "======================================== SAMPLE 33 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我感到魅惑[MASK]我就想把你放在我的身上[MASK]我感到魅惑[MASK]美丽女儿，一流到底[MASK]水儿仍旧从高向低[MASK][MASK]坐在三条\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.58it/s]\n",
      "======================================== SAMPLE 34 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海来说[MASK]简洁的诗人[MASK]以及诗人的远离故乡的噪音[MASK][MASK]宽阔的胸脯[MASK]用手指推推[MASK]把手指推向沙滩[MASK]我\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.20it/s]\n",
      "======================================== SAMPLE 35 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你象一片又瘦又长的树叶[MASK]落上稻草：[MASK]我病了[MASK]我不要你接受[MASK][MASK]我是赠给你的爱情[MASK]我是赠给你的子弹[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.64it/s]\n",
      "======================================== SAMPLE 36 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.67it/s]\n",
      "======================================== SAMPLE 37 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]愿意像一片叶子[MASK]落在我的头发上[MASK][MASK]脊背上有一只鸟[MASK]或远或近[MASK]－－[MASK][MASK]三丈母亲[MASK]一碗水[MASK]半\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.15it/s]\n",
      "======================================== SAMPLE 38 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于一个无比幸福的人[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]再次死在荒凉的地方[MASK]那么多的荒凉[MASK]在我的身上[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 135.74it/s]\n",
      "======================================== SAMPLE 39 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.29it/s]\n",
      "======================================== SAMPLE 40 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海来说，是一份甜蜜的结局[MASK][MASK]我感到魅惑[MASK]我们确实被太阳烤焦，秋天内外[MASK]我们更像是被晒焦，夏\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 130.66it/s]\n",
      "======================================== SAMPLE 41 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]头破血流[MASK][MASK]我愿意[MASK]面朝大海[MASK][MASK]我愿意[MASK]身体\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.50it/s]\n",
      "======================================== SAMPLE 42 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]这就是我张开手指所要叙说的故事[MASK]那洞窟不会在今夜关闭。明天夜晚也不会关闭[MASK]额头披满钟声的[MASK]土地[MASK]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.54it/s]\n",
      "======================================== SAMPLE 43 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你不用算命[MASK]命早就在算你[MASK]你举着筷子[MASK]你坐在碗沿上[MASK]你脱下黑色的女靴[MASK]就盖住城市的尸体[MASK]你裹着布\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.39it/s]\n",
      "======================================== SAMPLE 44 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对于大海的枣树[MASK]给我一个甜蜜的记忆[MASK][MASK]我感到魅惑[MASK]容易于口甘泉[MASK][MASK]我感到魅惑[MASK]小溪流水[MASK]在我的身\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.77it/s]\n",
      "======================================== SAMPLE 45 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]我们偶然相遇[MASK]没有留下痕迹[MASK][MASK]那个庸俗的故事[MASK]使用货币或麦子[MASK]卖鱼\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.00it/s]\n",
      "======================================== SAMPLE 46 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]我愿意[MASK]为了美丽[MASK]我愿意[MASK]我愿意[MASK]望着你[MASK]我愿意[MASK]我愿意倾听[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.55it/s]\n",
      "======================================== SAMPLE 47 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的心海[MASK]让我来到这座城市[MASK]也是一个公民[MASK][MASK]你的心上人[MASK]也是一个孝顺[MASK][MASK]我就是[MASK]两座旧房子[MASK]一个\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.64it/s]\n",
      "======================================== SAMPLE 48 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]你的一生[MASK]也可以说是没有幸福[MASK][MASK]我的亲人[MASK]一生的战友[MASK]在天堂里[MASK]我为你祈祷[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 137.06it/s]\n",
      "======================================== SAMPLE 49 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]等你[MASK]我[MASK]渡过两座驿站[MASK]我在意外[MASK]让你知道[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "100%|██████████████████████████████████████████| 50/50 [00:00<00:00, 136.91it/s]\n",
      "======================================== SAMPLE 50 ========================================\n",
      "\n",
      "[MASK][MASK][MASK]宽阔的大海[MASK][MASK]对我的窗户里埋着一只为你祝福的杯子[MASK][MASK]那是我最后一次想起的中午[MASK]那是我沉下海水的尸体[MASK]回忆起一个\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/dataaugmentation; bash generate_from_finetuned.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weijing/git/GPT2-Chinese\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
