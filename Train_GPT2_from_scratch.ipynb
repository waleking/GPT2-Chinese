{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所需环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers版本号为2.1.1，pytorch为1.2.0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0415 15:24:11.910086 140436257081152 file_utils.py:39] PyTorch version 1.2.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在requirments.txt中，列出了所需要的其他的包，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==2.1.1\n",
      "torch\n",
      "numpy\n",
      "tqdm\n",
      "sklearn\n",
      "keras\n",
      "tb-nightly\n",
      "future\n",
      "thulac\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录结构以及重要文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前文件夹中包含的文件如下。其中**doupo**文件夹下包含“斗破苍穹”的示例任务，用以简单说明如何构建字典、tokenization、以及在一个小文件上从头训练一个指定层数的GPT模型；**pretrain**文件夹下包含预训练任务，主要用以说明如何在260万篇新闻文档上预训练一个12层的GPT2；**dataaugmentation**文件夹下包含数据增强任务，主要用以说明如何利用预训练的GPT2模型在较小的文档集上进行微调，并用于数据增强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "├── cache\n",
      "├── config\n",
      "├── dataaugmentation\n",
      "├── eval.py\n",
      "├── generate.py\n",
      "├── generate_texts.py\n",
      "├── LICENSE\n",
      "├── pretrain\n",
      "├── README.md\n",
      "├── requirements.txt\n",
      "├── sample\n",
      "├── tasks\n",
      "├── tokenizations\n",
      "├── Train_GPT2_from_scratch.ipynb\n",
      "├── train.json\n",
      "├── train_on_small_file.py\n",
      "└── train_single.py\n",
      "\n",
      "7 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tree -L 1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要重点加以说明的是，上述列表中的tokenizations文件夹，train_single.py, 和train_on_small_file.py。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizations`提供了各种tokenization的方法，具体到本代码库中，使用的是该文件夹中的`tokenization_chars.py`文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization的第一阶段是将纯文本文件依照字典文件vocab.txt进行划分为token，第二阶段是将token转化为字典文件中token对应的数字。举例来说，\"[CLS]《斗破苍穹》天蚕土豆[SEP][CLS] ...\"在第一阶段被分割为列表：['[CLS]', '《', '斗', '破', '苍', '穹', '》', '天', '蚕', '土', '豆', '[SEP]', '[CLS]', ...]，其中[CLS], [SEP]在vocab.txt对应一个单独的占位符；第二阶段在查找vocab.txt之后，将字符串列表转化为整型数列表：101 517 3159 4788 5721 4957 518 1921 6014 1759 6486 102 101。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下是tokenization第一阶段的代码，输入为text字符串，输出为字符串列表result，对于形如[.\\*]的特殊字符使用栈来处理：如果该特殊字符出现在vocab.txt中，则被提取为一个单独的字符串存入列表中，如161行所示；反之若没有出现在vocab.txt中，则逐字符加入到列表中，如163-165行所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140     def _tokenize(self, text):\n",
      "141         #pdb.set_trace()\n",
      "142         stack = []\n",
      "143 \n",
      "144         result = []\n",
      "145         i = 0\n",
      "146         len_txt = len(text)\n",
      "147         while(i<len_txt):\n",
      "148             char = text[i]\n",
      "149             if(len(stack)==0):\n",
      "150                 if(char != '['):\n",
      "151                     result.append(char)\n",
      "152                 else:\n",
      "153                     stack.append('[')\n",
      "154             else:\n",
      "155                 if(char == ']'):\n",
      "156                     stack.append(char) # don't forget to append it firstly\n",
      "157                     # process the content in the stack\n",
      "158                     seq = \"\".join(stack)\n",
      "159                     if(seq in self.vocab):\n",
      "160                         # the [.*] stuff appeared in the vocabulary, e.g. [UNK], [CLS], [SEP], ...\n",
      "161                         result.append(seq)\n",
      "162                     else:\n",
      "163                         # other [.*] stuff which are not valid element in the vocabulary\n",
      "164                         for e in stack:\n",
      "165                             result.append(e)\n",
      "166                     # clean the stack after processing \n",
      "167                     stack = []    \n",
      "168                 else:\n",
      "169                     stack.append(char)\n",
      "170             i += 1\n",
      "171 \n",
      "172         if(len(stack)>0): \n",
      "173             for e in stack:\n",
      "174                 result.append(e)\n",
      "175         # pdb.set_trace()\n",
      "176         return result\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=140 and $.<=176)' tokenizations/tokenization_chars.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization的第二部分如下所示，对第一阶段获得的result列表中的token字符串进行查表操作，如果没有出现在vocab.txt中，那么就以[UNK]对应的id来代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178     def _convert_token_to_id(self, token):\n",
      "179         \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
      "180         #if(token not in self.vocab):\n",
      "181         #    print(token)\n",
      "182         return self.vocab.get(token, self.vocab.get(self.unk_token))\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=178 and $.<=182)' tokenizations/tokenization_chars.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，本代码库中的tokenization方法主要为中文设计，如果训练数据中混杂有英文字符，那么就将英文单词按照char level进行划分，例如\"word\"会被\"w\",\"o\",\"r\",\"d\"四个字符来代替。不同于BPE的方法，我们认为这样处理是最节省计算资源的，比起BPE能够更从容应对几十GB的中文训练语料。从具体的实际效果来看，在斗破苍穹的语料上进行tokenization，普通方法需要耗时77.9秒，使用了我们的方法之后，耗时12.1秒，用时为原来的15%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两种训练方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_single.py`：当输入的训练数据为一篇完整的长文档（例如小说）或是一个单一的大型文档，此时使用`train_single.py`。有两方面的原因。（1）train_single.py在训练过程中不对training samples进行random shuffle，保持training samples之间的顺序，因此完整的长文档使用train_single.py。（2）对顺序无关的超多的样本，例如包含260万篇新闻的大型训练集，在训练过程中进行random shuffle的代价是巨大的，包括shuffle的代价，重新划分training sample的代价和tokenization的代价，并且在大型训练集上我们进行预训练的轮数有限，1到2轮就能获得一个较好的预训练模型。综合考虑shuffle的代价和收益，因此也将大型训练集中的多个样本拼接为一个单一的大型文档，使用train_single.py。我们在《从头训练一个GPT2模型》一节对此进行更详细的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_on_small_file.py`：当输入的训练数据为多个样本，并且这多个样本的总体积较小时，将这些样本汇总到一个文件，每个样本占据一行，然后使用`train_on_small_file.py`。有如下两方面原因：（1）train_on_small_file.py针对每个样本建立一个单独的training sample送入Transformer Encoder中，如果一个样本的长度不足Transformer Encoder的长度（在本代码库中用n_ctx表示），那么不足部分以\\[PAD\\]来补足；超出n_ctx的部分直接截断丢弃不用。这一点和train_single.py不同，train_single.py中多个较短的样本可能会占据一个Transformer Encoder的输入长度（n_ctx）。（2）train_on_small_file.py会在不同的训练轮次中，对training samples进行random shuffle，以提高训练质量。由于train_on_small_file.py的训练数据体积较小，样本数也较小，因此每轮进行random shuffle是完全可行的。所以，如果是唐诗、宋词、现代诗、意图增强等文本数据，由于其规模较小，完全可以使用`train_on_small_file.py`来完成训练或者微调，这样得到的模型质量要高于`train_single.py`训练得到的模型。我们在《微调GPT2模型进行数据增强》一节对此进行更详细的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从头训练一个GPT2模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们使用一个规模较小的文件用以验证从头训练一个GPT2模型的可行性。该文件为斗破苍穹小说文本文件，规模为16MB，16万行。如下是该文件的一些基本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 weijing weijing 16M Apr  5 10:54 rawdata/train_raw.txt\n",
      "162111 rawdata/train_raw.txt\n",
      "《斗破苍穹》天蚕土豆\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.shuyaya.com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "在线阅读：http://www.shuyaya.com/read/18/\n",
      "--------------------------------------------------\n",
      "\n",
      "第一章 陨落的天才\n",
      "\n",
      "    “斗之力，三段！”\n",
      "\n",
      "    望着测验魔石碑上面闪亮得甚至有些刺眼的五个大字，少年面无表情，唇角有着一抹自嘲，紧握的手掌，因为大力，而导致略微尖锐的指甲深深的刺进了掌心之中，带来一阵阵钻心的疼痛…\n",
      "\n",
      "    “萧炎，斗之力，三段！级别：低级！”测验魔石碑之旁，一位中年男子，看了一眼碑上所显示出来的信息，语气漠然的将之公布了出来…\n",
      "\n",
      "    中年男子话刚刚脱口，便是不出意外的在人头汹涌的广场上带起了一阵嘲讽的骚动。\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; ls -laht rawdata/train_raw.txt; wc -l rawdata/train_raw.txt; head -n 15 rawdata/train_raw.txt; echo \"...\"; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有rawdata/train_raw.txt文件，那么运行`!cd tasks/doupo; bash train.sh`，该脚本帮助建立对应文件夹并下载train_raw.txt。如下是train.sh的部分信息。首先创建raw_data文件夹，divide文件夹，tokenized文件夹，model文件夹和config文件夹。其中，raw_data文件夹用于存储原始的训练数据和进行基本预处理之后的训练数据；divide文件夹用于存储分割后的大文件（在train_single.py中有体现）；tokenized文件夹用于存储token转为数字id之后的文件；model文件夹用于储存训练好的模型；config文件夹用于储存模型的基本配置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tasks/doupo/train.sh`的第26行用于从公网下载原始训练数据并存储到rawdata文件夹下的train_raw.txt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 job_dir=\"tasks/doupo\"\n",
      "2 \n",
      "3 cd ../..\n",
      "4 \n",
      "5 if [ ! -e $job_dir/rawdata ]; then\n",
      "6     mkdir $job_dir/rawdata\n",
      "7 fi\n",
      "8 \n",
      "9 if [ ! -e $job_dir/divide ]; then\n",
      "10     mkdir $job_dir/divide\n",
      "11 fi\n",
      "12 \n",
      "13 if [ ! -e $job_dir/tokenized ]; then\n",
      "14     mkdir $job_dir/tokenized\n",
      "15 fi\n",
      "16 \n",
      "17 if [ ! -e $job_dir/model ]; then\n",
      "18     mkdir $job_dir/model\n",
      "19 fi\n",
      "20 \n",
      "21 if [ ! -e $job_dir/config ]; then\n",
      "22     mkdir $job_dir/config\n",
      "23 fi\n",
      "24 \n",
      "25 if [ ! -e $job_dir/rawdata/train.txt ]; then\n",
      "26     wget -c -O $job_dir/rawdata/train_raw.txt https://github.com/GaoPeng97/transformer-xl-chinese/blob/master/data/doupo/train.txt?raw=true\n",
      "27     # remove empty lines\n",
      "28     python $job_dir/format_raw_txt.py $job_dir/rawdata/train_raw.txt $job_dir/rawdata/train.txt \n",
      "29 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=1 and $.<=29)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tasks/doupo/train.sh`的第28行调用`format_raw_txt.py`，用于移除原始文件中的空行和去除一行中左侧多余的空格(lstrip)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t'''\n",
      "     2\tThis script is used to remove empty lines in the input file \n",
      "     3\t'''\n",
      "     4\timport sys\n",
      "     5\t\n",
      "     6\tdef format(in_filename, out_filename):\n",
      "     7\t    with open(out_filename, \"w\") as fOut:\n",
      "     8\t        with open(in_filename, \"r\") as fIn:\n",
      "     9\t            for line in fIn:\n",
      "    10\t                if(line!=\"\"):\n",
      "    11\t                    fOut.write(line.lstrip())\n",
      "    12\t\n",
      "    13\t\n",
      "    14\tdef test():\n",
      "    15\t    in_filename=\"./rawdata/train_raw.txt\"\n",
      "    16\t    out_filename=\"./rawdata/train.txt\"\n",
      "    17\t    format(in_filename, out_filename)\n",
      "    18\t\n",
      "    19\t\n",
      "    20\tif __name__==\"__main__\":\n",
      "    21\t    if(len(sys.argv)<3):\n",
      "    22\t        print(\"usage: python format_raw_txt.py in_filename out_filename\")\n",
      "    23\t    else:\n",
      "    24\t        in_filename = sys.argv[1]\n",
      "    25\t        out_filename = sys.argv[2]\n",
      "    26\t        format(in_filename, out_filename)\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; cat -n format_raw_txt.py; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前多个模型都直接使用了BERT chinese字表，该字表拥有字符21128个。但是该字表有如下的问题：不包含大写字母A到Z，不包含制表符，空格等字符。我们可以将这些字符追加到BERT chinese字表之后，如`train.sh`代码中的第31行到44行所示，将新的字符表保存在doupo/config/vocab.txt文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 vocab_size=21128\n",
      "32 declare -a additional_chars=(\"“\" \"”\" \"…\" \"’\" \"‘\" \"—\" \" \" \"\\t\" \"\\`\")\n",
      "33 new_vocab_size=$(($vocab_size+${#additional_chars[@]}+26))\n",
      "34 echo 'setting config/vocab.txt and config/model_config.json'\n",
      "35 if [ ! -e $job_dir/config/vocab.txt ]; then\n",
      "36     wget -c -O $job_dir/config/vocab.txt https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\n",
      "37     # append new characters to the vocabulary\n",
      "38     for letter in \"${additional_chars[@]}\"; do\n",
      "39         echo -e \"$letter\" >> $job_dir/config/vocab.txt\n",
      "40     done\n",
      "41     for letter in {A..Z} ; do\n",
      "42         echo $letter >> $job_dir/config/vocab.txt\n",
      "43     done\n",
      "44 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=31 and $.<=44)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以进一步的设置模型参数，并将模型参数保存于config/model_config.json。如果不存在doupo/config/model_config.json，那么doupo/train.sh的第49行拷贝config/model_config.json模板到指定位置，并进行编辑。如下的代码第52，54，56，57行用于编辑模型参数，例如将层数修改为10层，将n_ctx和n_positions从1024修改为512。因为n_ctx是Transformer Encoder能够容纳的最大序列长度，因此减半之后可以大幅度节省训练时占用的显存，同时也考虑到512是一个合理的较长的长度，能够较大限度的捕获文本上远距离的依赖关系。此外我们将stride设置为256，也就是在一个长序列上窗口大小为512，每次移动窗口的幅度为256。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 stride=256\n",
      "47 n_layers=10\n",
      "48 n_ctx=512\n",
      "49 if [ ! -e $job_dir/config/model_config.json ]; then\n",
      "50     cp config/model_config.json $job_dir/config/model_config.json\n",
      "51     # change vocabulary size\n",
      "52     perl -pi -e 's/'$vocab_size'/'$new_vocab_size'/g' $job_dir/config/model_config.json\n",
      "53     # change the number of layers from 12 to $n_layers\n",
      "54     perl -pi -e 's/\"n_layer\": 12/\"n_layer\": '$n_layers'/g' $job_dir/config/model_config.json\n",
      "55     # change the model input length from 1024 to $n_ctx\n",
      "56     perl -pi -e 's/\"n_ctx\": 1024/\"n_ctx\": '$n_ctx'/g' $job_dir/config/model_config.json\n",
      "57     perl -pi -e 's/\"n_positions\": 1024/\"n_positions\": '$n_ctx'/g' $job_dir/config/model_config.json\n",
      "58 fi\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; perl -ne 'print \"$. $_\" if ($.>=46 and $.<=58)' train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在`config/model_config.json`中查看即将要训练的模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t{\n",
      "     2\t  \"initializer_range\": 0.02,\n",
      "     3\t  \"layer_norm_epsilon\": 1e-05,\n",
      "     4\t  \"n_ctx\": 512,\n",
      "     5\t  \"n_embd\": 768,\n",
      "     6\t  \"n_head\": 12,\n",
      "     7\t  \"n_layer\": 10,\n",
      "     8\t  \"n_positions\": 512,\n",
      "     9\t  \"vocab_size\": 21163\n",
      "    10\t}"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; cat -n config/model_config.json; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要的数据预处理发生于`train_single.py`的`build_files`文件中，如下所示。`build_files`包含有多个参数，其中有必要说明的是`num_pieces`和`full_tokenizer`。`full_tokenizer`调用的是《Tokenization》一小节中提到的两阶段方法，效率较高。`num_pieces`将一个大文件分割为若干个小文件，以减小训练时IO一个大文件带来的内存压力。\n",
    "如下，第26行到第35行用以分割大文件到divide文件夹。分割之后，每一行补充[CLS]字符，回车符'\\\\n'转为[SEP]字符，然后调用`full_tokenizer`将token转为id。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例来说，\"《斗破苍穹》天蚕土豆\"这一行转为\"[CLS]《斗破苍穹》天蚕土豆[SEP]\"并转为对应的数字id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 def build_files(raw_data_path, divide_path, tokenized_data_path, full_tokenizer, num_pieces):\n",
      "19     if not os.path.exists(tokenized_data_path):\n",
      "20         os.mkdir(tokenized_data_path)\n",
      "21     if not os.path.exists(divide_path):\n",
      "22         os.mkdir(divide_path)\n",
      "23     print(\"now time: \", datetime.now())\n",
      "24     print(\"begin to divide raw text ...\")\n",
      "25 \n",
      "26     writers = [open(divide_path + 'divide_piece_{}.txt'.format(i), 'w') for i in range(0,num_pieces)]\n",
      "27 \n",
      "28     with open(raw_data_path, 'r', encoding='utf8') as f:\n",
      "29         line_num = 0\n",
      "30         for line in f:\n",
      "31             writers[line_num % num_pieces].write(\"%s\" % line)\n",
      "32             line_num += 1\n",
      "33     \n",
      "34     for i in range(0, num_pieces):\n",
      "35         writers[i].close()\n",
      "36     \n",
      "37     print('now time: ', datetime.now())\n",
      "38     print(\"begin making tokenization ...\")\n",
      "39     files = [filename for filename in os.listdir(divide_path) if f!='.gitignore']\n",
      "40     for i, filename in enumerate(files):\n",
      "41         if(os.path.isdir(filename)):\n",
      "42             continue\n",
      "43 \n",
      "44         with open(divide_path+filename, 'r', encoding='utf8') as reader:\n",
      "45             print(\"reading file {}, now time is {}\".format(filename, datetime.now()))\n",
      "46             lines = []\n",
      "47             for line in reader:\n",
      "48                 line = line.replace('\\n', '[SEP]')\n",
      "49                 line = '[CLS]' + line\n",
      "50                 lines.append(line)\n",
      "51             \n",
      "52             single_file = ''.join(lines)\n",
      "53             single_ids = full_tokenizer.convert_tokens_to_ids(full_tokenizer._tokenize(single_file))\n",
      "54             # pdb.set_trace()\n",
      "55             with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'w') as f:\n",
      "56                 for id in single_ids[:-1]:\n",
      "57                     f.write(str(id) + ' ')\n",
      "58                 f.write(str(single_ids[-1]))\n",
      "59                 f.write('\\n')\n",
      "60         print('now time: {}, tokenized tokenized_train_{}.txt'.format(datetime.now(), i))\n",
      "61 \n",
      "62     print('finish')\n"
     ]
    }
   ],
   "source": [
    "!perl -ne 'print \"$. $_\" if ($.>=18 and $.<=62)' train_single.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting config/vocab.txt and config/model_config.json\n",
      "I0415 15:27:52.021245 140467110659904 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=32, device='0,1', divide_path='tasks/doupo/divide/', epochs=100, fp16=False, fp16_opt_level='O1', gradient_accumulation=1, ignore_intermediate_epoch_model=True, log_step=100, lr=0.00015, max_grad_norm=1.0, model_config='tasks/doupo/config/model_config.json', num_pieces=1, output_dir='tasks/doupo/model/', pretrained_model='', raw=False, raw_data_path='tasks/doupo/rawdata/train.txt', segment=False, stride=256, tokenized_data_path='tasks/doupo/tokenized/', tokenizer_path='tasks/doupo/config/vocab.txt', warmup_steps=2000)\n",
      "config:\n",
      "{\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 512,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21163\n",
      "}\n",
      "\n",
      "using device: cuda\n",
      "calculating total steps\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "total steps = 65418\n",
      "Let's use 2 GPUs!\n",
      "/home/weijing/.conda/envs/weijing-torch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:26: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "starting training\n",
      "epoch 1\n",
      "time: 2020-04-15 15:27:58.306466\n",
      "/home/weijing/.conda/envs/weijing-torch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "now time: 15:30. Step 100 of piece 0 of epoch 1, loss 9.187442922592163\n",
      "now time: 15:32. Step 200 of piece 0 of epoch 1, loss 7.699173674583435\n",
      "now time: 15:34. Step 300 of piece 0 of epoch 1, loss 6.2270995712280275\n",
      "now time: 15:37. Step 400 of piece 0 of epoch 1, loss 5.412385096549988\n",
      "now time: 15:39. Step 500 of piece 0 of epoch 1, loss 4.962663960456848\n",
      "now time: 15:41. Step 600 of piece 0 of epoch 1, loss 4.658888983726501\n",
      "epoch 2\n",
      "time: 2020-04-15 15:43:03.006114\n",
      "now time: 15:45. Step 100 of piece 0 of epoch 2, loss 6.769504337310791\n",
      "now time: 15:47. Step 200 of piece 0 of epoch 2, loss 4.2193977785110475\n",
      "now time: 15:50. Step 300 of piece 0 of epoch 2, loss 4.1231858348846435\n",
      "now time: 15:52. Step 400 of piece 0 of epoch 2, loss 4.030537283420562\n",
      "now time: 15:54. Step 500 of piece 0 of epoch 2, loss 3.9331782174110415\n",
      "now time: 15:57. Step 600 of piece 0 of epoch 2, loss 3.8633244395256043\n",
      "epoch 3\n",
      "time: 2020-04-15 15:58:18.507320\n",
      "now time: 16:0. Step 100 of piece 0 of epoch 3, loss 5.757383127212524\n",
      "now time: 16:2. Step 200 of piece 0 of epoch 3, loss 3.6136479139328004\n",
      "now time: 16:5. Step 300 of piece 0 of epoch 3, loss 3.5312264466285708\n",
      "now time: 16:7. Step 400 of piece 0 of epoch 3, loss 3.4566034865379334\n",
      "now time: 16:9. Step 500 of piece 0 of epoch 3, loss 3.36283864736557\n",
      "now time: 16:12. Step 600 of piece 0 of epoch 3, loss 3.2708581280708313\n",
      "epoch 4\n",
      "time: 2020-04-15 16:13:34.353918\n",
      "now time: 16:15. Step 100 of piece 0 of epoch 4, loss 4.8621430230140685\n",
      "now time: 16:18. Step 200 of piece 0 of epoch 4, loss 3.062667396068573\n",
      "now time: 16:20. Step 300 of piece 0 of epoch 4, loss 3.0098688435554504\n",
      "now time: 16:22. Step 400 of piece 0 of epoch 4, loss 2.9461682271957397\n",
      "now time: 16:25. Step 500 of piece 0 of epoch 4, loss 2.890516037940979\n",
      "now time: 16:27. Step 600 of piece 0 of epoch 4, loss 2.8562480306625364\n",
      "epoch 5\n",
      "time: 2020-04-15 16:28:50.528359\n",
      "now time: 16:31. Step 100 of piece 0 of epoch 5, loss 4.24622195482254\n",
      "now time: 16:33. Step 200 of piece 0 of epoch 5, loss 2.700613079071045\n",
      "now time: 16:35. Step 300 of piece 0 of epoch 5, loss 2.678796765804291\n",
      "now time: 16:38. Step 400 of piece 0 of epoch 5, loss 2.651676044464111\n",
      "now time: 16:40. Step 500 of piece 0 of epoch 5, loss 2.6254628038406373\n",
      "now time: 16:42. Step 600 of piece 0 of epoch 5, loss 2.5909028172492983\n",
      "epoch 6\n",
      "time: 2020-04-15 16:44:06.052810\n",
      "now time: 16:46. Step 100 of piece 0 of epoch 6, loss 3.8709467482566833\n",
      "now time: 16:48. Step 200 of piece 0 of epoch 6, loss 2.473355031013489\n",
      "now time: 16:51. Step 300 of piece 0 of epoch 6, loss 2.4580682468414308\n",
      "now time: 16:53. Step 400 of piece 0 of epoch 6, loss 2.43311398267746\n",
      "now time: 16:55. Step 500 of piece 0 of epoch 6, loss 2.4281823897361754\n",
      "now time: 16:58. Step 600 of piece 0 of epoch 6, loss 2.4086236548423767\n",
      "epoch 7\n",
      "time: 2020-04-15 16:59:21.544525\n",
      "now time: 17:1. Step 100 of piece 0 of epoch 7, loss 3.5917562770843507\n",
      "now time: 17:4. Step 200 of piece 0 of epoch 7, loss 2.2891901183128356\n",
      "now time: 17:6. Step 300 of piece 0 of epoch 7, loss 2.290775616168976\n",
      "now time: 17:8. Step 400 of piece 0 of epoch 7, loss 2.274896762371063\n",
      "now time: 17:11. Step 500 of piece 0 of epoch 7, loss 2.2604011845588685\n",
      "now time: 17:13. Step 600 of piece 0 of epoch 7, loss 2.2514867210388183\n",
      "epoch 8\n",
      "time: 2020-04-15 17:14:35.535781\n",
      "now time: 17:16. Step 100 of piece 0 of epoch 8, loss 3.353382387161255\n",
      "now time: 17:19. Step 200 of piece 0 of epoch 8, loss 2.1401731061935423\n",
      "now time: 17:21. Step 300 of piece 0 of epoch 8, loss 2.127545561790466\n",
      "now time: 17:23. Step 400 of piece 0 of epoch 8, loss 2.1278503799438475\n",
      "now time: 17:26. Step 500 of piece 0 of epoch 8, loss 2.119332499504089\n",
      "now time: 17:28. Step 600 of piece 0 of epoch 8, loss 2.117097055912018\n",
      "epoch 9\n",
      "time: 2020-04-15 17:29:48.394014\n",
      "now time: 17:32. Step 100 of piece 0 of epoch 9, loss 3.121765913963318\n",
      "now time: 17:34. Step 200 of piece 0 of epoch 9, loss 1.9978270614147187\n",
      "now time: 17:36. Step 300 of piece 0 of epoch 9, loss 1.9910723960399628\n",
      "now time: 17:39. Step 400 of piece 0 of epoch 9, loss 1.9931402373313905\n",
      "now time: 17:41. Step 500 of piece 0 of epoch 9, loss 1.993642988204956\n",
      "now time: 17:43. Step 600 of piece 0 of epoch 9, loss 1.9851500868797303\n",
      "epoch 10\n",
      "time: 2020-04-15 17:45:01.859552\n",
      "now time: 17:47. Step 100 of piece 0 of epoch 10, loss 2.919776859283447\n",
      "now time: 17:49. Step 200 of piece 0 of epoch 10, loss 1.8584397792816163\n",
      "now time: 17:52. Step 300 of piece 0 of epoch 10, loss 1.864848426580429\n",
      "now time: 17:54. Step 400 of piece 0 of epoch 10, loss 1.8621621799468995\n",
      "now time: 17:56. Step 500 of piece 0 of epoch 10, loss 1.8655998504161835\n",
      "now time: 17:58. Step 600 of piece 0 of epoch 10, loss 1.8593075788021087\n",
      "epoch 11\n",
      "time: 2020-04-15 18:00:14.249492\n",
      "now time: 18:2. Step 100 of piece 0 of epoch 11, loss 2.7112387478351594\n",
      "now time: 18:4. Step 200 of piece 0 of epoch 11, loss 1.7319213378429412\n",
      "now time: 18:7. Step 300 of piece 0 of epoch 11, loss 1.7361567258834838\n",
      "now time: 18:9. Step 400 of piece 0 of epoch 11, loss 1.7334405970573425\n",
      "now time: 18:11. Step 500 of piece 0 of epoch 11, loss 1.7372874903678894\n",
      "now time: 18:14. Step 600 of piece 0 of epoch 11, loss 1.7343984651565552\n",
      "epoch 12\n",
      "time: 2020-04-15 18:15:27.542051\n",
      "now time: 18:17. Step 100 of piece 0 of epoch 12, loss 2.519212577342987\n",
      "now time: 18:20. Step 200 of piece 0 of epoch 12, loss 1.5970988702774047\n",
      "now time: 18:22. Step 300 of piece 0 of epoch 12, loss 1.6091710793972016\n",
      "now time: 18:24. Step 400 of piece 0 of epoch 12, loss 1.6110911631584168\n",
      "now time: 18:27. Step 500 of piece 0 of epoch 12, loss 1.6152786648273467\n",
      "now time: 18:29. Step 600 of piece 0 of epoch 12, loss 1.6107683336734773\n",
      "epoch 13\n",
      "time: 2020-04-15 18:30:41.606360\n",
      "now time: 18:33. Step 100 of piece 0 of epoch 13, loss 2.3315857350826263\n",
      "now time: 18:35. Step 200 of piece 0 of epoch 13, loss 1.4754097938537598\n",
      "now time: 18:37. Step 300 of piece 0 of epoch 13, loss 1.4914902305603028\n",
      "now time: 18:40. Step 400 of piece 0 of epoch 13, loss 1.4918725061416627\n",
      "now time: 18:42. Step 500 of piece 0 of epoch 13, loss 1.499447865486145\n",
      "now time: 18:44. Step 600 of piece 0 of epoch 13, loss 1.5025176680088044\n",
      "epoch 14\n",
      "time: 2020-04-15 18:45:52.753296\n",
      "now time: 18:48. Step 100 of piece 0 of epoch 14, loss 2.1646343755722044\n",
      "now time: 18:50. Step 200 of piece 0 of epoch 14, loss 1.370161873102188\n",
      "now time: 18:52. Step 300 of piece 0 of epoch 14, loss 1.3796018946170807\n",
      "now time: 18:55. Step 400 of piece 0 of epoch 14, loss 1.3861185371875764\n",
      "now time: 18:57. Step 500 of piece 0 of epoch 14, loss 1.3928071820735932\n",
      "now time: 18:59. Step 600 of piece 0 of epoch 14, loss 1.3923952305316925\n",
      "epoch 15\n",
      "time: 2020-04-15 19:01:05.648191\n",
      "now time: 19:3. Step 100 of piece 0 of epoch 15, loss 2.0002172064781187\n",
      "now time: 19:5. Step 200 of piece 0 of epoch 15, loss 1.264787791967392\n",
      "now time: 19:8. Step 300 of piece 0 of epoch 15, loss 1.2819219279289245\n",
      "now time: 19:10. Step 400 of piece 0 of epoch 15, loss 1.2859075391292571\n",
      "now time: 19:12. Step 500 of piece 0 of epoch 15, loss 1.2888862931728362\n",
      "now time: 19:15. Step 600 of piece 0 of epoch 15, loss 1.2981469976902007\n",
      "epoch 16\n",
      "time: 2020-04-15 19:16:19.263707\n",
      "now time: 19:18. Step 100 of piece 0 of epoch 16, loss 1.8479178178310394\n",
      "now time: 19:20. Step 200 of piece 0 of epoch 16, loss 1.1740720319747924\n",
      "now time: 19:23. Step 300 of piece 0 of epoch 16, loss 1.182775330543518\n",
      "now time: 19:25. Step 400 of piece 0 of epoch 16, loss 1.195211055278778\n",
      "now time: 19:27. Step 500 of piece 0 of epoch 16, loss 1.1991245222091675\n",
      "now time: 19:30. Step 600 of piece 0 of epoch 16, loss 1.2076144707202912\n",
      "epoch 17\n",
      "time: 2020-04-15 19:31:32.889558\n",
      "now time: 19:33. Step 100 of piece 0 of epoch 17, loss 1.708329167366028\n",
      "now time: 19:36. Step 200 of piece 0 of epoch 17, loss 1.081036022901535\n",
      "now time: 19:38. Step 300 of piece 0 of epoch 17, loss 1.100201803445816\n",
      "now time: 19:40. Step 400 of piece 0 of epoch 17, loss 1.1070242464542388\n",
      "now time: 19:43. Step 500 of piece 0 of epoch 17, loss 1.112037981748581\n",
      "now time: 19:45. Step 600 of piece 0 of epoch 17, loss 1.1215759754180907\n",
      "epoch 18\n",
      "time: 2020-04-15 19:46:45.495784\n",
      "now time: 19:49. Step 100 of piece 0 of epoch 18, loss 1.5878221559524537\n",
      "now time: 19:51. Step 200 of piece 0 of epoch 18, loss 1.003579213619232\n",
      "now time: 19:53. Step 300 of piece 0 of epoch 18, loss 1.018480949997902\n",
      "now time: 19:56. Step 400 of piece 0 of epoch 18, loss 1.0251079487800598\n",
      "now time: 19:58. Step 500 of piece 0 of epoch 18, loss 1.0381407469511033\n",
      "now time: 20:0. Step 600 of piece 0 of epoch 18, loss 1.045699520111084\n",
      "epoch 19\n",
      "time: 2020-04-15 20:01:57.541254\n",
      "now time: 20:4. Step 100 of piece 0 of epoch 19, loss 1.4695838075876235\n",
      "now time: 20:6. Step 200 of piece 0 of epoch 19, loss 0.9290997576713562\n",
      "now time: 20:8. Step 300 of piece 0 of epoch 19, loss 0.9438606458902359\n",
      "now time: 20:11. Step 400 of piece 0 of epoch 19, loss 0.9564685702323914\n",
      "now time: 20:13. Step 500 of piece 0 of epoch 19, loss 0.96423868060112\n",
      "now time: 20:15. Step 600 of piece 0 of epoch 19, loss 0.9693721115589142\n",
      "epoch 20\n",
      "time: 2020-04-15 20:17:10.217411\n",
      "now time: 20:19. Step 100 of piece 0 of epoch 20, loss 1.36171619951725\n",
      "now time: 20:21. Step 200 of piece 0 of epoch 20, loss 0.8624824315309525\n",
      "now time: 20:24. Step 300 of piece 0 of epoch 20, loss 0.875018789768219\n",
      "now time: 20:26. Step 400 of piece 0 of epoch 20, loss 0.8895971018075943\n",
      "now time: 20:28. Step 500 of piece 0 of epoch 20, loss 0.8960015159845353\n",
      "now time: 20:31. Step 600 of piece 0 of epoch 20, loss 0.9056305712461472\n",
      "epoch 21\n",
      "time: 2020-04-15 20:32:23.822298\n",
      "now time: 20:34. Step 100 of piece 0 of epoch 21, loss 1.2663889384269715\n",
      "now time: 20:37. Step 200 of piece 0 of epoch 21, loss 0.801256622672081\n",
      "now time: 20:39. Step 300 of piece 0 of epoch 21, loss 0.8171230590343476\n",
      "now time: 20:41. Step 400 of piece 0 of epoch 21, loss 0.8297655349969864\n",
      "now time: 20:44. Step 500 of piece 0 of epoch 21, loss 0.8344705939292908\n",
      "now time: 20:46. Step 600 of piece 0 of epoch 21, loss 0.8446415865421295\n",
      "epoch 22\n",
      "time: 2020-04-15 20:47:35.347714\n",
      "now time: 20:49. Step 100 of piece 0 of epoch 22, loss 1.1805259597301483\n",
      "now time: 20:52. Step 200 of piece 0 of epoch 22, loss 0.7449513816833496\n",
      "now time: 20:54. Step 300 of piece 0 of epoch 22, loss 0.7607208514213561\n",
      "now time: 20:56. Step 400 of piece 0 of epoch 22, loss 0.7735857105255127\n",
      "now time: 20:59. Step 500 of piece 0 of epoch 22, loss 0.7789333832263946\n",
      "now time: 21:1. Step 600 of piece 0 of epoch 22, loss 0.7862894690036774\n",
      "epoch 23\n",
      "time: 2020-04-15 21:02:45.830907\n",
      "now time: 21:5. Step 100 of piece 0 of epoch 23, loss 1.101191281080246\n",
      "now time: 21:7. Step 200 of piece 0 of epoch 23, loss 0.6946304059028625\n",
      "now time: 21:9. Step 300 of piece 0 of epoch 23, loss 0.71115085542202\n",
      "now time: 21:12. Step 400 of piece 0 of epoch 23, loss 0.721987521648407\n",
      "now time: 21:14. Step 500 of piece 0 of epoch 23, loss 0.7310593557357788\n",
      "now time: 21:16. Step 600 of piece 0 of epoch 23, loss 0.7384589159488678\n",
      "epoch 24\n",
      "time: 2020-04-15 21:17:57.450499\n",
      "now time: 21:20. Step 100 of piece 0 of epoch 24, loss 1.0261419194936752\n",
      "now time: 21:22. Step 200 of piece 0 of epoch 24, loss 0.6483059084415436\n",
      "now time: 21:24. Step 300 of piece 0 of epoch 24, loss 0.6651965218782425\n",
      "now time: 21:27. Step 400 of piece 0 of epoch 24, loss 0.6785827565193177\n",
      "now time: 21:29. Step 500 of piece 0 of epoch 24, loss 0.6851316159963607\n",
      "now time: 21:31. Step 600 of piece 0 of epoch 24, loss 0.688796973824501\n",
      "epoch 25\n",
      "time: 2020-04-15 21:33:09.327792\n",
      "now time: 21:35. Step 100 of piece 0 of epoch 25, loss 0.9617976593971252\n",
      "now time: 21:37. Step 200 of piece 0 of epoch 25, loss 0.6090643048286438\n",
      "now time: 21:40. Step 300 of piece 0 of epoch 25, loss 0.6249949634075165\n",
      "now time: 21:42. Step 400 of piece 0 of epoch 25, loss 0.6318100428581238\n",
      "now time: 21:44. Step 500 of piece 0 of epoch 25, loss 0.6436722785234451\n",
      "now time: 21:47. Step 600 of piece 0 of epoch 25, loss 0.6496444445848465\n",
      "epoch 26\n",
      "time: 2020-04-15 21:48:21.041485\n",
      "now time: 21:50. Step 100 of piece 0 of epoch 26, loss 0.901117171049118\n",
      "now time: 21:53. Step 200 of piece 0 of epoch 26, loss 0.5753740096092224\n",
      "now time: 21:55. Step 300 of piece 0 of epoch 26, loss 0.583504564166069\n",
      "now time: 21:57. Step 400 of piece 0 of epoch 26, loss 0.594581612944603\n",
      "now time: 21:59. Step 500 of piece 0 of epoch 26, loss 0.6036182540655136\n",
      "now time: 22:2. Step 600 of piece 0 of epoch 26, loss 0.6093786668777466\n",
      "epoch 27\n",
      "time: 2020-04-15 22:03:33.613599\n",
      "now time: 22:5. Step 100 of piece 0 of epoch 27, loss 0.8510773688554764\n",
      "now time: 22:8. Step 200 of piece 0 of epoch 27, loss 0.537670938372612\n",
      "now time: 22:10. Step 300 of piece 0 of epoch 27, loss 0.5502716332674027\n",
      "now time: 22:12. Step 400 of piece 0 of epoch 27, loss 0.5642149746417999\n",
      "now time: 22:15. Step 500 of piece 0 of epoch 27, loss 0.5703799384832382\n",
      "now time: 22:17. Step 600 of piece 0 of epoch 27, loss 0.5749195080995559\n",
      "epoch 28\n",
      "time: 2020-04-15 22:18:45.924401\n",
      "now time: 22:21. Step 100 of piece 0 of epoch 28, loss 0.8030832180380821\n",
      "now time: 22:23. Step 200 of piece 0 of epoch 28, loss 0.5079865032434463\n",
      "now time: 22:25. Step 300 of piece 0 of epoch 28, loss 0.5229360622167587\n",
      "now time: 22:28. Step 400 of piece 0 of epoch 28, loss 0.5276629942655563\n",
      "now time: 22:30. Step 500 of piece 0 of epoch 28, loss 0.5372676402330399\n",
      "now time: 22:32. Step 600 of piece 0 of epoch 28, loss 0.5421740061044693\n",
      "epoch 29\n",
      "time: 2020-04-15 22:33:57.258159\n",
      "now time: 22:36. Step 100 of piece 0 of epoch 29, loss 0.7560884636640549\n",
      "now time: 22:38. Step 200 of piece 0 of epoch 29, loss 0.48082042604684827\n",
      "now time: 22:40. Step 300 of piece 0 of epoch 29, loss 0.49196827739477156\n",
      "now time: 22:43. Step 400 of piece 0 of epoch 29, loss 0.501786926984787\n",
      "now time: 22:45. Step 500 of piece 0 of epoch 29, loss 0.508795508146286\n",
      "now time: 22:47. Step 600 of piece 0 of epoch 29, loss 0.5156762874126435\n",
      "epoch 30\n",
      "time: 2020-04-15 22:49:07.736909\n",
      "now time: 22:51. Step 100 of piece 0 of epoch 30, loss 0.7185487020015716\n",
      "now time: 22:53. Step 200 of piece 0 of epoch 30, loss 0.4539361524581909\n",
      "now time: 22:56. Step 300 of piece 0 of epoch 30, loss 0.468003945350647\n",
      "now time: 22:58. Step 400 of piece 0 of epoch 30, loss 0.47392519295215607\n",
      "now time: 23:0. Step 500 of piece 0 of epoch 30, loss 0.48212786734104157\n",
      "now time: 23:3. Step 600 of piece 0 of epoch 30, loss 0.48897609204053877\n",
      "epoch 31\n",
      "time: 2020-04-15 23:04:18.715193\n",
      "now time: 23:6. Step 100 of piece 0 of epoch 31, loss 0.6784615129232406\n",
      "now time: 23:8. Step 200 of piece 0 of epoch 31, loss 0.4309553974866867\n",
      "now time: 23:11. Step 300 of piece 0 of epoch 31, loss 0.4419764783978462\n",
      "now time: 23:13. Step 400 of piece 0 of epoch 31, loss 0.45303363144397735\n",
      "now time: 23:15. Step 500 of piece 0 of epoch 31, loss 0.458934611082077\n",
      "now time: 23:18. Step 600 of piece 0 of epoch 31, loss 0.4648133584856987\n",
      "epoch 32\n",
      "time: 2020-04-15 23:19:31.183962\n",
      "now time: 23:21. Step 100 of piece 0 of epoch 32, loss 0.6468883296847343\n",
      "now time: 23:24. Step 200 of piece 0 of epoch 32, loss 0.41118053704500196\n",
      "now time: 23:26. Step 300 of piece 0 of epoch 32, loss 0.4236115476489067\n",
      "now time: 23:28. Step 400 of piece 0 of epoch 32, loss 0.4289156427979469\n",
      "now time: 23:31. Step 500 of piece 0 of epoch 32, loss 0.43609940618276594\n",
      "now time: 23:33. Step 600 of piece 0 of epoch 32, loss 0.4410631287097931\n",
      "epoch 33\n",
      "time: 2020-04-15 23:34:43.984086\n",
      "now time: 23:37. Step 100 of piece 0 of epoch 33, loss 0.617478822171688\n",
      "now time: 23:39. Step 200 of piece 0 of epoch 33, loss 0.3904386174678802\n",
      "now time: 23:41. Step 300 of piece 0 of epoch 33, loss 0.4035208335518837\n",
      "now time: 23:44. Step 400 of piece 0 of epoch 33, loss 0.4103273320198059\n",
      "now time: 23:46. Step 500 of piece 0 of epoch 33, loss 0.416548553109169\n",
      "now time: 23:48. Step 600 of piece 0 of epoch 33, loss 0.42012153297662735\n",
      "epoch 34\n",
      "time: 2020-04-15 23:49:56.326224\n",
      "now time: 23:52. Step 100 of piece 0 of epoch 34, loss 0.5874047026038169\n",
      "now time: 23:54. Step 200 of piece 0 of epoch 34, loss 0.37458421528339386\n",
      "now time: 23:56. Step 300 of piece 0 of epoch 34, loss 0.38481183499097826\n",
      "now time: 23:59. Step 400 of piece 0 of epoch 34, loss 0.3928172370791435\n",
      "now time: 0:1. Step 500 of piece 0 of epoch 34, loss 0.3966645523905754\n",
      "now time: 0:3. Step 600 of piece 0 of epoch 34, loss 0.4024667489528656\n",
      "epoch 35\n",
      "time: 2020-04-16 00:05:10.684395\n",
      "now time: 0:7. Step 100 of piece 0 of epoch 35, loss 0.5632502347230911\n",
      "now time: 0:9. Step 200 of piece 0 of epoch 35, loss 0.35609857708215714\n",
      "now time: 0:12. Step 300 of piece 0 of epoch 35, loss 0.3689779624342918\n",
      "now time: 0:14. Step 400 of piece 0 of epoch 35, loss 0.3744719043374062\n",
      "now time: 0:16. Step 500 of piece 0 of epoch 35, loss 0.38015346258878707\n",
      "now time: 0:19. Step 600 of piece 0 of epoch 35, loss 0.3845193722844124\n",
      "epoch 36\n",
      "time: 2020-04-16 00:20:25.155475\n",
      "now time: 0:22. Step 100 of piece 0 of epoch 36, loss 0.5398496600985527\n",
      "now time: 0:25. Step 200 of piece 0 of epoch 36, loss 0.3443050274252892\n",
      "now time: 0:27. Step 300 of piece 0 of epoch 36, loss 0.35219303488731385\n",
      "now time: 0:29. Step 400 of piece 0 of epoch 36, loss 0.35910763204097745\n",
      "now time: 0:32. Step 500 of piece 0 of epoch 36, loss 0.363262058198452\n",
      "now time: 0:34. Step 600 of piece 0 of epoch 36, loss 0.36868571549654006\n",
      "epoch 37\n",
      "time: 2020-04-16 00:35:38.830149\n",
      "now time: 0:37. Step 100 of piece 0 of epoch 37, loss 0.516792778968811\n",
      "now time: 0:40. Step 200 of piece 0 of epoch 37, loss 0.32857394516468047\n",
      "now time: 0:42. Step 300 of piece 0 of epoch 37, loss 0.33811508685350417\n",
      "now time: 0:44. Step 400 of piece 0 of epoch 37, loss 0.34416481733322146\n",
      "now time: 0:47. Step 500 of piece 0 of epoch 37, loss 0.3489814335107803\n",
      "now time: 0:49. Step 600 of piece 0 of epoch 37, loss 0.35296635299921036\n",
      "epoch 38\n",
      "time: 2020-04-16 00:50:52.420801\n",
      "now time: 0:53. Step 100 of piece 0 of epoch 38, loss 0.4966335815191269\n",
      "now time: 0:55. Step 200 of piece 0 of epoch 38, loss 0.31613801419734955\n",
      "now time: 0:57. Step 300 of piece 0 of epoch 38, loss 0.3226424378156662\n",
      "now time: 1:0. Step 400 of piece 0 of epoch 38, loss 0.3319207453727722\n",
      "now time: 1:2. Step 500 of piece 0 of epoch 38, loss 0.3349299710988998\n",
      "now time: 1:4. Step 600 of piece 0 of epoch 38, loss 0.3380966606736183\n",
      "epoch 39\n",
      "time: 2020-04-16 01:06:05.573532\n",
      "now time: 1:8. Step 100 of piece 0 of epoch 39, loss 0.4766260117292404\n",
      "now time: 1:10. Step 200 of piece 0 of epoch 39, loss 0.3028292840719223\n",
      "now time: 1:13. Step 300 of piece 0 of epoch 39, loss 0.31205051809549333\n"
     ]
    }
   ],
   "source": [
    "!cd tasks/doupo; bash train.sh; cd ../..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在训练好的模型上做Inference，生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0415 00:18:07.675502 140303028275008 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "args:\n",
      "Namespace(batch_size=1, device='0', fast_pattern=False, length=100, model_config='doupo/model/final_model/model_config.json', model_path='doupo/model/final_model/', no_wordpiece=False, nsamples=10, prefix='萧炎', repetition_penalty=1.0, save_samples=True, save_samples_path='doupo/outputs/', segment=False, temperature=0.8, tokenizer_path='doupo/config/vocab.txt', topk=50, topp=0)\n",
      "I0415 00:18:07.959635 140303028275008 configuration_utils.py:148] loading configuration file doupo/model/final_model/config.json\n",
      "I0415 00:18:07.960070 140303028275008 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 10,\n",
      "  \"n_positions\": 512,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21163\n",
      "}\n",
      "\n",
      "I0415 00:18:07.960801 140303028275008 modeling_utils.py:334] loading weights file doupo/model/final_model/pytorch_model.bin\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 147.02it/s]\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "萧炎眼睛紧紧的盯着药鼎之内，半晌后，缓缓道：“即使是以后，老师对我也不会太过如何忌惮，可如今你也不用太过担心，日后回复以往的实力，定然会极为恐怖，这炼药术固然有些麻烦，不过炼制这丹药，也并非是我想象中的那\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 145.14it/s]\n",
      "======================================== SAMPLE 2 ========================================\n",
      "\n",
      "萧炎这等强者来说，无疑是让得他有些哭笑不得，原来他们以为是因为自己在黑角域混乱中，可却始终不好胡乱想的出售这些强者的“黑盟”强横势力，萧炎他们对这个势力的吸引力很是了解，他们这些家伙想要把丹药当做什么？[SEP]\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 147.77it/s]\n",
      "======================================== SAMPLE 3 ========================================\n",
      "\n",
      "萧炎略微皱眉，这般打量着，他倒也并未如何掩饰，小医仙虽然实力不弱，但毕竟不管如何说也是斗宗强者，而且看气息，也并非是寻常之物，若非萧炎小医仙是她知道的这种厄难毒体，恐怕两人联手，即便是我也是得暂避锋芒。[SEP]\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 148.22it/s]\n",
      "======================================== SAMPLE 4 ========================================\n",
      "\n",
      "萧炎的实力，在这里，他可是无法与之抗衡。[SEP][CLS]“好了，今天早点休息一晚上，明日便动身离开。”萧炎对着吴昊等人挥了挥手，然后便是转身在前引路，其后，薰儿与吴昊等人紧随而上。[SEP][CLS]“呵呵，萧炎弟弟，多谢了。”与吴\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 145.72it/s]\n",
      "======================================== SAMPLE 5 ========================================\n",
      "\n",
      "萧炎体内斗气略微有些不安，按照这般速度，只要再施展三种异火，怕便是得当场毙命。[SEP][CLS]而见到萧炎并未过多的拖延，那慕兰二老却是冷笑一声，旋即身形一动，便是诡异的消失在了原地，而在这消失的天际之上，一道模糊黑影\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 147.39it/s]\n",
      "======================================== SAMPLE 6 ========================================\n",
      "\n",
      "萧炎一笑，摇了摇头，转身便走出现在街道四周，然后对着拍卖场行去。[SEP][CLS]在拍卖会场内部的一些商店时分，一些偏僻的商铺匾，便是出现在了萧炎目光之中。[SEP][CLS]在前面，大多都是商店员打开，萧炎这才将一些安全的商铺给柜台\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 148.32it/s]\n",
      "======================================== SAMPLE 7 ========================================\n",
      "\n",
      "萧炎微微一笑，道：“你便说了，若是他真的落败，日后，我萧炎定然会暴涨，即便如今能胜他，但要将之解救，也是会令得你难以置信。”[SEP][CLS]“萧炎先生如何？”天火尊者眉头一皱，沉声道。[SEP][CLS]“呵呵，这位朋友，你是炼药师\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 148.06it/s]\n",
      "======================================== SAMPLE 8 ========================================\n",
      "\n",
      "萧炎这话题，明显也不会给萧炎半分安无恙。[SEP][CLS]“那魂千陌，我所能做的，便只是这样了……”[SEP][CLS]萧炎迟疑了一下，突然想起当初在当初那时，他便是明白，为何当初那位斗圣强者，居然依旧会出现这等层次？[SEP][CLS]“魂千陌，你\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 145.84it/s]\n",
      "======================================== SAMPLE 9 ========================================\n",
      "\n",
      "萧炎哥哥要进去，你来得很安静啊，等我们黑印城被风暴打开，我们一定会出现不小心。”[SEP][CLS]“嗯。”[SEP][CLS]闻言，罗侯点了点头，沉吟了一会，笑道：“麻烦了。”[SEP][CLS]萧炎微微点头，抬头望着那一脸笑容清新生，笑道：“好吧，\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 147.83it/s]\n",
      "======================================== SAMPLE 10 ========================================\n",
      "\n",
      "萧炎也是面色凝重的点了点头，斗气在体内急速奔腾，而且那经脉之中，竟然是隐藏着一股极为雄浑的能量，这些能量汇聚的能量似乎都是在瞬间波动一般，不断的对着萧炎体内灌涌而去。[SEP][CLS]感受着体内那越来越雄浑的斗气，萧炎\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!cd doupo;bash generate.sh;cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\tformat_raw_txt.py  model    outputs_from_scratch  tokenized  vocab.txt\n",
      "divide\tgenerate.sh\t   outputs  rawdata\t\t  train.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调GPT2模型进行数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
